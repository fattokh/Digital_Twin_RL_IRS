{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fattokh/Digital_Twin_RL_IRS/blob/main/untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbTgXDoGuT6g",
        "outputId": "d45dd0fb-104c-4141-c43a-ce78ecdca488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signal\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-024456618fb5>:218: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_nn.load_state_dict(torch.load('/content/drive/MyDrive/model_based_5M_params_nn_M_36_signal_tr_size_4500000_repeat_02.pt'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Repeat: 0, TrSize: 9000000, Epoch: 1/50, Batch: 500/2250, Training loss NN: 1.07091.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 1/50, Batch: 1000/2250, Training loss NN: 0.76969.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 1/50, Batch: 1500/2250, Training loss NN: 0.73419.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 1/50, Batch: 2000/2250, Training loss NN: 0.93024.\n",
            " Y_hat_nn-1.127563,Y_true0.274786 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 1,  Val NN MSE/NMSE: 0.48085/203.83269.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 2/50, Batch: 500/2250, Training loss NN: 0.98689.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 2/50, Batch: 1000/2250, Training loss NN: 0.72940.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 2/50, Batch: 1500/2250, Training loss NN: 0.75989.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 2/50, Batch: 2000/2250, Training loss NN: 0.90134.\n",
            " Y_hat_nn-6.293762,Y_true-7.279428 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 2,  Val NN MSE/NMSE: 0.48546/195.89926.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 3/50, Batch: 500/2250, Training loss NN: 0.97323.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 3/50, Batch: 1000/2250, Training loss NN: 0.89628.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 3/50, Batch: 1500/2250, Training loss NN: 0.80736.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 3/50, Batch: 2000/2250, Training loss NN: 0.87997.\n",
            " Y_hat_nn-8.247696,Y_true-8.355784 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 3,  Val NN MSE/NMSE: 0.47284/164.17538.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 4/50, Batch: 500/2250, Training loss NN: 1.04842.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 4/50, Batch: 1000/2250, Training loss NN: 0.81814.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 4/50, Batch: 1500/2250, Training loss NN: 0.70050.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 4/50, Batch: 2000/2250, Training loss NN: 0.90197.\n",
            " Y_hat_nn-9.802841,Y_true-9.248377 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 4,  Val NN MSE/NMSE: 0.47340/168.09741.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 5/50, Batch: 500/2250, Training loss NN: 1.08138.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 5/50, Batch: 1000/2250, Training loss NN: 0.75328.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 5/50, Batch: 1500/2250, Training loss NN: 0.71276.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 5/50, Batch: 2000/2250, Training loss NN: 0.88530.\n",
            " Y_hat_nn-22.076149,Y_true-22.437048 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 5,  Val NN MSE/NMSE: 0.44668/151.12553.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 6/50, Batch: 500/2250, Training loss NN: 1.02257.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 6/50, Batch: 1000/2250, Training loss NN: 0.85789.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 6/50, Batch: 1500/2250, Training loss NN: 0.80539.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 6/50, Batch: 2000/2250, Training loss NN: 0.94659.\n",
            " Y_hat_nn-9.217697,Y_true-9.311558 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 6,  Val NN MSE/NMSE: 0.45204/192.07831.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 7/50, Batch: 500/2250, Training loss NN: 1.09010.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 7/50, Batch: 1000/2250, Training loss NN: 0.77292.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 7/50, Batch: 1500/2250, Training loss NN: 0.72765.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 7/50, Batch: 2000/2250, Training loss NN: 0.97646.\n",
            " Y_hat_nn-26.119267,Y_true-27.591892 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 7,  Val NN MSE/NMSE: 0.44999/159.66940.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 8/50, Batch: 500/2250, Training loss NN: 1.00490.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 8/50, Batch: 1000/2250, Training loss NN: 0.70705.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 8/50, Batch: 1500/2250, Training loss NN: 0.73306.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 8/50, Batch: 2000/2250, Training loss NN: 0.87572.\n",
            " Y_hat_nn-11.033684,Y_true-10.884615 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 8,  Val NN MSE/NMSE: 0.43170/156.09157.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 9/50, Batch: 500/2250, Training loss NN: 0.94274.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 9/50, Batch: 1000/2250, Training loss NN: 0.73904.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 9/50, Batch: 1500/2250, Training loss NN: 0.66249.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 9/50, Batch: 2000/2250, Training loss NN: 0.80054.\n",
            " Y_hat_nn-4.341530,Y_true-4.315256 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 9,  Val NN MSE/NMSE: 0.44532/158.33675.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 10/50, Batch: 500/2250, Training loss NN: 0.95274.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 10/50, Batch: 1000/2250, Training loss NN: 0.76519.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 10/50, Batch: 1500/2250, Training loss NN: 0.69963.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 10/50, Batch: 2000/2250, Training loss NN: 0.86287.\n",
            " Y_hat_nn-6.543297,Y_true-6.688213 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 10,  Val NN MSE/NMSE: 0.43472/146.75871.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 11/50, Batch: 500/2250, Training loss NN: 1.00212.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 11/50, Batch: 1000/2250, Training loss NN: 0.72266.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 11/50, Batch: 1500/2250, Training loss NN: 0.65194.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 11/50, Batch: 2000/2250, Training loss NN: 0.85799.\n",
            " Y_hat_nn-1.943703,Y_true-2.510236 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 11,  Val NN MSE/NMSE: 0.42939/156.98415.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 12/50, Batch: 500/2250, Training loss NN: 0.95092.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 12/50, Batch: 1000/2250, Training loss NN: 0.74039.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 12/50, Batch: 1500/2250, Training loss NN: 0.70097.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 12/50, Batch: 2000/2250, Training loss NN: 0.83727.\n",
            " Y_hat_nn-4.384613,Y_true-4.315256 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 12,  Val NN MSE/NMSE: 0.43234/171.67574.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 13/50, Batch: 500/2250, Training loss NN: 0.96156.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 13/50, Batch: 1000/2250, Training loss NN: 0.78279.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 13/50, Batch: 1500/2250, Training loss NN: 0.66021.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 13/50, Batch: 2000/2250, Training loss NN: 0.96037.\n",
            " Y_hat_nn-21.217117,Y_true-22.110147 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 13,  Val NN MSE/NMSE: 0.42227/153.74826.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 14/50, Batch: 500/2250, Training loss NN: 0.94355.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 14/50, Batch: 1000/2250, Training loss NN: 0.77961.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 14/50, Batch: 1500/2250, Training loss NN: 0.60900.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 14/50, Batch: 2000/2250, Training loss NN: 0.83540.\n",
            " Y_hat_nn-6.989021,Y_true-6.494100 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 14,  Val NN MSE/NMSE: 0.42335/159.43811.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 15/50, Batch: 500/2250, Training loss NN: 0.94109.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 15/50, Batch: 1000/2250, Training loss NN: 0.70406.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 15/50, Batch: 1500/2250, Training loss NN: 0.61592.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 15/50, Batch: 2000/2250, Training loss NN: 0.80016.\n",
            " Y_hat_nn-4.348274,Y_true-4.577337 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 15,  Val NN MSE/NMSE: 0.41741/162.53767.\n",
            "Learning rate reduced to 0.00001.\n",
            "Current LR: 1e-05\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 16/50, Batch: 500/2250, Training loss NN: 0.98366.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 16/50, Batch: 1000/2250, Training loss NN: 0.64257.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 16/50, Batch: 1500/2250, Training loss NN: 0.58749.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 16/50, Batch: 2000/2250, Training loss NN: 0.71412.\n",
            " Y_hat_nn-3.974998,Y_true-4.865125 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 16,  Val NN MSE/NMSE: 0.38609/202.04649.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 17/50, Batch: 500/2250, Training loss NN: 0.94860.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 17/50, Batch: 1000/2250, Training loss NN: 0.67253.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 17/50, Batch: 1500/2250, Training loss NN: 0.62054.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 17/50, Batch: 2000/2250, Training loss NN: 0.69097.\n",
            " Y_hat_nn-12.515366,Y_true-11.816010 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 17,  Val NN MSE/NMSE: 0.38478/194.76247.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 18/50, Batch: 500/2250, Training loss NN: 0.90859.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 18/50, Batch: 1000/2250, Training loss NN: 0.66863.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 18/50, Batch: 1500/2250, Training loss NN: 0.62447.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 18/50, Batch: 2000/2250, Training loss NN: 0.76492.\n",
            " Y_hat_nn-11.642288,Y_true-11.266683 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 18,  Val NN MSE/NMSE: 0.38253/214.14198.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 19/50, Batch: 500/2250, Training loss NN: 0.87632.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 19/50, Batch: 1000/2250, Training loss NN: 0.69378.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 19/50, Batch: 1500/2250, Training loss NN: 0.71679.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 19/50, Batch: 2000/2250, Training loss NN: 0.75107.\n",
            " Y_hat_nn-12.533501,Y_true-12.681110 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 19,  Val NN MSE/NMSE: 0.38230/192.53432.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 20/50, Batch: 500/2250, Training loss NN: 0.96657.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 20/50, Batch: 1000/2250, Training loss NN: 0.66986.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 20/50, Batch: 1500/2250, Training loss NN: 0.57552.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 20/50, Batch: 2000/2250, Training loss NN: 0.78229.\n",
            " Y_hat_nn-12.095291,Y_true-11.990887 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 20,  Val NN MSE/NMSE: 0.38009/200.03468.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 21/50, Batch: 500/2250, Training loss NN: 0.94011.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 21/50, Batch: 1000/2250, Training loss NN: 0.63833.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 21/50, Batch: 1500/2250, Training loss NN: 0.63099.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 21/50, Batch: 2000/2250, Training loss NN: 0.77276.\n",
            " Y_hat_nn0.774483,Y_true0.792120 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 21,  Val NN MSE/NMSE: 0.38147/186.15193.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 22/50, Batch: 500/2250, Training loss NN: 0.95584.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 22/50, Batch: 1000/2250, Training loss NN: 0.63850.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 22/50, Batch: 1500/2250, Training loss NN: 0.62354.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 22/50, Batch: 2000/2250, Training loss NN: 0.75980.\n",
            " Y_hat_nn-3.454361,Y_true-3.403888 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 22,  Val NN MSE/NMSE: 0.37972/198.03976.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 23/50, Batch: 500/2250, Training loss NN: 0.86690.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 23/50, Batch: 1000/2250, Training loss NN: 0.66945.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 23/50, Batch: 1500/2250, Training loss NN: 0.58087.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 23/50, Batch: 2000/2250, Training loss NN: 0.72378.\n",
            " Y_hat_nn-8.708382,Y_true-8.539483 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 23,  Val NN MSE/NMSE: 0.38061/188.63458.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 24/50, Batch: 500/2250, Training loss NN: 0.92659.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 24/50, Batch: 1000/2250, Training loss NN: 0.61506.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 24/50, Batch: 1500/2250, Training loss NN: 0.70567.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 24/50, Batch: 2000/2250, Training loss NN: 0.74880.\n",
            " Y_hat_nn1.660736,Y_true0.867592 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 24,  Val NN MSE/NMSE: 0.37938/182.40936.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 25/50, Batch: 500/2250, Training loss NN: 1.23631.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 25/50, Batch: 1000/2250, Training loss NN: 0.96958.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 25/50, Batch: 1500/2250, Training loss NN: 0.61563.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 25/50, Batch: 2000/2250, Training loss NN: 0.76300.\n",
            " Y_hat_nn-14.212448,Y_true-12.902034 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 25,  Val NN MSE/NMSE: 0.37977/194.88376.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 26/50, Batch: 500/2250, Training loss NN: 0.86364.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 26/50, Batch: 1000/2250, Training loss NN: 0.68151.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 26/50, Batch: 1500/2250, Training loss NN: 0.65686.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 26/50, Batch: 2000/2250, Training loss NN: 0.77821.\n",
            " Y_hat_nn-5.240044,Y_true-4.795181 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 26,  Val NN MSE/NMSE: 0.38074/189.81926.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 27/50, Batch: 500/2250, Training loss NN: 0.87868.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 27/50, Batch: 1000/2250, Training loss NN: 0.63486.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 27/50, Batch: 1500/2250, Training loss NN: 0.82758.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 27/50, Batch: 2000/2250, Training loss NN: 0.77788.\n",
            " Y_hat_nn4.620056,Y_true4.391215 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 27,  Val NN MSE/NMSE: 0.37880/201.40298.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 28/50, Batch: 500/2250, Training loss NN: 0.93734.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 28/50, Batch: 1000/2250, Training loss NN: 0.73246.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 28/50, Batch: 1500/2250, Training loss NN: 0.74035.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 28/50, Batch: 2000/2250, Training loss NN: 0.80699.\n",
            " Y_hat_nn-19.111076,Y_true-18.867867 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 28,  Val NN MSE/NMSE: 0.37793/201.31541.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 29/50, Batch: 500/2250, Training loss NN: 0.96537.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 29/50, Batch: 1000/2250, Training loss NN: 0.61816.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 29/50, Batch: 1500/2250, Training loss NN: 0.57679.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 29/50, Batch: 2000/2250, Training loss NN: 0.78078.\n",
            " Y_hat_nn-3.070305,Y_true-2.677441 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 29,  Val NN MSE/NMSE: 0.37756/189.52493.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 30/50, Batch: 500/2250, Training loss NN: 0.98271.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 30/50, Batch: 1000/2250, Training loss NN: 0.66636.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 30/50, Batch: 1500/2250, Training loss NN: 0.60392.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 30/50, Batch: 2000/2250, Training loss NN: 0.73198.\n",
            " Y_hat_nn-8.690598,Y_true-8.802938 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 30,  Val NN MSE/NMSE: 0.37875/190.92735.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 31/50, Batch: 500/2250, Training loss NN: 0.95286.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 31/50, Batch: 1000/2250, Training loss NN: 0.66745.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 31/50, Batch: 1500/2250, Training loss NN: 0.61038.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 31/50, Batch: 2000/2250, Training loss NN: 0.74825.\n",
            " Y_hat_nn2.863495,Y_true4.184665 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 31,  Val NN MSE/NMSE: 0.37794/174.00778.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 32/50, Batch: 500/2250, Training loss NN: 0.82489.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 32/50, Batch: 1000/2250, Training loss NN: 0.67123.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 32/50, Batch: 1500/2250, Training loss NN: 0.68588.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 32/50, Batch: 2000/2250, Training loss NN: 0.68596.\n",
            " Y_hat_nn-4.435715,Y_true-2.828565 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 32,  Val NN MSE/NMSE: 0.37703/188.41557.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 33/50, Batch: 500/2250, Training loss NN: 0.95649.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 33/50, Batch: 1000/2250, Training loss NN: 0.70393.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 33/50, Batch: 1500/2250, Training loss NN: 0.63874.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 33/50, Batch: 2000/2250, Training loss NN: 0.97178.\n",
            " Y_hat_nn-1.634766,Y_true-2.168299 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 33,  Val NN MSE/NMSE: 0.37623/187.91403.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 34/50, Batch: 500/2250, Training loss NN: 0.87558.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 34/50, Batch: 1000/2250, Training loss NN: 0.61092.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 34/50, Batch: 1500/2250, Training loss NN: 0.58095.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 34/50, Batch: 2000/2250, Training loss NN: 0.76679.\n",
            " Y_hat_nn-4.237297,Y_true-3.653312 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 34,  Val NN MSE/NMSE: 0.37807/177.16339.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 35/50, Batch: 500/2250, Training loss NN: 0.92839.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 35/50, Batch: 1000/2250, Training loss NN: 0.67040.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 35/50, Batch: 1500/2250, Training loss NN: 0.60710.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 35/50, Batch: 2000/2250, Training loss NN: 0.71614.\n",
            " Y_hat_nn-14.828049,Y_true-14.823807 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 35,  Val NN MSE/NMSE: 0.37842/185.09178.\n",
            "Learning rate reduced to 0.00000.\n",
            "Current LR: 1.0000000000000002e-06\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 36/50, Batch: 500/2250, Training loss NN: 0.87614.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 36/50, Batch: 1000/2250, Training loss NN: 0.69869.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 36/50, Batch: 1500/2250, Training loss NN: 0.61898.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 36/50, Batch: 2000/2250, Training loss NN: 0.63973.\n",
            " Y_hat_nn-1.658356,Y_true-0.611370 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 36,  Val NN MSE/NMSE: 0.37808/177.25201.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 37/50, Batch: 500/2250, Training loss NN: 0.85838.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 37/50, Batch: 1000/2250, Training loss NN: 0.64124.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 37/50, Batch: 1500/2250, Training loss NN: 0.57995.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 37/50, Batch: 2000/2250, Training loss NN: 0.73972.\n",
            " Y_hat_nn-7.692635,Y_true-8.098740 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 37,  Val NN MSE/NMSE: 0.37789/175.39073.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 38/50, Batch: 500/2250, Training loss NN: 0.87193.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 38/50, Batch: 1000/2250, Training loss NN: 0.60651.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 38/50, Batch: 1500/2250, Training loss NN: 0.61597.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 38/50, Batch: 2000/2250, Training loss NN: 0.70550.\n",
            " Y_hat_nn-0.484055,Y_true-1.360445 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 38,  Val NN MSE/NMSE: 0.37794/172.51172.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 39/50, Batch: 500/2250, Training loss NN: 0.85268.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 39/50, Batch: 1000/2250, Training loss NN: 0.62888.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 39/50, Batch: 1500/2250, Training loss NN: 0.59799.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 39/50, Batch: 2000/2250, Training loss NN: 0.69485.\n",
            " Y_hat_nn-0.830032,Y_true-1.377846 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 39,  Val NN MSE/NMSE: 0.37796/172.81938.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 40/50, Batch: 500/2250, Training loss NN: 0.94912.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 40/50, Batch: 1000/2250, Training loss NN: 0.80276.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 40/50, Batch: 1500/2250, Training loss NN: 0.59598.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 40/50, Batch: 2000/2250, Training loss NN: 0.67740.\n",
            " Y_hat_nn0.655479,Y_true0.219478 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 40,  Val NN MSE/NMSE: 0.37890/175.33568.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 41/50, Batch: 500/2250, Training loss NN: 0.84720.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 41/50, Batch: 1000/2250, Training loss NN: 0.58582.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 41/50, Batch: 1500/2250, Training loss NN: 0.66382.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 41/50, Batch: 2000/2250, Training loss NN: 0.75591.\n",
            " Y_hat_nn1.783585,Y_true1.931686 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 41,  Val NN MSE/NMSE: 0.37805/172.54669.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 42/50, Batch: 500/2250, Training loss NN: 0.92689.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 42/50, Batch: 1000/2250, Training loss NN: 0.67609.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 42/50, Batch: 1500/2250, Training loss NN: 0.70464.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 42/50, Batch: 2000/2250, Training loss NN: 0.78655.\n",
            " Y_hat_nn-2.271393,Y_true-2.046512 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 42,  Val NN MSE/NMSE: 0.37821/176.02841.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 43/50, Batch: 500/2250, Training loss NN: 0.86380.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 43/50, Batch: 1000/2250, Training loss NN: 0.67135.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 43/50, Batch: 1500/2250, Training loss NN: 0.57242.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 43/50, Batch: 2000/2250, Training loss NN: 0.72201.\n",
            " Y_hat_nn-7.009354,Y_true-6.488162 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 43,  Val NN MSE/NMSE: 0.37743/173.00529.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 44/50, Batch: 500/2250, Training loss NN: 0.90466.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 44/50, Batch: 1000/2250, Training loss NN: 0.74173.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 44/50, Batch: 1500/2250, Training loss NN: 0.62533.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 44/50, Batch: 2000/2250, Training loss NN: 0.73326.\n",
            " Y_hat_nn-6.571945,Y_true-6.839570 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 44,  Val NN MSE/NMSE: 0.37707/183.17484.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 45/50, Batch: 500/2250, Training loss NN: 0.95460.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 45/50, Batch: 1000/2250, Training loss NN: 0.64491.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 45/50, Batch: 1500/2250, Training loss NN: 0.62033.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 45/50, Batch: 2000/2250, Training loss NN: 0.70938.\n",
            " Y_hat_nn-10.546303,Y_true-10.755720 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 45,  Val NN MSE/NMSE: 0.37872/176.73273.\n",
            "Learning rate reduced to 0.00000.\n",
            "Current LR: 1.0000000000000002e-07\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 46/50, Batch: 500/2250, Training loss NN: 0.95772.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 46/50, Batch: 1000/2250, Training loss NN: 0.71472.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 46/50, Batch: 1500/2250, Training loss NN: 0.59321.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 46/50, Batch: 2000/2250, Training loss NN: 0.84868.\n",
            " Y_hat_nn-8.755707,Y_true-7.880730 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 46,  Val NN MSE/NMSE: 0.37744/177.81224.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 47/50, Batch: 500/2250, Training loss NN: 1.02377.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 47/50, Batch: 1000/2250, Training loss NN: 0.63494.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 47/50, Batch: 1500/2250, Training loss NN: 0.62970.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 47/50, Batch: 2000/2250, Training loss NN: 0.74487.\n",
            " Y_hat_nn-16.640091,Y_true-16.658978 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 47,  Val NN MSE/NMSE: 0.37777/183.29930.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 48/50, Batch: 500/2250, Training loss NN: 0.92546.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 48/50, Batch: 1000/2250, Training loss NN: 0.72087.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 48/50, Batch: 1500/2250, Training loss NN: 0.54671.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 48/50, Batch: 2000/2250, Training loss NN: 0.73056.\n",
            " Y_hat_nn-6.839859,Y_true-7.013726 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 48,  Val NN MSE/NMSE: 0.37824/178.07480.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 49/50, Batch: 500/2250, Training loss NN: 0.89856.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 49/50, Batch: 1000/2250, Training loss NN: 0.60367.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 49/50, Batch: 1500/2250, Training loss NN: 0.56442.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 49/50, Batch: 2000/2250, Training loss NN: 0.75330.\n",
            " Y_hat_nn-12.425774,Y_true-11.994923 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 49,  Val NN MSE/NMSE: 0.37772/177.04478.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 50/50, Batch: 500/2250, Training loss NN: 0.93441.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 50/50, Batch: 1000/2250, Training loss NN: 0.66261.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 50/50, Batch: 1500/2250, Training loss NN: 0.58066.\n",
            "Repeat: 0, TrSize: 9000000, Epoch: 50/50, Batch: 2000/2250, Training loss NN: 0.68185.\n",
            " Y_hat_nn-8.987518,Y_true-9.060048 \n",
            "Repeat: 0, TrSize: 9000000, Epoch: 50,  Val NN MSE/NMSE: 0.37846/177.63013.\n",
            "[[ -7.0841571 ]\n",
            " [ -0.33378968]\n",
            " [ -3.15535775]\n",
            " ...\n",
            " [ -8.46336433]\n",
            " [-18.02666187]\n",
            " [ -6.20719265]] [[ -7.215233 ]\n",
            " [ -0.6862488]\n",
            " [ -3.1083527]\n",
            " ...\n",
            " [ -8.399261 ]\n",
            " [-18.943748 ]\n",
            " [ -6.0753555]]\n",
            " Val NN: -2.0250954948075592\n"
          ]
        }
      ],
      "source": [
        "#first time rub with 1M\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optimizer\n",
        "import numpy as np\n",
        "import scipy.io as scio\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def model_pre_eval(model, data_path):\n",
        "\n",
        "    with h5py.File(data_path, 'r') as f:\n",
        "        # Access the dataset within the file\n",
        "        ph_vec = np.array(f['Phase_dataset'])\n",
        "        gain_vec1 = np.array(f['Gain'])  # Replace 'SIR' with the actual dataset key\n",
        "\n",
        "    #gain_vec1 = scio.loadmat(data_path)['Gain']\n",
        "    #ph_vec = scio.loadmat(data_path)['Phase_dataset']\n",
        "\n",
        "\n",
        "    azim_min = -148\n",
        "    azim_max = -25\n",
        "    elev_min = -58\n",
        "    elev_max = -17\n",
        "    dist_min = 10\n",
        "    dist_max = 29\n",
        "    x_min = 35\n",
        "    x_max = 65\n",
        "    y_min = -25\n",
        "    y_max = -5\n",
        "\n",
        "    ph_vec[:,0] = (ph_vec[:,0] - x_min)/(x_max- x_min)\n",
        "    ph_vec[:,1] = (ph_vec[:,1] - y_min)/(y_max- y_min)\n",
        "    ph_vec[:,2] = (ph_vec[:,2] - azim_min)/(azim_max- azim_min)\n",
        "    ph_vec[:,3] = (ph_vec[:,3] - elev_min)/(elev_max- elev_min)\n",
        "    ph_vec[:,4] = (ph_vec[:,4] - dist_min)/(dist_max- dist_min)\n",
        "    gain_vec = gain_vec1 #(gain_vec1-y_min)/(y_max - y_min)\n",
        "\n",
        "    x = ph_vec #.transpose()\n",
        "\n",
        "\n",
        "\n",
        "    y_true = gain_vec #.transpose()\n",
        "\n",
        "    num_of_sample = x.shape[0]\n",
        "    y_pr = torch.zeros((num_of_sample, 1))\n",
        "\n",
        "    model.eval()\n",
        "    model.cuda()\n",
        "\n",
        "    batch_size = 10000\n",
        "\n",
        "    batch_per_epoch = np.ceil(np.divide(num_of_sample, batch_size))\n",
        "    batch_count = 0\n",
        "\n",
        "    while batch_count < batch_per_epoch:\n",
        "        start = batch_count * batch_size\n",
        "        end = np.minimum(start + batch_size, num_of_sample)\n",
        "        batch_count += 1\n",
        "\n",
        "        X = torch.tensor(x[start:end, :] )#.cuda()\n",
        "        Y = torch.tensor(y_true[start:end, 0:1]) #.cuda()\n",
        "\n",
        "        Y_hat = model(X.float().cuda() )\n",
        "        y_pr[start:end, :] = Y_hat\n",
        "\n",
        "\n",
        "    y_pred = torch.Tensor.cpu(y_pr.detach()).numpy()\n",
        "    print(y_true,y_pred)\n",
        "    performance = 10 * np.log10(np.mean(np.abs(y_pred - y_true) ** 2))\n",
        "\n",
        "    return performance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import scipy.io as scio\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class NN_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NN_model, self).__init__()\n",
        "\n",
        "        self.M = 36\n",
        "        self.input = 77\n",
        "        self.J = 8 * self.input\n",
        "        self.K = 6 * self.input\n",
        "        self.L = 4 * self.input\n",
        "        self.N = 2 * self.input\n",
        "\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(self.input, self.J)  # 77 -> 77*8\n",
        "        self.bn1 = nn.BatchNorm1d(self.J)\n",
        "        self.fc2 = nn.Linear(self.J, self.K)      # 77*8 -> 77*6\n",
        "        self.bn2 = nn.BatchNorm1d(self.K)\n",
        "        self.fc3 = nn.Linear(self.K, self.K)      # 77*6 -> 77*6\n",
        "        self.bn3 = nn.BatchNorm1d(self.K)\n",
        "        self.fc4 = nn.Linear(self.K, self.L)      # 77*6 -> 77*4\n",
        "        self.bn4 = nn.BatchNorm1d(self.L)\n",
        "        self.fc5 = nn.Linear(self.L, self.L)      # 77*4 -> 77*4\n",
        "        self.bn5 = nn.BatchNorm1d(self.L)\n",
        "        self.fc6 = nn.Linear(self.L, self.N)      # 77*4 -> 77*2\n",
        "        self.bn6 = nn.BatchNorm1d(self.N)\n",
        "        self.fc7 = nn.Linear(self.N, 1)          # 77*2 -> 1\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        # Apply He initialization\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                # He initialization\n",
        "                nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
        "                if layer.bias is not None:\n",
        "                    nn.init.zeros_(layer.bias)\n",
        "\n",
        "    def forward(self, ph):\n",
        "        device = ph.device\n",
        "        bf_vec = self.phase2bf(ph[:, 5:]).to(device)\n",
        "        bf_vec_r, bf_vec_i = bf_vec[:, :self.M], bf_vec[:, self.M:]\n",
        "        bf_vec_cat = torch.cat((bf_vec_r, bf_vec_i), dim=1)\n",
        "        bf_vec = torch.cat((ph[:, :5].real, bf_vec_cat), 1)\n",
        "\n",
        "        x = torch.relu(self.bn1(self.fc1(bf_vec)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn5(self.fc5(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn6(self.fc6(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        y_min = -90\n",
        "        y_max = 35\n",
        "        out = torch.sigmoid(self.fc7(x)) * (y_max - y_min) + y_min\n",
        "        return out\n",
        "\n",
        "    def phase2bf(self, ph_mat):\n",
        "        bf_mat = torch.exp(1j * ph_mat)\n",
        "        bf_mat_r = torch.real(bf_mat)\n",
        "        bf_mat_i = torch.imag(bf_mat)\n",
        "        bf_mat_ = torch.cat((bf_mat_r, bf_mat_i), dim=1)\n",
        "        return bf_mat_\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    options = {\n",
        "        'gpu_idx': 0,\n",
        "        'num_ant': 36,\n",
        "        'mode': 'signal'\n",
        "    }\n",
        "    print(options['mode'])\n",
        "    file_path = './drive/MyDrive/Training_set_10000000_0.mat'\n",
        "\n",
        "    with h5py.File(file_path, 'r') as f:\n",
        "        # Access the dataset within the file\n",
        "        ph_vec = np.array(f['Phase_dataset'])\n",
        "\n",
        "        gain_vec1 = np.array(f['Gain'])  # Replace 'SIR' with the actual dataset key\n",
        "    azim_min = -148\n",
        "    azim_max = -25\n",
        "    elev_min = -58\n",
        "    elev_max = -17\n",
        "    dist_min = 10\n",
        "    dist_max = 29\n",
        "    x_min = 35\n",
        "    x_max = 65\n",
        "    y_min = -25\n",
        "    y_max = -5\n",
        "\n",
        "    ph_vec[:,0] = (ph_vec[:,0] - x_min)/(x_max- x_min)\n",
        "    ph_vec[:,1] = (ph_vec[:,1] - y_min)/(y_max- y_min)\n",
        "    ph_vec[:,2] = (ph_vec[:,2] - azim_min)/(azim_max- azim_min)\n",
        "    ph_vec[:,3] = (ph_vec[:,3] - elev_min)/(elev_max- elev_min)\n",
        "    ph_vec[:,4] = (ph_vec[:,4] - dist_min)/(dist_max- dist_min)\n",
        "    #gain_vec1 = scio.loadmat(file_path)['Gain']\n",
        "    #ph_vec = scio.loadmat(file_path)['Phase_dataset']\n",
        "    y_min = -87\n",
        "    y_max = 33\n",
        "    azim_min = -148\n",
        "    azim_max = -25\n",
        "    elev_min = -58\n",
        "    elev_max = -18\n",
        "    dist_min = 10\n",
        "    dist_max = 29\n",
        "    gain_vec = gain_vec1 #(gain_vec1-y_min)/(y_max - y_min)\n",
        "    x = ph_vec #.transpose()\n",
        "    y = gain_vec #.transpose()\n",
        "    num_of_sample = x.shape[0]\n",
        "    num_tr = 9000000\n",
        "    repeat_idx = 0\n",
        "\n",
        "\n",
        "    shuffle_ind = np.random.permutation(num_of_sample)\n",
        "    x_shuffled = x[shuffle_ind]\n",
        "    y_shuffled = y[shuffle_ind]\n",
        "\n",
        "\n",
        "\n",
        "    dataset = {'x_train': torch.from_numpy(x_shuffled[:num_tr, :]).float(),\n",
        "               'y_train': torch.from_numpy(y_shuffled[:num_tr, :]).float(),\n",
        "               'x_test': torch.from_numpy(x_shuffled[num_tr:, :]).float(),\n",
        "               'y_test': torch.from_numpy(y_shuffled[num_tr:, :]).float()}\n",
        "\n",
        "    model_nn = NN_model()\n",
        "    model_nn.load_state_dict(torch.load('/content/drive/MyDrive/model_based_5M_params_nn_M_36_signal_tr_size_4500000_repeat_02.pt'))\n",
        "\n",
        "    opt_nn = optimizer.Adam(model_nn.parameters(), lr=1e-4,weight_decay=1e-4)\n",
        "    #scheduler_nn = optimizer.lr_scheduler.MultiStepLR(opt_nn, [3,15,65,95,150,170, 190], gamma=0.1, last_epoch=-1)\n",
        "    scheduler_nn = optimizer.lr_scheduler.MultiStepLR(opt_nn, [5,25,45], gamma=0.1, last_epoch=-1)\n",
        "\n",
        "    num_of_epoch = 50\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    batch_size = 4000\n",
        "    val_loss_list_nn = []\n",
        "    val_loss_nmse_nn = []\n",
        "\n",
        "    with torch.cuda.device('cuda:' + str(options['gpu_idx'])):\n",
        "\n",
        "        model_nn.cuda()\n",
        "\n",
        "        for epoch in range(num_of_epoch):\n",
        "\n",
        "            batch_count = 0\n",
        "            batch_per_epoch = np.ceil(np.divide(dataset['x_train'].size(0), batch_size)).astype('int32')\n",
        "\n",
        "            model_nn.train()\n",
        "\n",
        "            while batch_count < batch_per_epoch:\n",
        "\n",
        "                start = batch_count * batch_size\n",
        "                end = np.minimum(start + batch_size, dataset['x_train'].size(0))\n",
        "                end_ph = end-start\n",
        "                batch_count += 1\n",
        "\n",
        "                X = dataset['x_train'][start:end, :].cuda()\n",
        "                Y = dataset['y_train'][start:end, 0:1].cuda()\n",
        "\n",
        "                Y_hat_nn = model_nn(X)\n",
        "                loss_nn  = criterion(Y, Y_hat_nn)\n",
        "                opt_nn.zero_grad()\n",
        "                loss_nn.backward()\n",
        "                nn.utils.clip_grad_value_(model_nn.parameters(), clip_value=0.1)\n",
        "                opt_nn.step()\n",
        "\n",
        "                if batch_count % 500 == 0:\n",
        "                    print(\"Repeat: {:d}, TrSize: {:d}, Epoch: {:d}/{:d}, Batch: {:d}/{:d}, Training loss NN: {:.5f}.\".format(repeat_idx, num_tr, epoch + 1, num_of_epoch, batch_count, batch_per_epoch,  loss_nn.item()))\n",
        "\n",
        "            model_nn.eval()\n",
        "            batch_per_epoch = np.ceil(np.divide(dataset['x_test'].size(0), batch_size))\n",
        "            batch_count = 0\n",
        "            val_loss_nn = 0\n",
        "            pred_nn = torch.zeros((dataset['y_test'].size(0), dataset['y_test'].size(1)))\n",
        "            while batch_count < batch_per_epoch:\n",
        "                start = batch_count * batch_size\n",
        "                end = np.minimum(start + batch_size, dataset['x_test'].size(0))\n",
        "                batch_count += 1\n",
        "\n",
        "                X_true = dataset['x_test'][start:end, :].cuda()\n",
        "                Y_true = dataset['y_test'][start:end, 0:1].cuda()\n",
        "                Y_hat_nn = model_nn(X_true)\n",
        "                loss_nn  = criterion(Y_true, Y_hat_nn)\n",
        "                val_loss_nn += loss_nn.item()\n",
        "\n",
        "                pred_nn[start:end, :] = Y_hat_nn.detach().clone()\n",
        "            index = torch.randint(0, 500, (1,)).item()\n",
        "            print( \" Y_hat_nn{:.6f},Y_true{:.6f} \".format(  Y_hat_nn[index].item(),Y_true[index].item() ))\n",
        "            val_loss_list_nn.append(val_loss_nn / batch_per_epoch)\n",
        "\n",
        "            nmse_nn = torch.mean(torch.div(torch.pow(torch.abs(dataset['y_test'] - pred_nn), 2), torch.pow(torch.abs(dataset['y_test']), 2)))\n",
        "            val_loss_nmse_nn.append(nmse_nn)\n",
        "            print(\"Repeat: {:d}, TrSize: {:d}, Epoch: {:d},  Val NN MSE/NMSE: {:.5f}/{:.5f}.\".format(repeat_idx, num_tr, epoch + 1,val_loss_nn / batch_per_epoch, nmse_nn))\n",
        "\n",
        "\n",
        "            current_lr = scheduler_nn.get_last_lr()[0]\n",
        "            scheduler_nn.step()\n",
        "            new_lr = scheduler_nn.get_last_lr()[0]\n",
        "\n",
        "            if current_lr != new_lr:\n",
        "                print(\"Learning rate reduced to {0:.5f}.\".format(new_lr))\n",
        "                for param_group in opt_nn.param_groups:\n",
        "                    print(\"Current LR:\",param_group['lr'])\n",
        "\n",
        "\n",
        "\n",
        "    model_save_path_nn = './drive/MyDrive//model_based_10M_params_nn_M_' + str(options['num_ant']) + '_' + options['mode'] + '_tr_size_' + str(num_tr) + '_repeat_' + str(repeat_idx) + '.pt'\n",
        "    torch.save(model_nn.state_dict(), model_save_path_nn)\n",
        "    val = model_pre_eval(model_nn, './drive/MyDrive/Testing_set_100000_1.mat')\n",
        "    print(\" Val NN:\", val)\n",
        "    #first time run with 1M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZVtsPsHJS7b",
        "outputId": "51c24036-b20e-4449-ddaf-399842fb99b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIo4DnONUQFA"
      },
      "source": [
        "RL Digital twin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0TWwldgWRip",
        "outputId": "de742b5e-fda2-46a9-b363-c9dfb3f223fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mColab_Notebooks\u001b[0m/\n",
            " model_based_0.5M_params_nn_M_36_signal_tr_size_4500000_repeat_02.pt\n",
            " pr_diary.pdf\n",
            " Test_set_100000.mat\n",
            " Training_set_10000000_0.mat\n",
            " Training_set_1000000_0.mat\n",
            " Training_set_100000_0.mat\n",
            " Training_set_11000000_11.mat\n",
            " Training_set_11000000_12.mat\n",
            " Training_set_11000000_13.mat\n",
            " Training_set_11000000_14.mat\n",
            " Training_set_11000000_15.mat\n",
            " Training_set_11000000_16.mat\n",
            " Training_set_11000000_17.mat\n",
            " Training_set_11000000_18.mat\n",
            " Training_set_11000000_1.mat\n",
            " Training_set_11000000_2.mat\n",
            " Training_set_11000000_3.mat\n",
            " Training_set_11000000_4.mat\n",
            " Training_set_5000000_0.mat\n",
            " Training_set_500000_0.mat\n",
            "'Video kuzatuv orqali odamlar sonini sanash.pptx'\n"
          ]
        }
      ],
      "source": [
        "%ls drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQOH95WFwe3A",
        "outputId": "b2dbef5b-e219-49e9-b1b3-9bb7d476d986"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[11.4359]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "azim_min = -148\n",
        "azim_max = -25\n",
        "elev_min = -58\n",
        "elev_max = -17\n",
        "dist_min = 10\n",
        "dist_max = 29\n",
        "x_min = 35\n",
        "x_max = 65\n",
        "y_min = -25\n",
        "y_max = -5\n",
        "\n",
        "to_test = [42.1280,\n",
        "   -7.9337,\n",
        " -134.7764,\n",
        "  -37.2540,\n",
        "   14.0415,\n",
        "      0,\n",
        "         0,\n",
        "   -1.5708,\n",
        "   -3.1416,\n",
        "    1.5708,\n",
        "    1.5708,\n",
        "   -1.5708,\n",
        "    1.5708,\n",
        "   -1.5708,\n",
        "    1.5708,\n",
        "    1.5708,\n",
        "   -3.1416,\n",
        "   -1.5708,\n",
        "   -3.1416,\n",
        "    1.5708,\n",
        "   -3.1416,\n",
        "    1.5708,\n",
        "   -1.5708,\n",
        "   -1.5708,\n",
        "   -1.5708,\n",
        "    1.5708,\n",
        "   -1.5708,\n",
        "         0,\n",
        "    1.5708,\n",
        "   -3.1416,\n",
        "         0,\n",
        "   -1.5708,\n",
        "    1.5708,\n",
        "   -3.1416,\n",
        "    1.5708,\n",
        "         0,\n",
        "    1.5708,\n",
        "   -3.1416,\n",
        "   -1.5708,\n",
        "   -1.5708,\n",
        "   -1.5708]\n",
        "to_test[0] = (to_test[0] - x_min)/(x_max- x_min)\n",
        "to_test[1] = (to_test[1] - y_min)/(y_max- y_min)\n",
        "to_test[2] = (to_test[2] - azim_min)/(azim_max- azim_min)\n",
        "to_test[3] = (to_test[3] - elev_min)/(elev_max- elev_min)\n",
        "to_test[4] = (to_test[4] - dist_min)/(dist_max- dist_min)\n",
        "model_train.eval()\n",
        "model_train.cuda()\n",
        "model_train(torch.tensor(to_test).reshape(1,-1).float().cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Zp5cS51eO75",
        "outputId": "b8489d8f-bf90-4d3b-b181-caa989a9618d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "main\n",
            "options 26\n",
            "Beam 10 training begins. GPU being used: 0\n",
            "GainPred_Dense(\n",
            "  (fc1): Linear(in_features=77, out_features=616, bias=True)\n",
            "  (bn1): BatchNorm1d(616, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dropout): Dropout(p=0.001, inplace=False)\n",
            "  (fc2): Linear(in_features=616, out_features=462, bias=True)\n",
            "  (bn2): BatchNorm1d(462, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=462, out_features=462, bias=True)\n",
            "  (bn3): BatchNorm1d(462, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc4): Linear(in_features=462, out_features=308, bias=True)\n",
            "  (bn4): BatchNorm1d(308, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc5): Linear(in_features=308, out_features=308, bias=True)\n",
            "  (bn5): BatchNorm1d(308, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc6): Linear(in_features=308, out_features=154, bias=True)\n",
            "  (bn6): BatchNorm1d(154, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc7): Linear(in_features=154, out_features=1, bias=True)\n",
            ")\n",
            "Initial State Activated.\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2\n",
            "Tr: 5.00,Beam: 10, Iteration: 2, Q value: 0.0097, Reward: -1.0000, BF Gain pred: -11.71, BF Gain: -9.13, BFtrue Gain pred: -6.72, BFtrue Gain: -9.18,Critic Loss: 1.00, Policy Loss: -0.01\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 4\n",
            "Tr: 5.00,Beam: 10, Iteration: 3, Q value: 0.0087, Reward: -1.0000, BF Gain pred: -11.71, BF Gain: -10.71, BFtrue Gain pred: -7.00, BFtrue Gain: -10.46,Critic Loss: 0.99, Policy Loss: -0.01\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-740d74dfd9f5>:362: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  real_time_perf[iteration] = torch.Tensor.cpu(bf_gain_pred.detach()).numpy()\n",
            "<ipython-input-3-740d74dfd9f5>:363: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  real_time_perf_n[iteration] = torch.Tensor.cpu(bf_gain.detach()).numpy()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Length of the buffer: 656\n",
            "Tr: 10.00,Beam: 17, Iteration: 329, Q value: 0.0395, Reward: -1.0000, BF Gain pred: -3.06, BF Gain: -7.42, BFtrue Gain pred: -2.95, BFtrue Gain: -6.82,Critic Loss: 0.54, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 658\n",
            "Tr: 10.00,Beam: 17, Iteration: 330, Q value: 0.0400, Reward: -1.0000, BF Gain pred: -3.06, BF Gain: -14.71, BFtrue Gain pred: -3.39, BFtrue Gain: -13.70,Critic Loss: 0.52, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 660\n",
            "Tr: 10.00,Beam: 17, Iteration: 331, Q value: 0.0405, Reward: -1.0000, BF Gain pred: -3.06, BF Gain: -12.56, BFtrue Gain pred: -3.88, BFtrue Gain: -11.73,Critic Loss: 0.50, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 662\n",
            "Tr: 10.00,Beam: 17, Iteration: 332, Q value: 0.0410, Reward: -1.0000, BF Gain pred: -4.53, BF Gain: -15.60, BFtrue Gain pred: -4.30, BFtrue Gain: -15.55,Critic Loss: 0.52, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 664\n",
            "Tr: 10.00,Beam: 17, Iteration: 333, Q value: 0.0415, Reward: -1.0000, BF Gain pred: -4.53, BF Gain: -13.71, BFtrue Gain pred: -3.94, BFtrue Gain: -13.96,Critic Loss: 0.51, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 666\n",
            "Tr: 10.00,Beam: 17, Iteration: 334, Q value: 0.0420, Reward: -1.0000, BF Gain pred: -4.53, BF Gain: -11.29, BFtrue Gain pred: -4.28, BFtrue Gain: -10.78,Critic Loss: 0.54, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 668\n",
            "Tr: 10.00,Beam: 17, Iteration: 335, Q value: 0.0424, Reward: 1.0000, BF Gain pred: -1.88, BF Gain: -6.83, BFtrue Gain pred: -4.47, BFtrue Gain: -6.72,Critic Loss: 0.53, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 670\n",
            "Tr: 10.00,Beam: 17, Iteration: 336, Q value: -0.0063, Reward: -1.0000, BF Gain pred: -1.88, BF Gain: -5.17, BFtrue Gain pred: -4.79, BFtrue Gain: -4.42,Critic Loss: 0.52, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 672\n",
            "Tr: 10.00,Beam: 17, Iteration: 337, Q value: -0.0098, Reward: -1.0000, BF Gain pred: -1.88, BF Gain: -0.35, BFtrue Gain pred: -4.95, BFtrue Gain: 0.33,Critic Loss: 0.52, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 674\n",
            "Tr: 10.00,Beam: 17, Iteration: 338, Q value: -0.0736, Reward: -1.0000, BF Gain pred: -5.63, BF Gain: 0.54, BFtrue Gain pred: -6.95, BFtrue Gain: 1.09,Critic Loss: 0.50, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 676\n",
            "Tr: 10.00,Beam: 17, Iteration: 339, Q value: -0.0397, Reward: -1.0000, BF Gain pred: -5.63, BF Gain: -3.16, BFtrue Gain pred: -7.68, BFtrue Gain: -3.44,Critic Loss: 0.50, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 678\n",
            "Tr: 10.00,Beam: 17, Iteration: 340, Q value: -0.0392, Reward: -1.0000, BF Gain pred: -5.63, BF Gain: -0.12, BFtrue Gain pred: -7.91, BFtrue Gain: -1.65,Critic Loss: 0.49, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 680\n",
            "Tr: 10.00,Beam: 17, Iteration: 341, Q value: -0.0827, Reward: -1.0000, BF Gain pred: -6.83, BF Gain: -4.18, BFtrue Gain pred: -8.12, BFtrue Gain: -4.28,Critic Loss: 0.55, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 682\n",
            "Tr: 10.00,Beam: 17, Iteration: 342, Q value: 0.0256, Reward: 1.0000, BF Gain pred: -5.48, BF Gain: -6.41, BFtrue Gain pred: -7.91, BFtrue Gain: -5.92,Critic Loss: 0.53, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 684\n",
            "Tr: 10.00,Beam: 17, Iteration: 343, Q value: 0.0368, Reward: -1.0000, BF Gain pred: -5.48, BF Gain: -2.33, BFtrue Gain pred: -8.30, BFtrue Gain: -2.79,Critic Loss: 0.51, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 686\n",
            "Tr: 10.00,Beam: 17, Iteration: 344, Q value: 0.0386, Reward: -1.0000, BF Gain pred: -5.48, BF Gain: -10.22, BFtrue Gain pred: -8.59, BFtrue Gain: -10.68,Critic Loss: 0.50, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 688\n",
            "Tr: 10.00,Beam: 17, Iteration: 345, Q value: 0.0472, Reward: 1.0000, BF Gain pred: -1.55, BF Gain: -11.39, BFtrue Gain pred: -8.98, BFtrue Gain: -11.44,Critic Loss: 0.51, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 690\n",
            "Tr: 10.00,Beam: 17, Iteration: 346, Q value: -0.0202, Reward: -1.0000, BF Gain pred: -1.55, BF Gain: -14.69, BFtrue Gain pred: -10.91, BFtrue Gain: -15.65,Critic Loss: 0.53, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 692\n",
            "Tr: 10.00,Beam: 17, Iteration: 347, Q value: -0.0633, Reward: -1.0000, BF Gain pred: -1.67, BF Gain: -10.68, BFtrue Gain pred: -16.04, BFtrue Gain: -10.60,Critic Loss: 0.49, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 694\n",
            "Tr: 10.00,Beam: 17, Iteration: 348, Q value: -0.0327, Reward: -1.0000, BF Gain pred: -2.34, BF Gain: -8.18, BFtrue Gain pred: -8.69, BFtrue Gain: -8.89,Critic Loss: 0.51, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 696\n",
            "Tr: 10.00,Beam: 17, Iteration: 349, Q value: -0.0838, Reward: -1.0000, BF Gain pred: -2.34, BF Gain: -7.08, BFtrue Gain pred: -6.48, BFtrue Gain: -7.91,Critic Loss: 0.53, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 698\n",
            "Tr: 10.00,Beam: 17, Iteration: 350, Q value: -0.0953, Reward: -1.0000, BF Gain pred: -2.34, BF Gain: -11.95, BFtrue Gain pred: -5.09, BFtrue Gain: -11.55,Critic Loss: 0.53, Policy Loss: -0.04\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 700\n",
            "Tr: 10.00,Beam: 17, Iteration: 351, Q value: -0.1603, Reward: -1.0000, BF Gain pred: -2.34, BF Gain: -15.56, BFtrue Gain pred: -4.72, BFtrue Gain: -14.87,Critic Loss: 0.55, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 702\n",
            "Tr: 10.00,Beam: 17, Iteration: 352, Q value: -0.2742, Reward: -1.0000, BF Gain pred: -5.35, BF Gain: -4.27, BFtrue Gain pred: -3.97, BFtrue Gain: -5.09,Critic Loss: 0.52, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 704\n",
            "Tr: 10.00,Beam: 17, Iteration: 353, Q value: -0.3368, Reward: -1.0000, BF Gain pred: -5.35, BF Gain: -6.35, BFtrue Gain pred: -3.77, BFtrue Gain: -6.99,Critic Loss: 0.53, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 706\n",
            "Tr: 10.00,Beam: 17, Iteration: 354, Q value: -0.2526, Reward: 1.0000, BF Gain pred: 0.50, BF Gain: -2.60, BFtrue Gain pred: -5.50, BFtrue Gain: -2.80,Critic Loss: 0.54, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 708\n",
            "Tr: 10.00,Beam: 17, Iteration: 355, Q value: -0.2744, Reward: 1.0000, BF Gain pred: 3.57, BF Gain: -1.62, BFtrue Gain pred: -6.49, BFtrue Gain: -2.68,Critic Loss: 0.53, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 710\n",
            "Tr: 10.00,Beam: 17, Iteration: 356, Q value: -0.2623, Reward: -1.0000, BF Gain pred: 0.84, BF Gain: -5.03, BFtrue Gain pred: -7.25, BFtrue Gain: -5.66,Critic Loss: 0.50, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 712\n",
            "Tr: 10.00,Beam: 17, Iteration: 357, Q value: -0.2423, Reward: 1.0000, BF Gain pred: 1.55, BF Gain: -4.72, BFtrue Gain pred: -11.87, BFtrue Gain: -5.04,Critic Loss: 0.56, Policy Loss: -0.02\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 714\n",
            "Tr: 10.00,Beam: 17, Iteration: 358, Q value: -0.2915, Reward: -1.0000, BF Gain pred: -2.15, BF Gain: -4.10, BFtrue Gain pred: -6.26, BFtrue Gain: -3.51,Critic Loss: 0.51, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 716\n",
            "Tr: 10.00,Beam: 17, Iteration: 359, Q value: -0.5593, Reward: -1.0000, BF Gain pred: -10.84, BF Gain: -0.17, BFtrue Gain pred: 2.58, BFtrue Gain: -0.59,Critic Loss: 0.52, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 718\n",
            "Tr: 10.00,Beam: 17, Iteration: 360, Q value: -0.1745, Reward: 1.0000, BF Gain pred: -9.12, BF Gain: -2.15, BFtrue Gain pred: 4.11, BFtrue Gain: -1.92,Critic Loss: 0.53, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 720\n",
            "Tr: 10.00,Beam: 17, Iteration: 361, Q value: -0.2325, Reward: 1.0000, BF Gain pred: -7.26, BF Gain: -9.35, BFtrue Gain pred: 1.95, BFtrue Gain: -9.29,Critic Loss: 0.50, Policy Loss: -0.05\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 722\n",
            "Tr: 10.00,Beam: 17, Iteration: 362, Q value: -0.1713, Reward: -1.0000, BF Gain pred: -12.09, BF Gain: -6.21, BFtrue Gain pred: -1.86, BFtrue Gain: -6.24,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 724\n",
            "Tr: 10.00,Beam: 17, Iteration: 363, Q value: -0.0387, Reward: -1.0000, BF Gain pred: -12.09, BF Gain: 1.16, BFtrue Gain pred: 0.48, BFtrue Gain: 3.39,Critic Loss: 0.51, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 726\n",
            "Tr: 10.00,Beam: 17, Iteration: 364, Q value: 0.0559, Reward: 1.0000, BF Gain pred: -10.29, BF Gain: -1.75, BFtrue Gain pred: -1.08, BFtrue Gain: -1.35,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 728\n",
            "Tr: 10.00,Beam: 17, Iteration: 365, Q value: 0.0564, Reward: -1.0000, BF Gain pred: -12.04, BF Gain: -9.01, BFtrue Gain pred: -2.05, BFtrue Gain: -8.84,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 730\n",
            "Tr: 10.00,Beam: 17, Iteration: 366, Q value: 0.0569, Reward: 1.0000, BF Gain pred: -8.99, BF Gain: -1.16, BFtrue Gain pred: -5.83, BFtrue Gain: -1.38,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 732\n",
            "Tr: 10.00,Beam: 17, Iteration: 367, Q value: 0.0573, Reward: 1.0000, BF Gain pred: -6.73, BF Gain: -16.06, BFtrue Gain pred: -7.87, BFtrue Gain: -15.59,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 734\n",
            "Tr: 10.00,Beam: 17, Iteration: 368, Q value: 0.0578, Reward: -1.0000, BF Gain pred: -10.38, BF Gain: 1.04, BFtrue Gain pred: -8.95, BFtrue Gain: 0.96,Critic Loss: 0.48, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 736\n",
            "Tr: 10.00,Beam: 17, Iteration: 369, Q value: 0.0582, Reward: 1.0000, BF Gain pred: -4.28, BF Gain: -2.16, BFtrue Gain pred: -7.11, BFtrue Gain: -2.15,Critic Loss: 0.48, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 738\n",
            "Tr: 10.00,Beam: 17, Iteration: 370, Q value: 0.0587, Reward: -1.0000, BF Gain pred: -6.52, BF Gain: -6.30, BFtrue Gain pred: -8.46, BFtrue Gain: -6.34,Critic Loss: 0.47, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 740\n",
            "Tr: 10.00,Beam: 17, Iteration: 371, Q value: 0.0592, Reward: -1.0000, BF Gain pred: -10.27, BF Gain: -3.89, BFtrue Gain pred: -10.71, BFtrue Gain: -3.25,Critic Loss: 0.48, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 742\n",
            "Tr: 10.00,Beam: 17, Iteration: 372, Q value: 0.0597, Reward: 1.0000, BF Gain pred: -4.28, BF Gain: 2.80, BFtrue Gain pred: -10.44, BFtrue Gain: 2.81,Critic Loss: 0.50, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 744\n",
            "Tr: 10.00,Beam: 17, Iteration: 373, Q value: 0.0602, Reward: -1.0000, BF Gain pred: -14.37, BF Gain: -7.74, BFtrue Gain pred: -12.34, BFtrue Gain: -7.53,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 746\n",
            "Tr: 10.00,Beam: 17, Iteration: 374, Q value: 0.0607, Reward: 1.0000, BF Gain pred: -4.28, BF Gain: -7.58, BFtrue Gain pred: -12.69, BFtrue Gain: -7.69,Critic Loss: 0.48, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 748\n",
            "Tr: 10.00,Beam: 17, Iteration: 375, Q value: 0.0612, Reward: -1.0000, BF Gain pred: -9.87, BF Gain: -3.21, BFtrue Gain pred: -14.98, BFtrue Gain: -2.96,Critic Loss: 0.47, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 750\n",
            "Tr: 10.00,Beam: 17, Iteration: 376, Q value: 0.0617, Reward: 1.0000, BF Gain pred: -3.41, BF Gain: -4.93, BFtrue Gain pred: -9.24, BFtrue Gain: -4.64,Critic Loss: 0.49, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 752\n",
            "Tr: 10.00,Beam: 17, Iteration: 377, Q value: 0.0622, Reward: -1.0000, BF Gain pred: -9.87, BF Gain: -4.78, BFtrue Gain pred: -14.20, BFtrue Gain: -4.49,Critic Loss: 0.51, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 754\n",
            "Tr: 10.00,Beam: 17, Iteration: 378, Q value: 0.0627, Reward: 1.0000, BF Gain pred: -3.41, BF Gain: -1.79, BFtrue Gain pred: -12.57, BFtrue Gain: -2.58,Critic Loss: 0.45, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 756\n",
            "Tr: 10.00,Beam: 17, Iteration: 379, Q value: 0.0633, Reward: -1.0000, BF Gain pred: -9.87, BF Gain: -4.71, BFtrue Gain pred: -16.47, BFtrue Gain: -3.84,Critic Loss: 0.48, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 758\n",
            "Tr: 10.00,Beam: 17, Iteration: 380, Q value: 0.0638, Reward: 1.0000, BF Gain pred: -3.41, BF Gain: -4.06, BFtrue Gain pred: -15.95, BFtrue Gain: -4.02,Critic Loss: 0.46, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 760\n",
            "Tr: 10.00,Beam: 17, Iteration: 381, Q value: 0.0643, Reward: -1.0000, BF Gain pred: -9.87, BF Gain: -7.58, BFtrue Gain pred: -18.56, BFtrue Gain: -7.18,Critic Loss: 0.46, Policy Loss: -0.06\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 762\n",
            "Tr: 10.00,Beam: 17, Iteration: 382, Q value: 0.0649, Reward: 1.0000, BF Gain pred: -3.89, BF Gain: -9.13, BFtrue Gain pred: -19.68, BFtrue Gain: -7.84,Critic Loss: 0.47, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 764\n",
            "Tr: 10.00,Beam: 17, Iteration: 383, Q value: 0.0654, Reward: -1.0000, BF Gain pred: -3.89, BF Gain: -5.82, BFtrue Gain pred: -18.79, BFtrue Gain: -5.50,Critic Loss: 0.50, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 766\n",
            "Tr: 10.00,Beam: 17, Iteration: 384, Q value: 0.0659, Reward: -1.0000, BF Gain pred: -3.89, BF Gain: -1.06, BFtrue Gain pred: -20.30, BFtrue Gain: -0.93,Critic Loss: 0.45, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 768\n",
            "Tr: 10.00,Beam: 17, Iteration: 385, Q value: 0.0665, Reward: -1.0000, BF Gain pred: -6.21, BF Gain: -6.66, BFtrue Gain pred: -21.74, BFtrue Gain: -7.22,Critic Loss: 0.47, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 770\n",
            "Tr: 10.00,Beam: 17, Iteration: 386, Q value: 0.0670, Reward: 1.0000, BF Gain pred: -3.89, BF Gain: -4.42, BFtrue Gain pred: -21.82, BFtrue Gain: -4.56,Critic Loss: 0.49, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 772\n",
            "Tr: 10.00,Beam: 17, Iteration: 387, Q value: 0.0663, Reward: -1.0000, BF Gain pred: -6.21, BF Gain: -5.77, BFtrue Gain pred: -24.46, BFtrue Gain: -5.81,Critic Loss: 0.48, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 774\n",
            "Tr: 10.00,Beam: 17, Iteration: 388, Q value: 0.0681, Reward: 1.0000, BF Gain pred: -3.89, BF Gain: -7.48, BFtrue Gain pred: -25.62, BFtrue Gain: -7.83,Critic Loss: 0.46, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 776\n",
            "Tr: 10.00,Beam: 17, Iteration: 389, Q value: 0.0686, Reward: -1.0000, BF Gain pred: -6.21, BF Gain: -3.49, BFtrue Gain pred: -25.92, BFtrue Gain: -3.09,Critic Loss: 0.47, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 778\n",
            "Tr: 10.00,Beam: 17, Iteration: 390, Q value: 0.0692, Reward: 1.0000, BF Gain pred: -3.89, BF Gain: -4.66, BFtrue Gain pred: -29.77, BFtrue Gain: -4.12,Critic Loss: 0.52, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 780\n",
            "Tr: 10.00,Beam: 17, Iteration: 391, Q value: 0.0697, Reward: -1.0000, BF Gain pred: -6.37, BF Gain: -8.46, BFtrue Gain pred: -26.29, BFtrue Gain: -7.61,Critic Loss: 0.47, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 782\n",
            "Tr: 10.00,Beam: 17, Iteration: 392, Q value: 0.0703, Reward: 1.0000, BF Gain pred: -3.89, BF Gain: -4.88, BFtrue Gain pred: -29.84, BFtrue Gain: -5.24,Critic Loss: 0.54, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 784\n",
            "Tr: 10.00,Beam: 17, Iteration: 393, Q value: 0.0709, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -13.79, BFtrue Gain pred: -23.46, BFtrue Gain: -13.55,Critic Loss: 0.50, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 786\n",
            "Tr: 10.00,Beam: 17, Iteration: 394, Q value: 0.0714, Reward: -1.0000, BF Gain pred: -7.65, BF Gain: -9.72, BFtrue Gain pred: -21.65, BFtrue Gain: -9.93,Critic Loss: 0.50, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 788\n",
            "Tr: 10.00,Beam: 17, Iteration: 395, Q value: 0.0720, Reward: 1.0000, BF Gain pred: -6.44, BF Gain: -4.15, BFtrue Gain pred: -21.63, BFtrue Gain: -4.30,Critic Loss: 0.50, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 790\n",
            "Tr: 10.00,Beam: 17, Iteration: 396, Q value: 0.0726, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -0.90, BFtrue Gain pred: -19.85, BFtrue Gain: -2.71,Critic Loss: 0.48, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 792\n",
            "Tr: 10.00,Beam: 17, Iteration: 397, Q value: 0.0731, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -10.15, BFtrue Gain pred: -19.86, BFtrue Gain: -10.62,Critic Loss: 0.47, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 794\n",
            "Tr: 10.00,Beam: 17, Iteration: 398, Q value: 0.0737, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -11.63, BFtrue Gain pred: -19.56, BFtrue Gain: -12.70,Critic Loss: 0.47, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 796\n",
            "Tr: 10.00,Beam: 17, Iteration: 399, Q value: 0.0743, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -8.14, BFtrue Gain pred: -19.30, BFtrue Gain: -8.05,Critic Loss: 0.52, Policy Loss: -0.07\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 798\n",
            "Tr: 10.00,Beam: 17, Iteration: 400, Q value: 0.0748, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -8.91, BFtrue Gain pred: -19.12, BFtrue Gain: -8.97,Critic Loss: 0.54, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 800\n",
            "Tr: 10.00,Beam: 17, Iteration: 401, Q value: 0.0754, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -9.61, BFtrue Gain pred: -18.96, BFtrue Gain: -10.04,Critic Loss: 0.49, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 802\n",
            "Tr: 10.00,Beam: 17, Iteration: 402, Q value: 0.0759, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -2.92, BFtrue Gain pred: -18.83, BFtrue Gain: -3.69,Critic Loss: 0.52, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 804\n",
            "Tr: 10.00,Beam: 17, Iteration: 403, Q value: 0.0764, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -12.84, BFtrue Gain pred: -18.70, BFtrue Gain: -11.62,Critic Loss: 0.55, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 806\n",
            "Tr: 10.00,Beam: 17, Iteration: 404, Q value: 0.0770, Reward: -1.0000, BF Gain pred: -9.13, BF Gain: -8.26, BFtrue Gain pred: -18.62, BFtrue Gain: -6.12,Critic Loss: 0.57, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 808\n",
            "Tr: 10.00,Beam: 17, Iteration: 405, Q value: 0.0775, Reward: -1.0000, BF Gain pred: -13.07, BF Gain: -10.50, BFtrue Gain pred: -18.28, BFtrue Gain: -10.99,Critic Loss: 0.56, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 810\n",
            "Tr: 10.00,Beam: 17, Iteration: 406, Q value: 0.0781, Reward: 1.0000, BF Gain pred: -11.01, BF Gain: -5.07, BFtrue Gain pred: -17.70, BFtrue Gain: -4.97,Critic Loss: 0.51, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 812\n",
            "Tr: 10.00,Beam: 17, Iteration: 407, Q value: 0.0786, Reward: 1.0000, BF Gain pred: -8.66, BF Gain: -2.08, BFtrue Gain pred: -18.97, BFtrue Gain: -1.91,Critic Loss: 0.49, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 814\n",
            "Tr: 10.00,Beam: 17, Iteration: 408, Q value: 0.0791, Reward: 1.0000, BF Gain pred: -6.58, BF Gain: -11.62, BFtrue Gain pred: -13.51, BFtrue Gain: -10.61,Critic Loss: 0.50, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 816\n",
            "Tr: 10.00,Beam: 17, Iteration: 409, Q value: 0.0797, Reward: -1.0000, BF Gain pred: -11.74, BF Gain: -15.56, BFtrue Gain pred: -14.68, BFtrue Gain: -15.70,Critic Loss: 0.50, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 818\n",
            "Tr: 10.00,Beam: 17, Iteration: 410, Q value: 0.0802, Reward: 1.0000, BF Gain pred: -4.26, BF Gain: -10.00, BFtrue Gain pred: -11.90, BFtrue Gain: -8.65,Critic Loss: 0.48, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 820\n",
            "Tr: 10.00,Beam: 17, Iteration: 411, Q value: 0.0807, Reward: -1.0000, BF Gain pred: -11.84, BF Gain: -10.76, BFtrue Gain pred: -14.85, BFtrue Gain: -10.67,Critic Loss: 0.47, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 822\n",
            "Tr: 10.00,Beam: 17, Iteration: 412, Q value: 0.0813, Reward: 1.0000, BF Gain pred: -6.05, BF Gain: -12.64, BFtrue Gain pred: -12.96, BFtrue Gain: -14.29,Critic Loss: 0.51, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 824\n",
            "Tr: 10.00,Beam: 17, Iteration: 413, Q value: 0.0818, Reward: -1.0000, BF Gain pred: -15.39, BF Gain: -7.26, BFtrue Gain pred: -15.39, BFtrue Gain: -7.82,Critic Loss: 0.50, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 826\n",
            "Tr: 10.00,Beam: 17, Iteration: 414, Q value: 0.0823, Reward: 1.0000, BF Gain pred: -7.55, BF Gain: -5.97, BFtrue Gain pred: -12.92, BFtrue Gain: -5.65,Critic Loss: 0.52, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 828\n",
            "Tr: 10.00,Beam: 17, Iteration: 415, Q value: 0.0828, Reward: -1.0000, BF Gain pred: -15.39, BF Gain: -19.45, BFtrue Gain pred: -15.68, BFtrue Gain: -18.55,Critic Loss: 0.48, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 830\n",
            "Tr: 10.00,Beam: 17, Iteration: 416, Q value: 0.0833, Reward: 1.0000, BF Gain pred: -2.23, BF Gain: -6.02, BFtrue Gain pred: -13.57, BFtrue Gain: -5.79,Critic Loss: 0.52, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 832\n",
            "Tr: 10.00,Beam: 17, Iteration: 417, Q value: 0.0839, Reward: -1.0000, BF Gain pred: -18.21, BF Gain: -6.74, BFtrue Gain pred: -18.78, BFtrue Gain: -6.90,Critic Loss: 0.48, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 834\n",
            "Tr: 10.00,Beam: 17, Iteration: 418, Q value: 0.0844, Reward: 1.0000, BF Gain pred: -2.23, BF Gain: -0.40, BFtrue Gain pred: -14.30, BFtrue Gain: -0.97,Critic Loss: 0.49, Policy Loss: -0.08\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 836\n",
            "Tr: 10.00,Beam: 17, Iteration: 419, Q value: 0.0849, Reward: -1.0000, BF Gain pred: -18.21, BF Gain: -3.99, BFtrue Gain pred: -19.45, BFtrue Gain: -2.64,Critic Loss: 0.51, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 838\n",
            "Tr: 10.00,Beam: 17, Iteration: 420, Q value: 0.0855, Reward: 1.0000, BF Gain pred: -2.23, BF Gain: 0.74, BFtrue Gain pred: -15.20, BFtrue Gain: 1.34,Critic Loss: 0.47, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 840\n",
            "Tr: 10.00,Beam: 17, Iteration: 421, Q value: 0.0860, Reward: -1.0000, BF Gain pred: -19.82, BF Gain: -1.25, BFtrue Gain pred: -20.31, BFtrue Gain: 0.27,Critic Loss: 0.49, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 842\n",
            "Tr: 10.00,Beam: 17, Iteration: 422, Q value: 0.0866, Reward: 1.0000, BF Gain pred: -2.23, BF Gain: -2.28, BFtrue Gain pred: -14.96, BFtrue Gain: -1.26,Critic Loss: 0.45, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 844\n",
            "Tr: 10.00,Beam: 17, Iteration: 423, Q value: 0.0871, Reward: -1.0000, BF Gain pred: -19.82, BF Gain: -2.87, BFtrue Gain pred: -21.32, BFtrue Gain: -2.01,Critic Loss: 0.49, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 846\n",
            "Tr: 10.00,Beam: 17, Iteration: 424, Q value: 0.0876, Reward: 1.0000, BF Gain pred: -2.23, BF Gain: -5.55, BFtrue Gain pred: -15.00, BFtrue Gain: -5.13,Critic Loss: 0.48, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 848\n",
            "Tr: 10.00,Beam: 17, Iteration: 425, Q value: 0.0882, Reward: -1.0000, BF Gain pred: -22.71, BF Gain: -10.23, BFtrue Gain pred: -22.09, BFtrue Gain: -9.82,Critic Loss: 0.47, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 850\n",
            "Tr: 10.00,Beam: 17, Iteration: 426, Q value: 0.0887, Reward: 1.0000, BF Gain pred: 3.19, BF Gain: -10.02, BFtrue Gain pred: -13.97, BFtrue Gain: -9.95,Critic Loss: 0.49, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 852\n",
            "Tr: 10.00,Beam: 17, Iteration: 427, Q value: 0.0893, Reward: -1.0000, BF Gain pred: -22.71, BF Gain: -5.89, BFtrue Gain pred: -23.28, BFtrue Gain: -5.98,Critic Loss: 0.44, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 854\n",
            "Tr: 10.00,Beam: 17, Iteration: 428, Q value: 0.0898, Reward: 1.0000, BF Gain pred: 3.19, BF Gain: -4.72, BFtrue Gain pred: -14.14, BFtrue Gain: -4.13,Critic Loss: 0.48, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 856\n",
            "Tr: 10.00,Beam: 17, Iteration: 429, Q value: 0.0903, Reward: -1.0000, BF Gain pred: -18.45, BF Gain: -12.15, BFtrue Gain pred: -23.55, BFtrue Gain: -10.96,Critic Loss: 0.47, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 858\n",
            "Tr: 10.00,Beam: 17, Iteration: 430, Q value: 0.0908, Reward: 1.0000, BF Gain pred: -9.56, BF Gain: -4.86, BFtrue Gain pred: -15.01, BFtrue Gain: -4.82,Critic Loss: 0.48, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 860\n",
            "Tr: 10.00,Beam: 17, Iteration: 431, Q value: 0.0914, Reward: -1.0000, BF Gain pred: -18.45, BF Gain: -7.43, BFtrue Gain pred: -22.13, BFtrue Gain: -6.88,Critic Loss: 0.46, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 862\n",
            "Tr: 10.00,Beam: 17, Iteration: 432, Q value: 0.0919, Reward: 1.0000, BF Gain pred: -6.74, BF Gain: -0.99, BFtrue Gain pred: -15.19, BFtrue Gain: -1.65,Critic Loss: 0.45, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 864\n",
            "Tr: 10.00,Beam: 17, Iteration: 433, Q value: 0.0925, Reward: -1.0000, BF Gain pred: -18.45, BF Gain: -4.37, BFtrue Gain pred: -26.03, BFtrue Gain: -4.65,Critic Loss: 0.49, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 866\n",
            "Tr: 10.00,Beam: 17, Iteration: 434, Q value: 0.0930, Reward: 1.0000, BF Gain pred: -6.74, BF Gain: -7.46, BFtrue Gain pred: -15.52, BFtrue Gain: -7.24,Critic Loss: 0.50, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 868\n",
            "Tr: 10.00,Beam: 17, Iteration: 435, Q value: 0.0935, Reward: -1.0000, BF Gain pred: -18.45, BF Gain: -10.18, BFtrue Gain pred: -26.44, BFtrue Gain: -9.17,Critic Loss: 0.48, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 870\n",
            "Tr: 10.00,Beam: 17, Iteration: 436, Q value: 0.0941, Reward: 1.0000, BF Gain pred: -6.74, BF Gain: -3.78, BFtrue Gain pred: -15.96, BFtrue Gain: -4.35,Critic Loss: 0.46, Policy Loss: -0.09\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 872\n",
            "Tr: 10.00,Beam: 17, Iteration: 437, Q value: 0.0946, Reward: -1.0000, BF Gain pred: -14.17, BF Gain: -8.02, BFtrue Gain pred: -26.77, BFtrue Gain: -7.98,Critic Loss: 0.48, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 874\n",
            "Tr: 10.00,Beam: 17, Iteration: 438, Q value: 0.0952, Reward: 1.0000, BF Gain pred: -6.17, BF Gain: -3.41, BFtrue Gain pred: -17.98, BFtrue Gain: -3.85,Critic Loss: 0.49, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 876\n",
            "Tr: 10.00,Beam: 17, Iteration: 439, Q value: 0.0958, Reward: -1.0000, BF Gain pred: -14.17, BF Gain: -2.01, BFtrue Gain pred: -23.11, BFtrue Gain: -1.38,Critic Loss: 0.46, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 878\n",
            "Tr: 10.00,Beam: 17, Iteration: 440, Q value: 0.0963, Reward: 1.0000, BF Gain pred: -6.17, BF Gain: -2.68, BFtrue Gain pred: -18.25, BFtrue Gain: -2.31,Critic Loss: 0.46, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 880\n",
            "Tr: 10.00,Beam: 17, Iteration: 441, Q value: 0.0969, Reward: -1.0000, BF Gain pred: -14.17, BF Gain: -3.97, BFtrue Gain pred: -22.14, BFtrue Gain: -3.70,Critic Loss: 0.46, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 882\n",
            "Tr: 10.00,Beam: 17, Iteration: 442, Q value: 0.0974, Reward: 1.0000, BF Gain pred: -6.58, BF Gain: -11.49, BFtrue Gain pred: -18.46, BFtrue Gain: -10.92,Critic Loss: 0.49, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 884\n",
            "Tr: 10.00,Beam: 17, Iteration: 443, Q value: 0.0855, Reward: -1.0000, BF Gain pred: -14.17, BF Gain: -9.63, BFtrue Gain pred: -21.51, BFtrue Gain: -9.90,Critic Loss: 0.55, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 886\n",
            "Tr: 10.00,Beam: 17, Iteration: 444, Q value: 0.0986, Reward: 1.0000, BF Gain pred: -9.44, BF Gain: -9.50, BFtrue Gain pred: -18.79, BFtrue Gain: -8.45,Critic Loss: 0.53, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 888\n",
            "Tr: 10.00,Beam: 17, Iteration: 445, Q value: 0.0992, Reward: -1.0000, BF Gain pred: -11.85, BF Gain: -8.43, BFtrue Gain pred: -16.94, BFtrue Gain: -8.47,Critic Loss: 0.56, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 890\n",
            "Tr: 10.00,Beam: 17, Iteration: 446, Q value: 0.0998, Reward: 1.0000, BF Gain pred: -9.90, BF Gain: -1.26, BFtrue Gain pred: -19.99, BFtrue Gain: -1.61,Critic Loss: 0.50, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 892\n",
            "Tr: 10.00,Beam: 17, Iteration: 447, Q value: 0.1005, Reward: -1.0000, BF Gain pred: -14.00, BF Gain: 0.29, BFtrue Gain pred: -12.87, BFtrue Gain: 2.23,Critic Loss: 0.52, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 894\n",
            "Tr: 10.00,Beam: 17, Iteration: 448, Q value: 0.1011, Reward: 1.0000, BF Gain pred: -7.11, BF Gain: -3.97, BFtrue Gain pred: -16.63, BFtrue Gain: -4.42,Critic Loss: 0.47, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 896\n",
            "Tr: 10.00,Beam: 17, Iteration: 449, Q value: 0.1017, Reward: -1.0000, BF Gain pred: -9.22, BF Gain: -4.90, BFtrue Gain pred: -11.24, BFtrue Gain: -4.31,Critic Loss: 0.53, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 898\n",
            "Tr: 10.00,Beam: 17, Iteration: 450, Q value: 0.1023, Reward: -1.0000, BF Gain pred: -9.22, BF Gain: -6.75, BFtrue Gain pred: -11.45, BFtrue Gain: -6.33,Critic Loss: 0.47, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 900\n",
            "Tr: 10.00,Beam: 17, Iteration: 451, Q value: 0.1029, Reward: -1.0000, BF Gain pred: -11.84, BF Gain: -30.13, BFtrue Gain pred: -11.95, BFtrue Gain: -30.43,Critic Loss: 0.47, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 902\n",
            "Tr: 10.00,Beam: 17, Iteration: 452, Q value: 0.1036, Reward: 1.0000, BF Gain pred: -6.75, BF Gain: -16.54, BFtrue Gain pred: -13.56, BFtrue Gain: -15.42,Critic Loss: 0.47, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 904\n",
            "Tr: 10.00,Beam: 17, Iteration: 453, Q value: 0.1042, Reward: -1.0000, BF Gain pred: -7.96, BF Gain: -7.20, BFtrue Gain pred: -15.16, BFtrue Gain: -6.57,Critic Loss: 0.46, Policy Loss: -0.10\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 906\n",
            "Tr: 10.00,Beam: 17, Iteration: 454, Q value: 0.1048, Reward: -1.0000, BF Gain pred: -10.96, BF Gain: -15.99, BFtrue Gain pred: -17.11, BFtrue Gain: -14.43,Critic Loss: 0.47, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 908\n",
            "Tr: 10.00,Beam: 17, Iteration: 455, Q value: 0.1054, Reward: 1.0000, BF Gain pred: -8.38, BF Gain: -2.55, BFtrue Gain pred: -16.44, BFtrue Gain: -2.19,Critic Loss: 0.42, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 910\n",
            "Tr: 10.00,Beam: 17, Iteration: 456, Q value: 0.1060, Reward: -1.0000, BF Gain pred: -8.38, BF Gain: -4.17, BFtrue Gain pred: -16.52, BFtrue Gain: -3.93,Critic Loss: 0.47, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 912\n",
            "Tr: 10.00,Beam: 17, Iteration: 457, Q value: 0.1066, Reward: 1.0000, BF Gain pred: -6.11, BF Gain: -6.34, BFtrue Gain pred: -16.14, BFtrue Gain: -6.25,Critic Loss: 0.42, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 914\n",
            "Tr: 10.00,Beam: 17, Iteration: 458, Q value: 0.1072, Reward: -1.0000, BF Gain pred: -7.30, BF Gain: 1.37, BFtrue Gain pred: -16.69, BFtrue Gain: 0.70,Critic Loss: 0.44, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 916\n",
            "Tr: 10.00,Beam: 17, Iteration: 459, Q value: 0.1077, Reward: -1.0000, BF Gain pred: -7.30, BF Gain: -7.00, BFtrue Gain pred: -16.86, BFtrue Gain: -6.88,Critic Loss: 0.45, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 918\n",
            "Tr: 10.00,Beam: 17, Iteration: 460, Q value: 0.1083, Reward: -1.0000, BF Gain pred: -9.05, BF Gain: -17.49, BFtrue Gain pred: -17.48, BFtrue Gain: -17.11,Critic Loss: 0.44, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 920\n",
            "Tr: 10.00,Beam: 17, Iteration: 461, Q value: 0.1089, Reward: -1.0000, BF Gain pred: -9.30, BF Gain: -7.63, BFtrue Gain pred: -20.43, BFtrue Gain: -8.09,Critic Loss: 0.47, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 922\n",
            "Tr: 10.00,Beam: 17, Iteration: 462, Q value: 0.1095, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -0.54, BFtrue Gain pred: -17.11, BFtrue Gain: -1.54,Critic Loss: 0.45, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 924\n",
            "Tr: 10.00,Beam: 17, Iteration: 463, Q value: 0.1101, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -6.83, BFtrue Gain pred: -20.04, BFtrue Gain: -7.25,Critic Loss: 0.46, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 926\n",
            "Tr: 10.00,Beam: 17, Iteration: 464, Q value: 0.1107, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -4.94, BFtrue Gain pred: -19.98, BFtrue Gain: -4.81,Critic Loss: 0.43, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 928\n",
            "Tr: 10.00,Beam: 17, Iteration: 465, Q value: 0.1113, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -5.95, BFtrue Gain pred: -19.80, BFtrue Gain: -5.61,Critic Loss: 0.44, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 930\n",
            "Tr: 10.00,Beam: 17, Iteration: 466, Q value: 0.1119, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -9.28, BFtrue Gain pred: -19.69, BFtrue Gain: -9.24,Critic Loss: 0.43, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 932\n",
            "Tr: 10.00,Beam: 17, Iteration: 467, Q value: 0.1125, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -13.58, BFtrue Gain pred: -19.29, BFtrue Gain: -11.10,Critic Loss: 0.45, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 934\n",
            "Tr: 10.00,Beam: 17, Iteration: 468, Q value: 0.1131, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -7.93, BFtrue Gain pred: -19.12, BFtrue Gain: -8.02,Critic Loss: 0.47, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 936\n",
            "Tr: 10.00,Beam: 17, Iteration: 469, Q value: 0.1137, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -5.69, BFtrue Gain pred: -18.89, BFtrue Gain: -3.94,Critic Loss: 0.46, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 938\n",
            "Tr: 10.00,Beam: 17, Iteration: 470, Q value: 0.1143, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -10.80, BFtrue Gain pred: -18.60, BFtrue Gain: -10.69,Critic Loss: 0.44, Policy Loss: -0.11\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 940\n",
            "Tr: 10.00,Beam: 17, Iteration: 471, Q value: 0.1149, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -6.30, BFtrue Gain pred: -18.47, BFtrue Gain: -7.00,Critic Loss: 0.45, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 942\n",
            "Tr: 10.00,Beam: 17, Iteration: 472, Q value: 0.1155, Reward: 1.0000, BF Gain pred: -9.72, BF Gain: -9.21, BFtrue Gain pred: -17.98, BFtrue Gain: -8.72,Critic Loss: 0.44, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 944\n",
            "Tr: 10.00,Beam: 17, Iteration: 473, Q value: 0.1161, Reward: -1.0000, BF Gain pred: -11.99, BF Gain: -9.22, BFtrue Gain pred: -16.72, BFtrue Gain: -9.19,Critic Loss: 0.49, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 946\n",
            "Tr: 10.00,Beam: 17, Iteration: 474, Q value: 0.1167, Reward: 1.0000, BF Gain pred: -7.52, BF Gain: -10.06, BFtrue Gain pred: -17.32, BFtrue Gain: -10.04,Critic Loss: 0.45, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 948\n",
            "Tr: 10.00,Beam: 17, Iteration: 475, Q value: 0.0812, Reward: -1.0000, BF Gain pred: -8.89, BF Gain: -10.16, BFtrue Gain pred: -13.92, BFtrue Gain: -10.50,Critic Loss: 0.50, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 950\n",
            "Tr: 10.00,Beam: 17, Iteration: 476, Q value: 0.1088, Reward: 1.0000, BF Gain pred: -5.02, BF Gain: -0.10, BFtrue Gain pred: -13.06, BFtrue Gain: -0.29,Critic Loss: 0.48, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 952\n",
            "Tr: 10.00,Beam: 17, Iteration: 477, Q value: 0.0967, Reward: 1.0000, BF Gain pred: -3.56, BF Gain: 3.40, BFtrue Gain pred: -6.57, BFtrue Gain: 3.24,Critic Loss: 0.46, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 954\n",
            "Tr: 10.00,Beam: 17, Iteration: 478, Q value: 0.1055, Reward: 1.0000, BF Gain pred: 3.12, BF Gain: -6.73, BFtrue Gain pred: -5.70, BFtrue Gain: -6.99,Critic Loss: 0.50, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 956\n",
            "Tr: 10.00,Beam: 17, Iteration: 479, Q value: -0.1025, Reward: -1.0000, BF Gain pred: -0.60, BF Gain: -5.93, BFtrue Gain pred: -4.33, BFtrue Gain: -5.20,Critic Loss: 0.45, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 958\n",
            "Tr: 10.00,Beam: 17, Iteration: 480, Q value: -0.3588, Reward: -1.0000, BF Gain pred: -3.52, BF Gain: -3.47, BFtrue Gain pred: -7.38, BFtrue Gain: -2.92,Critic Loss: 0.49, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 960\n",
            "Tr: 10.00,Beam: 17, Iteration: 481, Q value: -0.0999, Reward: -1.0000, BF Gain pred: -8.61, BF Gain: -6.69, BFtrue Gain pred: -12.50, BFtrue Gain: -6.96,Critic Loss: 0.44, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 962\n",
            "Tr: 10.00,Beam: 17, Iteration: 482, Q value: -0.2976, Reward: 1.0000, BF Gain pred: -5.72, BF Gain: 5.97, BFtrue Gain pred: -7.41, BFtrue Gain: 8.17,Critic Loss: 0.50, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 964\n",
            "Tr: 10.00,Beam: 17, Iteration: 483, Q value: -0.5447, Reward: 1.0000, BF Gain pred: -3.70, BF Gain: 1.94, BFtrue Gain pred: -6.24, BFtrue Gain: 2.73,Critic Loss: 0.47, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 966\n",
            "Tr: 10.00,Beam: 17, Iteration: 484, Q value: -0.2309, Reward: -1.0000, BF Gain pred: -9.79, BF Gain: 2.38, BFtrue Gain pred: -6.66, BFtrue Gain: 3.39,Critic Loss: 0.50, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 968\n",
            "Tr: 10.00,Beam: 17, Iteration: 485, Q value: 0.1232, Reward: 1.0000, BF Gain pred: -3.99, BF Gain: -3.85, BFtrue Gain pred: -7.69, BFtrue Gain: -3.73,Critic Loss: 0.45, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 970\n",
            "Tr: 10.00,Beam: 17, Iteration: 486, Q value: 0.1238, Reward: -1.0000, BF Gain pred: -6.57, BF Gain: -6.79, BFtrue Gain pred: -1.27, BFtrue Gain: -6.88,Critic Loss: 0.49, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 972\n",
            "Tr: 10.00,Beam: 17, Iteration: 487, Q value: 0.1243, Reward: -1.0000, BF Gain pred: -9.13, BF Gain: -10.32, BFtrue Gain pred: 1.39, BFtrue Gain: -10.45,Critic Loss: 0.45, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 974\n",
            "Tr: 10.00,Beam: 17, Iteration: 488, Q value: 0.1249, Reward: -1.0000, BF Gain pred: -9.82, BF Gain: -4.63, BFtrue Gain pred: -0.73, BFtrue Gain: -4.14,Critic Loss: 0.47, Policy Loss: -0.12\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 976\n",
            "Tr: 10.00,Beam: 17, Iteration: 489, Q value: 0.1255, Reward: -1.0000, BF Gain pred: -11.49, BF Gain: -15.74, BFtrue Gain pred: -1.25, BFtrue Gain: -12.45,Critic Loss: 0.51, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 978\n",
            "Tr: 10.00,Beam: 17, Iteration: 490, Q value: 0.1261, Reward: 1.0000, BF Gain pred: -7.51, BF Gain: -19.00, BFtrue Gain pred: -0.99, BFtrue Gain: -18.16,Critic Loss: 0.48, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 980\n",
            "Tr: 10.00,Beam: 17, Iteration: 491, Q value: 0.1266, Reward: -1.0000, BF Gain pred: -10.88, BF Gain: -9.92, BFtrue Gain pred: -0.47, BFtrue Gain: -9.64,Critic Loss: 0.47, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 982\n",
            "Tr: 10.00,Beam: 17, Iteration: 492, Q value: 0.1272, Reward: -1.0000, BF Gain pred: -10.88, BF Gain: -10.47, BFtrue Gain pred: 0.46, BFtrue Gain: -9.54,Critic Loss: 0.47, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 984\n",
            "Tr: 10.00,Beam: 17, Iteration: 493, Q value: 0.1277, Reward: -1.0000, BF Gain pred: -10.88, BF Gain: -5.37, BFtrue Gain pred: 0.62, BFtrue Gain: -5.49,Critic Loss: 0.43, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 986\n",
            "Tr: 10.00,Beam: 17, Iteration: 494, Q value: 0.1282, Reward: -1.0000, BF Gain pred: -10.88, BF Gain: -6.97, BFtrue Gain pred: 0.70, BFtrue Gain: -6.89,Critic Loss: 0.50, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 988\n",
            "Tr: 10.00,Beam: 17, Iteration: 495, Q value: 0.1288, Reward: 1.0000, BF Gain pred: -8.95, BF Gain: -9.73, BFtrue Gain pred: 0.74, BFtrue Gain: -9.56,Critic Loss: 0.47, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 990\n",
            "Tr: 10.00,Beam: 17, Iteration: 496, Q value: 0.1293, Reward: -1.0000, BF Gain pred: -8.95, BF Gain: -3.47, BFtrue Gain pred: 1.41, BFtrue Gain: -3.64,Critic Loss: 0.46, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 992\n",
            "Tr: 10.00,Beam: 17, Iteration: 497, Q value: 0.1298, Reward: 1.0000, BF Gain pred: -5.22, BF Gain: -3.79, BFtrue Gain pred: 1.30, BFtrue Gain: -3.60,Critic Loss: 0.45, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 994\n",
            "Tr: 10.00,Beam: 17, Iteration: 498, Q value: 0.1303, Reward: -1.0000, BF Gain pred: -5.22, BF Gain: -7.80, BFtrue Gain pred: 1.06, BFtrue Gain: -7.28,Critic Loss: 0.47, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 996\n",
            "Tr: 10.00,Beam: 17, Iteration: 499, Q value: 0.1308, Reward: 1.0000, BF Gain pred: -3.68, BF Gain: -12.86, BFtrue Gain pred: 0.88, BFtrue Gain: -12.45,Critic Loss: 0.48, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 998\n",
            "Tr: 10.00,Beam: 17, Iteration: 500, Q value: 0.1314, Reward: -1.0000, BF Gain pred: -3.68, BF Gain: -8.16, BFtrue Gain pred: 0.75, BFtrue Gain: -7.42,Critic Loss: 0.52, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1000\n",
            "Tr: 10.00,Beam: 17, Iteration: 501, Q value: 0.1319, Reward: -1.0000, BF Gain pred: -3.68, BF Gain: -11.69, BFtrue Gain pred: 0.44, BFtrue Gain: -10.54,Critic Loss: 0.48, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1002\n",
            "Tr: 10.00,Beam: 17, Iteration: 502, Q value: 0.1324, Reward: -1.0000, BF Gain pred: -3.68, BF Gain: -17.54, BFtrue Gain pred: 0.20, BFtrue Gain: -16.57,Critic Loss: 0.56, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1004\n",
            "Tr: 10.00,Beam: 17, Iteration: 503, Q value: 0.1329, Reward: -1.0000, BF Gain pred: -5.22, BF Gain: -6.07, BFtrue Gain pred: -0.19, BFtrue Gain: -4.85,Critic Loss: 0.52, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1006\n",
            "Tr: 10.00,Beam: 17, Iteration: 504, Q value: 0.1334, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -6.45, BFtrue Gain pred: -0.61, BFtrue Gain: -5.09,Critic Loss: 0.48, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1008\n",
            "Tr: 10.00,Beam: 17, Iteration: 505, Q value: 0.1339, Reward: -1.0000, BF Gain pred: -9.51, BF Gain: -7.38, BFtrue Gain pred: -2.35, BFtrue Gain: -5.23,Critic Loss: 0.53, Policy Loss: -0.13\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1010\n",
            "Tr: 10.00,Beam: 17, Iteration: 506, Q value: 0.1345, Reward: 1.0000, BF Gain pred: -8.80, BF Gain: -6.58, BFtrue Gain pred: -2.95, BFtrue Gain: -5.33,Critic Loss: 0.50, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1012\n",
            "Tr: 10.00,Beam: 17, Iteration: 507, Q value: 0.1350, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -7.10, BFtrue Gain pred: -3.31, BFtrue Gain: -7.00,Critic Loss: 0.49, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1014\n",
            "Tr: 10.00,Beam: 17, Iteration: 508, Q value: 0.1356, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -5.56, BFtrue Gain pred: -3.94, BFtrue Gain: -3.63,Critic Loss: 0.49, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1016\n",
            "Tr: 10.00,Beam: 17, Iteration: 509, Q value: 0.1361, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -10.01, BFtrue Gain pred: -4.72, BFtrue Gain: -9.27,Critic Loss: 0.44, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1018\n",
            "Tr: 10.00,Beam: 17, Iteration: 510, Q value: 0.1367, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -11.81, BFtrue Gain pred: -5.39, BFtrue Gain: -11.01,Critic Loss: 0.48, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1020\n",
            "Tr: 10.00,Beam: 17, Iteration: 511, Q value: 0.1372, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -10.77, BFtrue Gain pred: -6.01, BFtrue Gain: -10.39,Critic Loss: 0.54, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1022\n",
            "Tr: 10.00,Beam: 17, Iteration: 512, Q value: 0.1377, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -19.61, BFtrue Gain pred: -6.56, BFtrue Gain: -19.88,Critic Loss: 0.48, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1024\n",
            "Tr: 10.00,Beam: 17, Iteration: 513, Q value: 0.1382, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -12.53, BFtrue Gain pred: -6.98, BFtrue Gain: -11.05,Critic Loss: 0.50, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1026\n",
            "Tr: 10.00,Beam: 17, Iteration: 514, Q value: 0.0851, Reward: -1.0000, BF Gain pred: -8.80, BF Gain: -12.82, BFtrue Gain pred: -7.32, BFtrue Gain: -11.42,Critic Loss: 0.51, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1028\n",
            "Tr: 10.00,Beam: 17, Iteration: 515, Q value: -0.0316, Reward: -1.0000, BF Gain pred: -9.05, BF Gain: -15.25, BFtrue Gain pred: -6.56, BFtrue Gain: -15.59,Critic Loss: 0.49, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1030\n",
            "Tr: 10.00,Beam: 17, Iteration: 516, Q value: -0.1064, Reward: -1.0000, BF Gain pred: -10.89, BF Gain: -11.81, BFtrue Gain pred: -4.73, BFtrue Gain: -10.92,Critic Loss: 0.52, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1032\n",
            "Tr: 10.00,Beam: 17, Iteration: 517, Q value: -0.4547, Reward: 1.0000, BF Gain pred: -3.90, BF Gain: -11.96, BFtrue Gain pred: -1.33, BFtrue Gain: -10.84,Critic Loss: 0.53, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1034\n",
            "Tr: 10.00,Beam: 17, Iteration: 518, Q value: -1.0724, Reward: -1.0000, BF Gain pred: -3.93, BF Gain: -8.18, BFtrue Gain pred: -1.17, BFtrue Gain: -7.48,Critic Loss: 0.52, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1036\n",
            "Tr: 10.00,Beam: 17, Iteration: 519, Q value: 0.0255, Reward: -1.0000, BF Gain pred: -5.85, BF Gain: -9.26, BFtrue Gain pred: 0.36, BFtrue Gain: -10.29,Critic Loss: 0.50, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1038\n",
            "Tr: 10.00,Beam: 17, Iteration: 520, Q value: -0.4455, Reward: 1.0000, BF Gain pred: -5.21, BF Gain: 0.08, BFtrue Gain pred: -1.12, BFtrue Gain: 1.06,Critic Loss: 0.48, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1040\n",
            "Tr: 10.00,Beam: 17, Iteration: 521, Q value: -0.5993, Reward: -1.0000, BF Gain pred: -5.58, BF Gain: -10.62, BFtrue Gain pred: -1.93, BFtrue Gain: -8.08,Critic Loss: 0.52, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1042\n",
            "Tr: 10.00,Beam: 17, Iteration: 522, Q value: -0.4412, Reward: 1.0000, BF Gain pred: -0.83, BF Gain: -5.93, BFtrue Gain pred: -2.52, BFtrue Gain: -5.20,Critic Loss: 0.49, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1044\n",
            "Tr: 10.00,Beam: 17, Iteration: 523, Q value: -0.5441, Reward: -1.0000, BF Gain pred: -0.83, BF Gain: -11.28, BFtrue Gain pred: -6.42, BFtrue Gain: -10.73,Critic Loss: 0.46, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1046\n",
            "Tr: 10.00,Beam: 17, Iteration: 524, Q value: -0.6606, Reward: -1.0000, BF Gain pred: -3.42, BF Gain: -8.52, BFtrue Gain pred: -6.68, BFtrue Gain: -8.54,Critic Loss: 0.48, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1048\n",
            "Tr: 10.00,Beam: 17, Iteration: 525, Q value: -0.6610, Reward: -1.0000, BF Gain pred: -3.42, BF Gain: -14.45, BFtrue Gain pred: -4.73, BFtrue Gain: -14.96,Critic Loss: 0.49, Policy Loss: -0.14\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1050\n",
            "Tr: 10.00,Beam: 17, Iteration: 526, Q value: -0.4866, Reward: 1.0000, BF Gain pred: -2.98, BF Gain: -18.39, BFtrue Gain pred: -3.33, BFtrue Gain: -17.88,Critic Loss: 0.51, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1052\n",
            "Tr: 10.00,Beam: 17, Iteration: 527, Q value: -0.0140, Reward: -1.0000, BF Gain pred: -3.99, BF Gain: -15.90, BFtrue Gain pred: -2.54, BFtrue Gain: -15.48,Critic Loss: 0.50, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1054\n",
            "Tr: 10.00,Beam: 17, Iteration: 528, Q value: 0.0887, Reward: -1.0000, BF Gain pred: -3.99, BF Gain: -28.84, BFtrue Gain pred: -1.12, BFtrue Gain: -26.48,Critic Loss: 0.48, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1056\n",
            "Tr: 10.00,Beam: 17, Iteration: 529, Q value: 0.0642, Reward: -1.0000, BF Gain pred: -3.99, BF Gain: -18.44, BFtrue Gain pred: -0.26, BFtrue Gain: -20.19,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1058\n",
            "Tr: 10.00,Beam: 17, Iteration: 530, Q value: 0.1152, Reward: 1.0000, BF Gain pred: -1.41, BF Gain: -19.86, BFtrue Gain pred: 0.01, BFtrue Gain: -18.74,Critic Loss: 0.48, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1060\n",
            "Tr: 10.00,Beam: 17, Iteration: 531, Q value: 0.1210, Reward: 1.0000, BF Gain pred: -1.08, BF Gain: -16.56, BFtrue Gain pred: 2.43, BFtrue Gain: -14.79,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1062\n",
            "Tr: 10.00,Beam: 17, Iteration: 532, Q value: 0.1137, Reward: -1.0000, BF Gain pred: -1.08, BF Gain: -8.59, BFtrue Gain pred: 0.40, BFtrue Gain: -9.53,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1064\n",
            "Tr: 10.00,Beam: 17, Iteration: 533, Q value: 0.0945, Reward: -1.0000, BF Gain pred: -5.68, BF Gain: -24.36, BFtrue Gain pred: -2.17, BFtrue Gain: -24.35,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1066\n",
            "Tr: 10.00,Beam: 17, Iteration: 534, Q value: -0.0103, Reward: -1.0000, BF Gain pred: -5.68, BF Gain: -19.46, BFtrue Gain pred: -5.81, BFtrue Gain: -22.36,Critic Loss: 0.47, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1068\n",
            "Tr: 10.00,Beam: 17, Iteration: 535, Q value: -0.0106, Reward: -1.0000, BF Gain pred: -5.68, BF Gain: -9.86, BFtrue Gain pred: -11.74, BFtrue Gain: -10.19,Critic Loss: 0.48, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1070\n",
            "Tr: 10.00,Beam: 17, Iteration: 536, Q value: 0.1056, Reward: -1.0000, BF Gain pred: -9.73, BF Gain: -16.89, BFtrue Gain pred: -16.93, BFtrue Gain: -17.76,Critic Loss: 0.44, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1072\n",
            "Tr: 10.00,Beam: 17, Iteration: 537, Q value: 0.0799, Reward: -1.0000, BF Gain pred: -13.50, BF Gain: -11.77, BFtrue Gain pred: -15.82, BFtrue Gain: -12.29,Critic Loss: 0.42, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1074\n",
            "Tr: 10.00,Beam: 17, Iteration: 538, Q value: 0.0622, Reward: 1.0000, BF Gain pred: -10.51, BF Gain: -13.29, BFtrue Gain pred: -13.13, BFtrue Gain: -14.72,Critic Loss: 0.44, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1076\n",
            "Tr: 10.00,Beam: 17, Iteration: 539, Q value: 0.0219, Reward: -1.0000, BF Gain pred: -10.51, BF Gain: -12.61, BFtrue Gain pred: -11.01, BFtrue Gain: -13.32,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1078\n",
            "Tr: 10.00,Beam: 17, Iteration: 540, Q value: -0.1010, Reward: -1.0000, BF Gain pred: -13.50, BF Gain: -11.33, BFtrue Gain pred: -9.15, BFtrue Gain: -12.43,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1080\n",
            "Tr: 10.00,Beam: 17, Iteration: 541, Q value: -0.1967, Reward: -1.0000, BF Gain pred: -13.50, BF Gain: -7.70, BFtrue Gain pred: -5.31, BFtrue Gain: -8.07,Critic Loss: 0.45, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1082\n",
            "Tr: 10.00,Beam: 17, Iteration: 542, Q value: -0.0759, Reward: -1.0000, BF Gain pred: -14.36, BF Gain: -8.16, BFtrue Gain pred: -1.44, BFtrue Gain: -8.24,Critic Loss: 0.43, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1084\n",
            "Tr: 10.00,Beam: 17, Iteration: 543, Q value: 0.0775, Reward: 1.0000, BF Gain pred: -13.46, BF Gain: -1.04, BFtrue Gain pred: -2.91, BFtrue Gain: -0.62,Critic Loss: 0.44, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1086\n",
            "Tr: 10.00,Beam: 17, Iteration: 544, Q value: 0.0871, Reward: -1.0000, BF Gain pred: -15.21, BF Gain: -8.50, BFtrue Gain pred: -3.93, BFtrue Gain: -6.87,Critic Loss: 0.46, Policy Loss: -0.15\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1088\n",
            "Tr: 10.00,Beam: 17, Iteration: 545, Q value: 0.1107, Reward: 1.0000, BF Gain pred: -11.74, BF Gain: -0.71, BFtrue Gain pred: -4.36, BFtrue Gain: -1.52,Critic Loss: 0.43, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1090\n",
            "Tr: 10.00,Beam: 17, Iteration: 546, Q value: 0.1553, Reward: 1.0000, BF Gain pred: -3.24, BF Gain: -4.20, BFtrue Gain pred: -14.44, BFtrue Gain: -5.38,Critic Loss: 0.48, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1092\n",
            "Tr: 10.00,Beam: 17, Iteration: 547, Q value: 0.1558, Reward: -1.0000, BF Gain pred: -6.76, BF Gain: -3.66, BFtrue Gain pred: -8.15, BFtrue Gain: -2.77,Critic Loss: 0.46, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1094\n",
            "Tr: 10.00,Beam: 17, Iteration: 548, Q value: 0.1563, Reward: -1.0000, BF Gain pred: -10.91, BF Gain: -10.41, BFtrue Gain pred: -14.26, BFtrue Gain: -10.24,Critic Loss: 0.46, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1096\n",
            "Tr: 10.00,Beam: 17, Iteration: 549, Q value: 0.1569, Reward: 1.0000, BF Gain pred: -5.73, BF Gain: -13.54, BFtrue Gain pred: -9.48, BFtrue Gain: -14.67,Critic Loss: 0.42, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1098\n",
            "Tr: 10.00,Beam: 17, Iteration: 550, Q value: 0.1574, Reward: -1.0000, BF Gain pred: -9.41, BF Gain: -18.72, BFtrue Gain pred: -5.94, BFtrue Gain: -16.03,Critic Loss: 0.44, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1100\n",
            "Tr: 10.00,Beam: 17, Iteration: 551, Q value: 0.1579, Reward: 1.0000, BF Gain pred: -2.43, BF Gain: 2.00, BFtrue Gain pred: -5.88, BFtrue Gain: 4.51,Critic Loss: 0.45, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1102\n",
            "Tr: 10.00,Beam: 17, Iteration: 552, Q value: 0.1584, Reward: -1.0000, BF Gain pred: -8.88, BF Gain: -3.02, BFtrue Gain pred: -7.32, BFtrue Gain: -3.27,Critic Loss: 0.46, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1104\n",
            "Tr: 10.00,Beam: 17, Iteration: 553, Q value: 0.1590, Reward: -1.0000, BF Gain pred: -12.64, BF Gain: -6.54, BFtrue Gain pred: -14.95, BFtrue Gain: -6.51,Critic Loss: 0.46, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1106\n",
            "Tr: 10.00,Beam: 17, Iteration: 554, Q value: 0.1595, Reward: 1.0000, BF Gain pred: -4.22, BF Gain: -8.83, BFtrue Gain pred: -7.66, BFtrue Gain: -9.26,Critic Loss: 0.45, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1108\n",
            "Tr: 10.00,Beam: 17, Iteration: 555, Q value: 0.1600, Reward: -1.0000, BF Gain pred: -10.71, BF Gain: -5.64, BFtrue Gain pred: -10.83, BFtrue Gain: -5.27,Critic Loss: 0.48, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1110\n",
            "Tr: 10.00,Beam: 17, Iteration: 556, Q value: 0.1605, Reward: 1.0000, BF Gain pred: 3.02, BF Gain: -2.68, BFtrue Gain pred: -0.71, BFtrue Gain: -2.48,Critic Loss: 0.45, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1112\n",
            "Tr: 10.00,Beam: 17, Iteration: 557, Q value: 0.1610, Reward: -1.0000, BF Gain pred: -8.66, BF Gain: -4.56, BFtrue Gain pred: -13.15, BFtrue Gain: -4.52,Critic Loss: 0.47, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1114\n",
            "Tr: 10.00,Beam: 17, Iteration: 558, Q value: 0.1615, Reward: 1.0000, BF Gain pred: -5.73, BF Gain: -2.21, BFtrue Gain pred: -9.97, BFtrue Gain: -2.40,Critic Loss: 0.44, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1116\n",
            "Tr: 10.00,Beam: 17, Iteration: 559, Q value: 0.1620, Reward: -1.0000, BF Gain pred: -11.83, BF Gain: -0.30, BFtrue Gain pred: -8.71, BFtrue Gain: 0.83,Critic Loss: 0.49, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1118\n",
            "Tr: 10.00,Beam: 17, Iteration: 560, Q value: 0.1626, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: -9.20, BFtrue Gain pred: -5.99, BFtrue Gain: -8.98,Critic Loss: 0.49, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1120\n",
            "Tr: 10.00,Beam: 17, Iteration: 561, Q value: 0.1590, Reward: -1.0000, BF Gain pred: -9.74, BF Gain: -7.94, BFtrue Gain pred: -6.29, BFtrue Gain: -9.00,Critic Loss: 0.43, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1122\n",
            "Tr: 10.00,Beam: 17, Iteration: 562, Q value: 0.1636, Reward: 1.0000, BF Gain pred: -7.20, BF Gain: -12.67, BFtrue Gain pred: -11.47, BFtrue Gain: -12.94,Critic Loss: 0.45, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1124\n",
            "Tr: 10.00,Beam: 17, Iteration: 563, Q value: 0.1641, Reward: -1.0000, BF Gain pred: -17.22, BF Gain: -13.35, BFtrue Gain pred: -12.73, BFtrue Gain: -14.01,Critic Loss: 0.44, Policy Loss: -0.16\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1126\n",
            "Tr: 10.00,Beam: 17, Iteration: 564, Q value: 0.1646, Reward: 1.0000, BF Gain pred: -11.03, BF Gain: -13.93, BFtrue Gain pred: -16.95, BFtrue Gain: -12.68,Critic Loss: 0.49, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1128\n",
            "Tr: 10.00,Beam: 17, Iteration: 565, Q value: 0.1652, Reward: 1.0000, BF Gain pred: -2.73, BF Gain: -5.44, BFtrue Gain pred: -2.05, BFtrue Gain: -6.09,Critic Loss: 0.45, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1130\n",
            "Tr: 10.00,Beam: 17, Iteration: 566, Q value: 0.1658, Reward: -1.0000, BF Gain pred: -6.34, BF Gain: -8.60, BFtrue Gain pred: -5.37, BFtrue Gain: -8.33,Critic Loss: 0.45, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1132\n",
            "Tr: 10.00,Beam: 17, Iteration: 567, Q value: 0.1663, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -5.58, BFtrue Gain pred: -10.32, BFtrue Gain: -5.98,Critic Loss: 0.43, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1134\n",
            "Tr: 10.00,Beam: 17, Iteration: 568, Q value: 0.1669, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -9.15, BFtrue Gain pred: -12.52, BFtrue Gain: -8.63,Critic Loss: 0.50, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1136\n",
            "Tr: 10.00,Beam: 17, Iteration: 569, Q value: 0.1674, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -9.03, BFtrue Gain pred: -11.77, BFtrue Gain: -9.27,Critic Loss: 0.49, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1138\n",
            "Tr: 10.00,Beam: 17, Iteration: 570, Q value: 0.1680, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -11.99, BFtrue Gain pred: -11.39, BFtrue Gain: -11.11,Critic Loss: 0.50, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1140\n",
            "Tr: 10.00,Beam: 17, Iteration: 571, Q value: 0.1686, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -11.33, BFtrue Gain pred: -10.80, BFtrue Gain: -11.47,Critic Loss: 0.48, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1142\n",
            "Tr: 10.00,Beam: 17, Iteration: 572, Q value: 0.1692, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -9.58, BFtrue Gain pred: -10.55, BFtrue Gain: -10.14,Critic Loss: 0.48, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1144\n",
            "Tr: 10.00,Beam: 17, Iteration: 573, Q value: 0.1698, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -8.00, BFtrue Gain pred: -10.35, BFtrue Gain: -7.10,Critic Loss: 0.53, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1146\n",
            "Tr: 10.00,Beam: 17, Iteration: 574, Q value: 0.1704, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -1.64, BFtrue Gain pred: -10.00, BFtrue Gain: 0.54,Critic Loss: 0.48, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1148\n",
            "Tr: 10.00,Beam: 17, Iteration: 575, Q value: 0.1711, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -4.79, BFtrue Gain pred: -9.64, BFtrue Gain: -3.89,Critic Loss: 0.43, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1150\n",
            "Tr: 10.00,Beam: 17, Iteration: 576, Q value: 0.1717, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -6.28, BFtrue Gain pred: -9.55, BFtrue Gain: -6.02,Critic Loss: 0.50, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1152\n",
            "Tr: 10.00,Beam: 17, Iteration: 577, Q value: 0.1723, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -11.39, BFtrue Gain pred: -9.50, BFtrue Gain: -11.61,Critic Loss: 0.51, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1154\n",
            "Tr: 10.00,Beam: 17, Iteration: 578, Q value: 0.1729, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -5.67, BFtrue Gain pred: -9.28, BFtrue Gain: -5.53,Critic Loss: 0.53, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1156\n",
            "Tr: 10.00,Beam: 17, Iteration: 579, Q value: 0.1734, Reward: -1.0000, BF Gain pred: -8.47, BF Gain: -10.40, BFtrue Gain pred: -8.83, BFtrue Gain: -9.25,Critic Loss: 0.54, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1158\n",
            "Tr: 10.00,Beam: 17, Iteration: 580, Q value: 0.1740, Reward: 1.0000, BF Gain pred: -5.29, BF Gain: -7.22, BFtrue Gain pred: -8.41, BFtrue Gain: -6.53,Critic Loss: 0.47, Policy Loss: -0.17\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1160\n",
            "Tr: 10.00,Beam: 17, Iteration: 581, Q value: 0.1745, Reward: 1.0000, BF Gain pred: -4.11, BF Gain: -5.79, BFtrue Gain pred: -6.44, BFtrue Gain: -5.03,Critic Loss: 0.51, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1162\n",
            "Tr: 10.00,Beam: 17, Iteration: 582, Q value: 0.1750, Reward: -1.0000, BF Gain pred: -12.40, BF Gain: -7.65, BFtrue Gain pred: -10.93, BFtrue Gain: -7.66,Critic Loss: 0.50, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1164\n",
            "Tr: 10.00,Beam: 17, Iteration: 583, Q value: 0.1755, Reward: 1.0000, BF Gain pred: -7.52, BF Gain: -12.42, BFtrue Gain pred: -9.50, BFtrue Gain: -11.95,Critic Loss: 0.56, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1166\n",
            "Tr: 10.00,Beam: 17, Iteration: 584, Q value: 0.1760, Reward: -1.0000, BF Gain pred: -11.38, BF Gain: -4.70, BFtrue Gain pred: -13.55, BFtrue Gain: -3.96,Critic Loss: 0.51, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1168\n",
            "Tr: 10.00,Beam: 17, Iteration: 585, Q value: 0.1766, Reward: -1.0000, BF Gain pred: -11.88, BF Gain: -7.76, BFtrue Gain pred: -11.08, BFtrue Gain: -7.24,Critic Loss: 0.47, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1170\n",
            "Tr: 10.00,Beam: 17, Iteration: 586, Q value: 0.1771, Reward: 1.0000, BF Gain pred: -4.12, BF Gain: -7.10, BFtrue Gain pred: -7.65, BFtrue Gain: -6.47,Critic Loss: 0.45, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1172\n",
            "Tr: 10.00,Beam: 17, Iteration: 587, Q value: 0.1555, Reward: -1.0000, BF Gain pred: -9.32, BF Gain: -11.31, BFtrue Gain pred: -12.12, BFtrue Gain: -10.91,Critic Loss: 0.46, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1174\n",
            "Tr: 10.00,Beam: 17, Iteration: 588, Q value: 0.1780, Reward: 1.0000, BF Gain pred: -5.68, BF Gain: -5.34, BFtrue Gain pred: -7.48, BFtrue Gain: -5.28,Critic Loss: 0.50, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1176\n",
            "Tr: 10.00,Beam: 17, Iteration: 589, Q value: 0.1785, Reward: -1.0000, BF Gain pred: -6.84, BF Gain: -14.67, BFtrue Gain pred: -5.85, BFtrue Gain: -14.39,Critic Loss: 0.43, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1178\n",
            "Tr: 10.00,Beam: 17, Iteration: 590, Q value: 0.1790, Reward: -1.0000, BF Gain pred: -7.20, BF Gain: -6.27, BFtrue Gain pred: -7.21, BFtrue Gain: -6.30,Critic Loss: 0.46, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1180\n",
            "Tr: 10.00,Beam: 17, Iteration: 591, Q value: 0.1796, Reward: -1.0000, BF Gain pred: -9.55, BF Gain: -6.53, BFtrue Gain pred: -10.01, BFtrue Gain: -6.74,Critic Loss: 0.48, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1182\n",
            "Tr: 10.00,Beam: 17, Iteration: 592, Q value: 0.1801, Reward: -1.0000, BF Gain pred: -10.05, BF Gain: -3.83, BFtrue Gain pred: -10.58, BFtrue Gain: -3.65,Critic Loss: 0.46, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1184\n",
            "Tr: 10.00,Beam: 17, Iteration: 593, Q value: 0.1806, Reward: 1.0000, BF Gain pred: -6.55, BF Gain: -1.35, BFtrue Gain pred: -13.08, BFtrue Gain: -0.83,Critic Loss: 0.46, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1186\n",
            "Tr: 10.00,Beam: 17, Iteration: 594, Q value: 0.1811, Reward: -1.0000, BF Gain pred: -6.92, BF Gain: -9.62, BFtrue Gain pred: -11.09, BFtrue Gain: -9.24,Critic Loss: 0.47, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1188\n",
            "Tr: 10.00,Beam: 17, Iteration: 595, Q value: 0.1817, Reward: -1.0000, BF Gain pred: -6.92, BF Gain: -5.20, BFtrue Gain pred: -11.21, BFtrue Gain: -5.91,Critic Loss: 0.45, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1190\n",
            "Tr: 10.00,Beam: 17, Iteration: 596, Q value: 0.1822, Reward: 1.0000, BF Gain pred: -6.57, BF Gain: 0.50, BFtrue Gain pred: -14.94, BFtrue Gain: 1.05,Critic Loss: 0.44, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1192\n",
            "Tr: 10.00,Beam: 17, Iteration: 597, Q value: 0.1828, Reward: -1.0000, BF Gain pred: -14.61, BF Gain: 0.46, BFtrue Gain pred: -23.24, BFtrue Gain: 0.50,Critic Loss: 0.44, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1194\n",
            "Tr: 10.00,Beam: 17, Iteration: 598, Q value: 0.1834, Reward: -1.0000, BF Gain pred: -14.61, BF Gain: 4.50, BFtrue Gain pred: -21.68, BFtrue Gain: 3.69,Critic Loss: 0.47, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1196\n",
            "Tr: 10.00,Beam: 17, Iteration: 599, Q value: 0.1839, Reward: -1.0000, BF Gain pred: -15.70, BF Gain: 8.05, BFtrue Gain pred: -22.07, BFtrue Gain: 8.94,Critic Loss: 0.42, Policy Loss: -0.18\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1198\n",
            "Tr: 10.00,Beam: 17, Iteration: 600, Q value: 0.1845, Reward: 1.0000, BF Gain pred: -9.30, BF Gain: 2.28, BFtrue Gain pred: -34.41, BFtrue Gain: 1.76,Critic Loss: 0.43, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1200\n",
            "Tr: 10.00,Beam: 17, Iteration: 601, Q value: 0.1851, Reward: -1.0000, BF Gain pred: -12.40, BF Gain: -3.52, BFtrue Gain pred: -26.48, BFtrue Gain: -3.61,Critic Loss: 0.41, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1202\n",
            "Tr: 10.00,Beam: 17, Iteration: 602, Q value: 0.1856, Reward: 1.0000, BF Gain pred: -2.27, BF Gain: -2.06, BFtrue Gain pred: -15.32, BFtrue Gain: -3.24,Critic Loss: 0.46, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1204\n",
            "Tr: 10.00,Beam: 17, Iteration: 603, Q value: 0.1862, Reward: -1.0000, BF Gain pred: -2.27, BF Gain: -9.54, BFtrue Gain pred: -20.19, BFtrue Gain: -8.67,Critic Loss: 0.41, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1206\n",
            "Tr: 10.00,Beam: 17, Iteration: 604, Q value: 0.1867, Reward: -1.0000, BF Gain pred: -2.27, BF Gain: -7.38, BFtrue Gain pred: -19.68, BFtrue Gain: -7.24,Critic Loss: 0.46, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1208\n",
            "Tr: 10.00,Beam: 17, Iteration: 605, Q value: 0.1873, Reward: -1.0000, BF Gain pred: -2.27, BF Gain: -5.72, BFtrue Gain pred: -17.01, BFtrue Gain: -5.06,Critic Loss: 0.45, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1210\n",
            "Tr: 10.00,Beam: 17, Iteration: 606, Q value: 0.1878, Reward: -1.0000, BF Gain pred: -2.27, BF Gain: -9.98, BFtrue Gain pred: -14.46, BFtrue Gain: -9.48,Critic Loss: 0.49, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1212\n",
            "Tr: 10.00,Beam: 17, Iteration: 607, Q value: 0.1883, Reward: -1.0000, BF Gain pred: -2.27, BF Gain: -8.89, BFtrue Gain pred: -12.81, BFtrue Gain: -8.26,Critic Loss: 0.42, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1214\n",
            "Tr: 10.00,Beam: 17, Iteration: 608, Q value: 0.1888, Reward: -1.0000, BF Gain pred: -2.27, BF Gain: -14.67, BFtrue Gain pred: -11.61, BFtrue Gain: -12.68,Critic Loss: 0.47, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1216\n",
            "Tr: 10.00,Beam: 17, Iteration: 609, Q value: 0.1894, Reward: 1.0000, BF Gain pred: -1.98, BF Gain: -2.40, BFtrue Gain pred: -10.87, BFtrue Gain: -1.13,Critic Loss: 0.49, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1218\n",
            "Tr: 10.00,Beam: 17, Iteration: 610, Q value: 0.1899, Reward: 1.0000, BF Gain pred: -1.72, BF Gain: -18.44, BFtrue Gain pred: -10.31, BFtrue Gain: -16.69,Critic Loss: 0.51, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1220\n",
            "Tr: 10.00,Beam: 17, Iteration: 611, Q value: 0.1904, Reward: -1.0000, BF Gain pred: -1.72, BF Gain: -6.07, BFtrue Gain pred: -11.45, BFtrue Gain: -6.17,Critic Loss: 0.47, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1222\n",
            "Tr: 10.00,Beam: 17, Iteration: 612, Q value: 0.1909, Reward: -1.0000, BF Gain pred: -7.17, BF Gain: -5.21, BFtrue Gain pred: -9.47, BFtrue Gain: -5.18,Critic Loss: 0.50, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1224\n",
            "Tr: 10.00,Beam: 17, Iteration: 613, Q value: 0.1914, Reward: 1.0000, BF Gain pred: -4.68, BF Gain: -5.03, BFtrue Gain pred: -8.46, BFtrue Gain: -4.49,Critic Loss: 0.43, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1226\n",
            "Tr: 10.00,Beam: 17, Iteration: 614, Q value: 0.1913, Reward: -1.0000, BF Gain pred: -9.90, BF Gain: -5.26, BFtrue Gain pred: -13.63, BFtrue Gain: -4.20,Critic Loss: 0.42, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1228\n",
            "Tr: 10.00,Beam: 17, Iteration: 615, Q value: 0.1856, Reward: 1.0000, BF Gain pred: -6.89, BF Gain: -5.19, BFtrue Gain pred: -8.77, BFtrue Gain: -5.14,Critic Loss: 0.45, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1230\n",
            "Tr: 10.00,Beam: 17, Iteration: 616, Q value: 0.0898, Reward: -1.0000, BF Gain pred: -13.84, BF Gain: -10.31, BFtrue Gain pred: -12.11, BFtrue Gain: -10.23,Critic Loss: 0.46, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1232\n",
            "Tr: 10.00,Beam: 17, Iteration: 617, Q value: 0.0202, Reward: 1.0000, BF Gain pred: -9.01, BF Gain: -1.93, BFtrue Gain pred: -9.44, BFtrue Gain: -1.17,Critic Loss: 0.44, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1234\n",
            "Tr: 10.00,Beam: 17, Iteration: 618, Q value: 0.0996, Reward: -1.0000, BF Gain pred: -11.96, BF Gain: -3.38, BFtrue Gain pred: -9.42, BFtrue Gain: -2.86,Critic Loss: 0.45, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1236\n",
            "Tr: 10.00,Beam: 17, Iteration: 619, Q value: 0.1947, Reward: 1.0000, BF Gain pred: -8.79, BF Gain: -1.90, BFtrue Gain pred: -9.95, BFtrue Gain: -2.95,Critic Loss: 0.46, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1238\n",
            "Tr: 10.00,Beam: 17, Iteration: 620, Q value: 0.1837, Reward: -1.0000, BF Gain pred: -11.47, BF Gain: 1.48, BFtrue Gain pred: -8.38, BFtrue Gain: 0.16,Critic Loss: 0.46, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1240\n",
            "Tr: 10.00,Beam: 17, Iteration: 621, Q value: -0.2809, Reward: -1.0000, BF Gain pred: -27.64, BF Gain: -12.62, BFtrue Gain pred: -10.47, BFtrue Gain: -11.60,Critic Loss: 0.44, Policy Loss: -0.19\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1242\n",
            "Tr: 10.00,Beam: 17, Iteration: 622, Q value: 0.1848, Reward: 1.0000, BF Gain pred: -12.31, BF Gain: 0.76, BFtrue Gain pred: -6.94, BFtrue Gain: 1.00,Critic Loss: 0.41, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1244\n",
            "Tr: 10.00,Beam: 17, Iteration: 623, Q value: 0.1968, Reward: 1.0000, BF Gain pred: -9.02, BF Gain: -2.07, BFtrue Gain pred: -6.12, BFtrue Gain: -1.68,Critic Loss: 0.45, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1246\n",
            "Tr: 10.00,Beam: 17, Iteration: 624, Q value: 0.1803, Reward: -1.0000, BF Gain pred: -9.36, BF Gain: -3.66, BFtrue Gain pred: -8.06, BFtrue Gain: -2.74,Critic Loss: 0.42, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1248\n",
            "Tr: 10.00,Beam: 17, Iteration: 625, Q value: 0.1978, Reward: -1.0000, BF Gain pred: -10.61, BF Gain: -3.29, BFtrue Gain pred: -8.91, BFtrue Gain: -2.49,Critic Loss: 0.43, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1250\n",
            "Tr: 10.00,Beam: 17, Iteration: 626, Q value: 0.1983, Reward: 1.0000, BF Gain pred: -8.13, BF Gain: -5.66, BFtrue Gain pred: -6.83, BFtrue Gain: -7.07,Critic Loss: 0.43, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1252\n",
            "Tr: 10.00,Beam: 17, Iteration: 627, Q value: 0.1988, Reward: 1.0000, BF Gain pred: -0.92, BF Gain: -2.39, BFtrue Gain pred: -4.41, BFtrue Gain: -2.85,Critic Loss: 0.42, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1254\n",
            "Tr: 10.00,Beam: 17, Iteration: 628, Q value: 0.1993, Reward: 1.0000, BF Gain pred: -0.89, BF Gain: -7.01, BFtrue Gain pred: -7.70, BFtrue Gain: -6.84,Critic Loss: 0.42, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1256\n",
            "Tr: 10.00,Beam: 17, Iteration: 629, Q value: 0.1998, Reward: -1.0000, BF Gain pred: -8.13, BF Gain: -13.33, BFtrue Gain pred: -19.66, BFtrue Gain: -13.60,Critic Loss: 0.37, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1258\n",
            "Tr: 10.00,Beam: 17, Iteration: 630, Q value: 0.1571, Reward: 1.0000, BF Gain pred: -1.35, BF Gain: -1.22, BFtrue Gain pred: -4.94, BFtrue Gain: -2.27,Critic Loss: 0.44, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1260\n",
            "Tr: 10.00,Beam: 17, Iteration: 631, Q value: 0.2009, Reward: -1.0000, BF Gain pred: -4.05, BF Gain: -6.78, BFtrue Gain pred: -14.16, BFtrue Gain: -6.38,Critic Loss: 0.40, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1262\n",
            "Tr: 10.00,Beam: 17, Iteration: 632, Q value: 0.0625, Reward: 1.0000, BF Gain pred: -2.89, BF Gain: -11.92, BFtrue Gain pred: -12.66, BFtrue Gain: -12.46,Critic Loss: 0.43, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1264\n",
            "Tr: 10.00,Beam: 17, Iteration: 633, Q value: 0.1836, Reward: -1.0000, BF Gain pred: -4.26, BF Gain: -8.40, BFtrue Gain pred: -8.09, BFtrue Gain: -9.10,Critic Loss: 0.42, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1266\n",
            "Tr: 10.00,Beam: 17, Iteration: 634, Q value: 0.2025, Reward: -1.0000, BF Gain pred: -12.50, BF Gain: -8.60, BFtrue Gain pred: -11.06, BFtrue Gain: -8.14,Critic Loss: 0.42, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1268\n",
            "Tr: 10.00,Beam: 17, Iteration: 635, Q value: 0.2031, Reward: 1.0000, BF Gain pred: -7.82, BF Gain: -9.50, BFtrue Gain pred: -11.51, BFtrue Gain: -9.23,Critic Loss: 0.46, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1270\n",
            "Tr: 10.00,Beam: 17, Iteration: 636, Q value: 0.2036, Reward: 1.0000, BF Gain pred: -3.75, BF Gain: -0.80, BFtrue Gain pred: -5.60, BFtrue Gain: 0.96,Critic Loss: 0.42, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1272\n",
            "Tr: 10.00,Beam: 17, Iteration: 637, Q value: 0.2042, Reward: -1.0000, BF Gain pred: -5.41, BF Gain: -2.23, BFtrue Gain pred: -7.61, BFtrue Gain: -3.02,Critic Loss: 0.39, Policy Loss: -0.20\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1274\n",
            "Tr: 10.00,Beam: 17, Iteration: 638, Q value: 0.0558, Reward: 1.0000, BF Gain pred: -0.66, BF Gain: -14.89, BFtrue Gain pred: -8.26, BFtrue Gain: -14.37,Critic Loss: 0.39, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1276\n",
            "Tr: 10.00,Beam: 17, Iteration: 639, Q value: 0.2031, Reward: -1.0000, BF Gain pred: -3.54, BF Gain: 0.34, BFtrue Gain pred: -4.52, BFtrue Gain: 1.84,Critic Loss: 0.42, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1278\n",
            "Tr: 10.00,Beam: 17, Iteration: 640, Q value: 0.2059, Reward: 1.0000, BF Gain pred: 6.59, BF Gain: -9.32, BFtrue Gain pred: -1.50, BFtrue Gain: -9.57,Critic Loss: 0.40, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1280\n",
            "Tr: 10.00,Beam: 17, Iteration: 641, Q value: 0.2064, Reward: -1.0000, BF Gain pred: 0.22, BF Gain: 3.60, BFtrue Gain pred: -3.59, BFtrue Gain: 2.12,Critic Loss: 0.41, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1282\n",
            "Tr: 10.00,Beam: 17, Iteration: 642, Q value: 0.2070, Reward: -1.0000, BF Gain pred: -6.45, BF Gain: -2.36, BFtrue Gain pred: -5.54, BFtrue Gain: -3.11,Critic Loss: 0.40, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1284\n",
            "Tr: 10.00,Beam: 17, Iteration: 643, Q value: 0.2076, Reward: 1.0000, BF Gain pred: -3.74, BF Gain: 0.82, BFtrue Gain pred: -3.83, BFtrue Gain: 0.68,Critic Loss: 0.46, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1286\n",
            "Tr: 10.00,Beam: 17, Iteration: 644, Q value: -0.5843, Reward: 1.0000, BF Gain pred: 1.46, BF Gain: -2.83, BFtrue Gain pred: -1.91, BFtrue Gain: -3.26,Critic Loss: 0.43, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1288\n",
            "Tr: 10.00,Beam: 17, Iteration: 645, Q value: 0.2088, Reward: -1.0000, BF Gain pred: -3.45, BF Gain: -12.94, BFtrue Gain pred: -4.04, BFtrue Gain: -13.00,Critic Loss: 0.42, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1290\n",
            "Tr: 10.00,Beam: 17, Iteration: 646, Q value: 0.0316, Reward: -1.0000, BF Gain pred: -5.10, BF Gain: -7.75, BFtrue Gain pred: -9.03, BFtrue Gain: -9.04,Critic Loss: 0.42, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1292\n",
            "Tr: 10.00,Beam: 17, Iteration: 647, Q value: 0.2101, Reward: 1.0000, BF Gain pred: -2.32, BF Gain: -2.99, BFtrue Gain pred: -6.84, BFtrue Gain: -2.54,Critic Loss: 0.40, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1294\n",
            "Tr: 10.00,Beam: 17, Iteration: 648, Q value: 0.1898, Reward: 1.0000, BF Gain pred: -2.31, BF Gain: -10.75, BFtrue Gain pred: -8.84, BFtrue Gain: -11.41,Critic Loss: 0.42, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1296\n",
            "Tr: 10.00,Beam: 17, Iteration: 649, Q value: 0.2114, Reward: -1.0000, BF Gain pred: -3.81, BF Gain: -6.86, BFtrue Gain pred: -6.32, BFtrue Gain: -6.57,Critic Loss: 0.40, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1298\n",
            "Tr: 10.00,Beam: 17, Iteration: 650, Q value: 0.1539, Reward: 1.0000, BF Gain pred: 0.49, BF Gain: -12.74, BFtrue Gain pred: -6.11, BFtrue Gain: -13.06,Critic Loss: 0.40, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1300\n",
            "Tr: 10.00,Beam: 17, Iteration: 651, Q value: 0.2128, Reward: -1.0000, BF Gain pred: -1.08, BF Gain: -8.11, BFtrue Gain pred: -5.56, BFtrue Gain: -8.66,Critic Loss: 0.43, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1302\n",
            "Tr: 10.00,Beam: 17, Iteration: 652, Q value: 0.1525, Reward: -1.0000, BF Gain pred: -3.79, BF Gain: -12.90, BFtrue Gain pred: -5.92, BFtrue Gain: -13.17,Critic Loss: 0.40, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1304\n",
            "Tr: 10.00,Beam: 17, Iteration: 653, Q value: 0.1343, Reward: -1.0000, BF Gain pred: -4.59, BF Gain: -9.12, BFtrue Gain pred: -8.04, BFtrue Gain: -10.12,Critic Loss: 0.41, Policy Loss: -0.21\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1306\n",
            "Tr: 10.00,Beam: 17, Iteration: 654, Q value: 0.0578, Reward: 1.0000, BF Gain pred: -3.77, BF Gain: -1.17, BFtrue Gain pred: -6.36, BFtrue Gain: -1.97,Critic Loss: 0.38, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1308\n",
            "Tr: 10.00,Beam: 17, Iteration: 655, Q value: -0.8200, Reward: 1.0000, BF Gain pred: -0.59, BF Gain: -5.99, BFtrue Gain pred: -8.39, BFtrue Gain: -5.50,Critic Loss: 0.42, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1310\n",
            "Tr: 10.00,Beam: 17, Iteration: 656, Q value: -0.0374, Reward: -1.0000, BF Gain pred: -1.91, BF Gain: -10.48, BFtrue Gain pred: -6.83, BFtrue Gain: -11.18,Critic Loss: 0.39, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1312\n",
            "Tr: 10.00,Beam: 17, Iteration: 657, Q value: 0.1026, Reward: 1.0000, BF Gain pred: -0.63, BF Gain: 0.40, BFtrue Gain pred: -4.41, BFtrue Gain: 0.14,Critic Loss: 0.43, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1314\n",
            "Tr: 10.00,Beam: 17, Iteration: 658, Q value: 0.2173, Reward: 1.0000, BF Gain pred: -0.56, BF Gain: 0.15, BFtrue Gain pred: -2.07, BFtrue Gain: 0.40,Critic Loss: 0.39, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1316\n",
            "Tr: 10.00,Beam: 17, Iteration: 659, Q value: 0.2180, Reward: 1.0000, BF Gain pred: 1.74, BF Gain: 0.94, BFtrue Gain pred: -2.13, BFtrue Gain: -0.06,Critic Loss: 0.40, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1318\n",
            "Tr: 10.00,Beam: 17, Iteration: 660, Q value: 0.2186, Reward: 1.0000, BF Gain pred: 1.92, BF Gain: 4.98, BFtrue Gain pred: 1.13, BFtrue Gain: 5.43,Critic Loss: 0.38, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1320\n",
            "Tr: 10.00,Beam: 17, Iteration: 661, Q value: 0.2192, Reward: 1.0000, BF Gain pred: 4.88, BF Gain: -3.16, BFtrue Gain pred: 1.03, BFtrue Gain: -4.18,Critic Loss: 0.39, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1322\n",
            "Tr: 10.00,Beam: 17, Iteration: 662, Q value: 0.2199, Reward: -1.0000, BF Gain pred: 4.88, BF Gain: -7.79, BFtrue Gain pred: 2.23, BFtrue Gain: -7.92,Critic Loss: 0.39, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1324\n",
            "Tr: 10.00,Beam: 17, Iteration: 663, Q value: 0.2205, Reward: -1.0000, BF Gain pred: 4.88, BF Gain: 5.87, BFtrue Gain pred: 3.22, BFtrue Gain: 5.49,Critic Loss: 0.41, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1326\n",
            "Tr: 10.00,Beam: 17, Iteration: 664, Q value: 0.2211, Reward: 1.0000, BF Gain pred: 5.84, BF Gain: 3.85, BFtrue Gain pred: 4.02, BFtrue Gain: 1.96,Critic Loss: 0.40, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1328\n",
            "Tr: 10.00,Beam: 17, Iteration: 665, Q value: 0.2218, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: -4.15, BFtrue Gain pred: 2.99, BFtrue Gain: -3.74,Critic Loss: 0.44, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1330\n",
            "Tr: 10.00,Beam: 17, Iteration: 666, Q value: 0.2224, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: 7.74, BFtrue Gain pred: 3.44, BFtrue Gain: 6.74,Critic Loss: 0.45, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1332\n",
            "Tr: 10.00,Beam: 17, Iteration: 667, Q value: 0.2230, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: -0.85, BFtrue Gain pred: 3.83, BFtrue Gain: -0.40,Critic Loss: 0.37, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1334\n",
            "Tr: 10.00,Beam: 17, Iteration: 668, Q value: 0.2236, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: 6.22, BFtrue Gain pred: 4.23, BFtrue Gain: 7.15,Critic Loss: 0.41, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1336\n",
            "Tr: 10.00,Beam: 17, Iteration: 669, Q value: 0.2241, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: 3.22, BFtrue Gain pred: 4.55, BFtrue Gain: 0.22,Critic Loss: 0.43, Policy Loss: -0.22\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1338\n",
            "Tr: 10.00,Beam: 17, Iteration: 670, Q value: 0.2247, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: 1.85, BFtrue Gain pred: 4.76, BFtrue Gain: 0.24,Critic Loss: 0.44, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1340\n",
            "Tr: 10.00,Beam: 17, Iteration: 671, Q value: 0.2253, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: -6.11, BFtrue Gain pred: 5.12, BFtrue Gain: -7.25,Critic Loss: 0.44, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1342\n",
            "Tr: 10.00,Beam: 17, Iteration: 672, Q value: 0.2259, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: -4.39, BFtrue Gain pred: 5.19, BFtrue Gain: -5.80,Critic Loss: 0.46, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1344\n",
            "Tr: 10.00,Beam: 17, Iteration: 673, Q value: 0.2264, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: 0.78, BFtrue Gain pred: 5.28, BFtrue Gain: 0.15,Critic Loss: 0.46, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1346\n",
            "Tr: 10.00,Beam: 17, Iteration: 674, Q value: 0.2270, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: -1.94, BFtrue Gain pred: 5.36, BFtrue Gain: -3.22,Critic Loss: 0.44, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1348\n",
            "Tr: 10.00,Beam: 17, Iteration: 675, Q value: 0.1893, Reward: -1.0000, BF Gain pred: 5.84, BF Gain: -2.02, BFtrue Gain pred: 5.53, BFtrue Gain: -2.88,Critic Loss: 0.43, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1350\n",
            "Tr: 10.00,Beam: 17, Iteration: 676, Q value: 0.1630, Reward: 1.0000, BF Gain pred: 8.45, BF Gain: 0.82, BFtrue Gain pred: 5.75, BFtrue Gain: -0.54,Critic Loss: 0.42, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1352\n",
            "Tr: 10.00,Beam: 17, Iteration: 677, Q value: 0.0574, Reward: 1.0000, BF Gain pred: 9.16, BF Gain: -3.05, BFtrue Gain pred: 6.06, BFtrue Gain: -3.30,Critic Loss: 0.40, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1354\n",
            "Tr: 10.00,Beam: 17, Iteration: 678, Q value: 0.1466, Reward: -1.0000, BF Gain pred: 8.45, BF Gain: -6.93, BFtrue Gain pred: 5.14, BFtrue Gain: -7.73,Critic Loss: 0.43, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1356\n",
            "Tr: 10.00,Beam: 17, Iteration: 679, Q value: 0.2298, Reward: -1.0000, BF Gain pred: 7.66, BF Gain: 5.81, BFtrue Gain pred: 4.41, BFtrue Gain: 3.81,Critic Loss: 0.44, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1358\n",
            "Tr: 10.00,Beam: 17, Iteration: 680, Q value: 0.2303, Reward: -1.0000, BF Gain pred: 4.27, BF Gain: 1.87, BFtrue Gain pred: -0.32, BFtrue Gain: -0.11,Critic Loss: 0.44, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1360\n",
            "Tr: 10.00,Beam: 17, Iteration: 681, Q value: 0.1024, Reward: 1.0000, BF Gain pred: 8.39, BF Gain: -5.71, BFtrue Gain pred: 7.37, BFtrue Gain: -4.84,Critic Loss: 0.42, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1362\n",
            "Tr: 10.00,Beam: 17, Iteration: 682, Q value: 0.1583, Reward: -1.0000, BF Gain pred: 0.68, BF Gain: -7.53, BFtrue Gain pred: -2.04, BFtrue Gain: -7.57,Critic Loss: 0.41, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1364\n",
            "Tr: 10.00,Beam: 17, Iteration: 683, Q value: 0.0608, Reward: 1.0000, BF Gain pred: 5.40, BF Gain: -7.79, BFtrue Gain pred: 3.47, BFtrue Gain: -8.23,Critic Loss: 0.42, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1366\n",
            "Tr: 10.00,Beam: 17, Iteration: 684, Q value: 0.2325, Reward: -1.0000, BF Gain pred: -1.37, BF Gain: 2.22, BFtrue Gain pred: -2.31, BFtrue Gain: 3.81,Critic Loss: 0.45, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1368\n",
            "Tr: 10.00,Beam: 17, Iteration: 685, Q value: 0.1553, Reward: 1.0000, BF Gain pred: 5.40, BF Gain: 0.94, BFtrue Gain pred: 2.66, BFtrue Gain: 1.73,Critic Loss: 0.44, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1370\n",
            "Tr: 10.00,Beam: 17, Iteration: 686, Q value: 0.2336, Reward: -1.0000, BF Gain pred: -2.95, BF Gain: 2.05, BFtrue Gain pred: -2.50, BFtrue Gain: 2.30,Critic Loss: 0.40, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1372\n",
            "Tr: 10.00,Beam: 17, Iteration: 687, Q value: 0.1793, Reward: 1.0000, BF Gain pred: 1.35, BF Gain: 5.30, BFtrue Gain pred: 2.81, BFtrue Gain: 5.50,Critic Loss: 0.43, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1374\n",
            "Tr: 10.00,Beam: 17, Iteration: 688, Q value: 0.2035, Reward: -1.0000, BF Gain pred: -0.89, BF Gain: 2.56, BFtrue Gain pred: -2.78, BFtrue Gain: 2.53,Critic Loss: 0.40, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1376\n",
            "Tr: 10.00,Beam: 17, Iteration: 689, Q value: 0.1342, Reward: 1.0000, BF Gain pred: 5.72, BF Gain: 1.33, BFtrue Gain pred: 1.83, BFtrue Gain: 0.15,Critic Loss: 0.42, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1378\n",
            "Tr: 10.00,Beam: 17, Iteration: 690, Q value: -0.0395, Reward: -1.0000, BF Gain pred: -2.38, BF Gain: -1.31, BFtrue Gain pred: -4.05, BFtrue Gain: -1.16,Critic Loss: 0.44, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1380\n",
            "Tr: 10.00,Beam: 17, Iteration: 691, Q value: 0.0985, Reward: 1.0000, BF Gain pred: 4.05, BF Gain: -4.02, BFtrue Gain pred: -3.28, BFtrue Gain: -3.61,Critic Loss: 0.48, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1382\n",
            "Tr: 10.00,Beam: 17, Iteration: 692, Q value: -1.0906, Reward: -1.0000, BF Gain pred: -4.47, BF Gain: -0.55, BFtrue Gain pred: -2.63, BFtrue Gain: -1.32,Critic Loss: 0.44, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1384\n",
            "Tr: 10.00,Beam: 17, Iteration: 693, Q value: -0.0283, Reward: 1.0000, BF Gain pred: 0.48, BF Gain: 1.63, BFtrue Gain pred: -7.17, BFtrue Gain: 1.41,Critic Loss: 0.40, Policy Loss: -0.23\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1386\n",
            "Tr: 10.00,Beam: 17, Iteration: 694, Q value: -2.2544, Reward: -1.0000, BF Gain pred: -5.50, BF Gain: -4.83, BFtrue Gain pred: -4.04, BFtrue Gain: -4.61,Critic Loss: 0.41, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1388\n",
            "Tr: 10.00,Beam: 17, Iteration: 695, Q value: 0.1783, Reward: 1.0000, BF Gain pred: -3.38, BF Gain: -4.58, BFtrue Gain pred: -15.70, BFtrue Gain: -4.42,Critic Loss: 0.43, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1390\n",
            "Tr: 10.00,Beam: 17, Iteration: 696, Q value: -0.8549, Reward: 1.0000, BF Gain pred: 0.37, BF Gain: -6.50, BFtrue Gain pred: -4.82, BFtrue Gain: -6.77,Critic Loss: 0.41, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1392\n",
            "Tr: 10.00,Beam: 17, Iteration: 697, Q value: 0.2077, Reward: 1.0000, BF Gain pred: 0.66, BF Gain: -18.87, BFtrue Gain pred: -9.65, BFtrue Gain: -14.88,Critic Loss: 0.40, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1394\n",
            "Tr: 10.00,Beam: 17, Iteration: 698, Q value: 0.1212, Reward: -1.0000, BF Gain pred: -0.79, BF Gain: -6.89, BFtrue Gain pred: -1.66, BFtrue Gain: -6.98,Critic Loss: 0.37, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1396\n",
            "Tr: 10.00,Beam: 17, Iteration: 699, Q value: 0.2347, Reward: 1.0000, BF Gain pred: 2.92, BF Gain: -4.90, BFtrue Gain pred: -1.24, BFtrue Gain: -4.70,Critic Loss: 0.43, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1398\n",
            "Tr: 10.00,Beam: 17, Iteration: 700, Q value: 0.1347, Reward: -1.0000, BF Gain pred: -1.87, BF Gain: -10.74, BFtrue Gain pred: -4.36, BFtrue Gain: -11.35,Critic Loss: 0.42, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1400\n",
            "Tr: 10.00,Beam: 17, Iteration: 701, Q value: 0.0546, Reward: 1.0000, BF Gain pred: 0.66, BF Gain: -10.30, BFtrue Gain pred: -5.15, BFtrue Gain: -11.13,Critic Loss: 0.40, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1402\n",
            "Tr: 10.00,Beam: 17, Iteration: 702, Q value: 0.0418, Reward: -1.0000, BF Gain pred: -3.52, BF Gain: -6.01, BFtrue Gain pred: -5.34, BFtrue Gain: -6.03,Critic Loss: 0.39, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1404\n",
            "Tr: 10.00,Beam: 17, Iteration: 703, Q value: -0.0603, Reward: 1.0000, BF Gain pred: -1.91, BF Gain: -7.51, BFtrue Gain pred: -5.15, BFtrue Gain: -8.08,Critic Loss: 0.40, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1406\n",
            "Tr: 10.00,Beam: 17, Iteration: 704, Q value: -0.4731, Reward: -1.0000, BF Gain pred: -2.00, BF Gain: -4.61, BFtrue Gain pred: -5.18, BFtrue Gain: -5.23,Critic Loss: 0.38, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1408\n",
            "Tr: 10.00,Beam: 17, Iteration: 705, Q value: -0.4048, Reward: 1.0000, BF Gain pred: -1.67, BF Gain: -5.56, BFtrue Gain pred: -4.02, BFtrue Gain: -5.97,Critic Loss: 0.45, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1410\n",
            "Tr: 10.00,Beam: 17, Iteration: 706, Q value: -0.4773, Reward: 1.0000, BF Gain pred: 0.63, BF Gain: -4.19, BFtrue Gain pred: -2.19, BFtrue Gain: -4.65,Critic Loss: 0.42, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1412\n",
            "Tr: 10.00,Beam: 17, Iteration: 707, Q value: -0.3342, Reward: 1.0000, BF Gain pred: 1.93, BF Gain: 0.79, BFtrue Gain pred: -1.93, BFtrue Gain: 0.28,Critic Loss: 0.38, Policy Loss: -0.24\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1414\n",
            "Tr: 10.00,Beam: 17, Iteration: 708, Q value: -0.2297, Reward: -1.0000, BF Gain pred: 0.95, BF Gain: -3.27, BFtrue Gain pred: -4.52, BFtrue Gain: -4.73,Critic Loss: 0.41, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1416\n",
            "Tr: 10.00,Beam: 17, Iteration: 709, Q value: 0.1636, Reward: -1.0000, BF Gain pred: 0.95, BF Gain: -1.03, BFtrue Gain pred: -2.01, BFtrue Gain: -2.09,Critic Loss: 0.43, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1418\n",
            "Tr: 10.00,Beam: 17, Iteration: 710, Q value: 0.2189, Reward: 1.0000, BF Gain pred: 1.70, BF Gain: -5.81, BFtrue Gain pred: -7.99, BFtrue Gain: -5.86,Critic Loss: 0.42, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1420\n",
            "Tr: 10.00,Beam: 17, Iteration: 711, Q value: 0.2214, Reward: -1.0000, BF Gain pred: 1.70, BF Gain: -7.87, BFtrue Gain pred: -10.43, BFtrue Gain: -7.28,Critic Loss: 0.40, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1422\n",
            "Tr: 10.00,Beam: 17, Iteration: 712, Q value: 0.1734, Reward: -1.0000, BF Gain pred: 1.70, BF Gain: -13.19, BFtrue Gain pred: -11.89, BFtrue Gain: -13.39,Critic Loss: 0.42, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1424\n",
            "Tr: 10.00,Beam: 17, Iteration: 713, Q value: -0.0319, Reward: -1.0000, BF Gain pred: 0.30, BF Gain: -6.91, BFtrue Gain pred: -13.09, BFtrue Gain: -8.16,Critic Loss: 0.42, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1426\n",
            "Tr: 10.00,Beam: 17, Iteration: 714, Q value: -0.0323, Reward: -1.0000, BF Gain pred: -0.26, BF Gain: -5.34, BFtrue Gain pred: -12.79, BFtrue Gain: -5.52,Critic Loss: 0.43, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1428\n",
            "Tr: 10.00,Beam: 17, Iteration: 715, Q value: 0.1783, Reward: -1.0000, BF Gain pred: -3.16, BF Gain: -7.08, BFtrue Gain pred: -1.66, BFtrue Gain: -7.74,Critic Loss: 0.43, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1430\n",
            "Tr: 10.00,Beam: 17, Iteration: 716, Q value: 0.2065, Reward: -1.0000, BF Gain pred: -3.16, BF Gain: -7.60, BFtrue Gain pred: -1.96, BFtrue Gain: -7.98,Critic Loss: 0.42, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1432\n",
            "Tr: 10.00,Beam: 17, Iteration: 717, Q value: 0.1170, Reward: -1.0000, BF Gain pred: -6.41, BF Gain: -10.08, BFtrue Gain pred: -2.02, BFtrue Gain: -10.21,Critic Loss: 0.39, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1434\n",
            "Tr: 10.00,Beam: 17, Iteration: 718, Q value: 0.2511, Reward: -1.0000, BF Gain pred: -6.41, BF Gain: -10.14, BFtrue Gain pred: -2.47, BFtrue Gain: -11.05,Critic Loss: 0.38, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1436\n",
            "Tr: 10.00,Beam: 17, Iteration: 719, Q value: 0.2127, Reward: -1.0000, BF Gain pred: -6.41, BF Gain: -10.10, BFtrue Gain pred: -4.04, BFtrue Gain: -11.89,Critic Loss: 0.40, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1438\n",
            "Tr: 10.00,Beam: 17, Iteration: 720, Q value: 0.0254, Reward: 1.0000, BF Gain pred: -6.03, BF Gain: -7.94, BFtrue Gain pred: -8.29, BFtrue Gain: -6.64,Critic Loss: 0.40, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1440\n",
            "Tr: 10.00,Beam: 17, Iteration: 721, Q value: 0.2508, Reward: 1.0000, BF Gain pred: -3.04, BF Gain: -8.84, BFtrue Gain pred: -5.76, BFtrue Gain: -8.73,Critic Loss: 0.42, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1442\n",
            "Tr: 10.00,Beam: 17, Iteration: 722, Q value: 0.2240, Reward: -1.0000, BF Gain pred: -8.03, BF Gain: -4.50, BFtrue Gain pred: -12.78, BFtrue Gain: -3.85,Critic Loss: 0.41, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1444\n",
            "Tr: 10.00,Beam: 17, Iteration: 723, Q value: 0.0669, Reward: -1.0000, BF Gain pred: -10.05, BF Gain: -10.69, BFtrue Gain pred: -4.55, BFtrue Gain: -10.36,Critic Loss: 0.37, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1446\n",
            "Tr: 10.00,Beam: 17, Iteration: 724, Q value: -0.0188, Reward: 1.0000, BF Gain pred: -8.79, BF Gain: -11.78, BFtrue Gain pred: -6.69, BFtrue Gain: -10.66,Critic Loss: 0.42, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1448\n",
            "Tr: 10.00,Beam: 17, Iteration: 725, Q value: -0.0994, Reward: 1.0000, BF Gain pred: -4.20, BF Gain: -13.71, BFtrue Gain pred: -5.55, BFtrue Gain: -13.05,Critic Loss: 0.43, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1450\n",
            "Tr: 10.00,Beam: 17, Iteration: 726, Q value: -0.1020, Reward: -1.0000, BF Gain pred: -5.12, BF Gain: -0.25, BFtrue Gain pred: -19.19, BFtrue Gain: -0.08,Critic Loss: 0.37, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1452\n",
            "Tr: 10.00,Beam: 17, Iteration: 727, Q value: 0.0980, Reward: 1.0000, BF Gain pred: 3.18, BF Gain: 5.31, BFtrue Gain pred: -3.09, BFtrue Gain: 3.74,Critic Loss: 0.36, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1454\n",
            "Tr: 10.00,Beam: 17, Iteration: 728, Q value: 0.2383, Reward: -1.0000, BF Gain pred: -10.21, BF Gain: -7.48, BFtrue Gain pred: -4.83, BFtrue Gain: -5.23,Critic Loss: 0.38, Policy Loss: -0.25\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1456\n",
            "Tr: 10.00,Beam: 17, Iteration: 729, Q value: -0.8627, Reward: 1.0000, BF Gain pred: -10.02, BF Gain: -1.82, BFtrue Gain pred: -6.63, BFtrue Gain: -0.60,Critic Loss: 0.39, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1458\n",
            "Tr: 10.00,Beam: 17, Iteration: 730, Q value: 0.2577, Reward: 1.0000, BF Gain pred: -0.98, BF Gain: -3.80, BFtrue Gain pred: -2.65, BFtrue Gain: -2.79,Critic Loss: 0.42, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1460\n",
            "Tr: 10.00,Beam: 17, Iteration: 731, Q value: 0.2583, Reward: -1.0000, BF Gain pred: -10.35, BF Gain: -11.38, BFtrue Gain pred: -5.56, BFtrue Gain: -10.06,Critic Loss: 0.36, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1462\n",
            "Tr: 10.00,Beam: 17, Iteration: 732, Q value: -0.2323, Reward: 1.0000, BF Gain pred: -5.80, BF Gain: 2.03, BFtrue Gain pred: -7.41, BFtrue Gain: 2.31,Critic Loss: 0.38, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1464\n",
            "Tr: 10.00,Beam: 17, Iteration: 733, Q value: 0.2595, Reward: -1.0000, BF Gain pred: -6.08, BF Gain: 4.93, BFtrue Gain pred: -9.26, BFtrue Gain: 3.64,Critic Loss: 0.39, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1466\n",
            "Tr: 10.00,Beam: 17, Iteration: 734, Q value: 0.2601, Reward: 1.0000, BF Gain pred: -2.45, BF Gain: -14.73, BFtrue Gain pred: -2.48, BFtrue Gain: -15.69,Critic Loss: 0.38, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1468\n",
            "Tr: 10.00,Beam: 17, Iteration: 735, Q value: 0.2606, Reward: 1.0000, BF Gain pred: -2.31, BF Gain: -0.40, BFtrue Gain pred: -3.75, BFtrue Gain: -1.04,Critic Loss: 0.38, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1470\n",
            "Tr: 10.00,Beam: 17, Iteration: 736, Q value: 0.2612, Reward: -1.0000, BF Gain pred: -3.98, BF Gain: -6.35, BFtrue Gain pred: 0.08, BFtrue Gain: -7.23,Critic Loss: 0.35, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1472\n",
            "Tr: 10.00,Beam: 17, Iteration: 737, Q value: -0.6845, Reward: -1.0000, BF Gain pred: -4.87, BF Gain: -0.33, BFtrue Gain pred: -7.66, BFtrue Gain: -0.98,Critic Loss: 0.40, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1474\n",
            "Tr: 10.00,Beam: 17, Iteration: 738, Q value: 0.2624, Reward: 1.0000, BF Gain pred: -3.64, BF Gain: -9.19, BFtrue Gain pred: -4.65, BFtrue Gain: -8.88,Critic Loss: 0.43, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1476\n",
            "Tr: 10.00,Beam: 17, Iteration: 739, Q value: 0.2629, Reward: 1.0000, BF Gain pred: -3.47, BF Gain: -6.73, BFtrue Gain pred: -4.00, BFtrue Gain: -6.71,Critic Loss: 0.41, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1478\n",
            "Tr: 10.00,Beam: 17, Iteration: 740, Q value: 0.2635, Reward: 1.0000, BF Gain pred: -2.52, BF Gain: -4.08, BFtrue Gain pred: -4.12, BFtrue Gain: -4.75,Critic Loss: 0.38, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1480\n",
            "Tr: 10.00,Beam: 17, Iteration: 741, Q value: 0.2641, Reward: 1.0000, BF Gain pred: -1.12, BF Gain: -7.43, BFtrue Gain pred: -6.48, BFtrue Gain: -7.21,Critic Loss: 0.42, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1482\n",
            "Tr: 10.00,Beam: 17, Iteration: 742, Q value: 0.2580, Reward: -1.0000, BF Gain pred: -3.22, BF Gain: -0.88, BFtrue Gain pred: -4.04, BFtrue Gain: -2.79,Critic Loss: 0.39, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1484\n",
            "Tr: 10.00,Beam: 17, Iteration: 743, Q value: 0.2653, Reward: -1.0000, BF Gain pred: -6.87, BF Gain: -4.74, BFtrue Gain pred: -5.80, BFtrue Gain: -4.94,Critic Loss: 0.38, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1486\n",
            "Tr: 10.00,Beam: 17, Iteration: 744, Q value: 0.2658, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: -4.75, BFtrue Gain pred: -2.50, BFtrue Gain: -5.48,Critic Loss: 0.38, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1488\n",
            "Tr: 10.00,Beam: 17, Iteration: 745, Q value: 0.2664, Reward: -1.0000, BF Gain pred: -6.87, BF Gain: -4.05, BFtrue Gain pred: -5.39, BFtrue Gain: -3.58,Critic Loss: 0.41, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1490\n",
            "Tr: 10.00,Beam: 17, Iteration: 746, Q value: 0.2670, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: 0.93, BFtrue Gain pred: -2.73, BFtrue Gain: 1.70,Critic Loss: 0.40, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1492\n",
            "Tr: 10.00,Beam: 17, Iteration: 747, Q value: 0.2675, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -6.30, BFtrue Gain pred: -4.93, BFtrue Gain: -6.78,Critic Loss: 0.41, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1494\n",
            "Tr: 10.00,Beam: 17, Iteration: 748, Q value: 0.2681, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: -0.72, BFtrue Gain pred: -4.29, BFtrue Gain: -0.60,Critic Loss: 0.40, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1496\n",
            "Tr: 10.00,Beam: 17, Iteration: 749, Q value: 0.2686, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: -8.92, BFtrue Gain pred: -4.29, BFtrue Gain: -8.89,Critic Loss: 0.39, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1498\n",
            "Tr: 10.00,Beam: 17, Iteration: 750, Q value: 0.2692, Reward: 1.0000, BF Gain pred: -6.69, BF Gain: -9.17, BFtrue Gain pred: -4.99, BFtrue Gain: -10.40,Critic Loss: 0.36, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1500\n",
            "Tr: 10.00,Beam: 17, Iteration: 751, Q value: 0.2697, Reward: -1.0000, BF Gain pred: -6.94, BF Gain: 0.35, BFtrue Gain pred: -3.61, BFtrue Gain: -0.12,Critic Loss: 0.38, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1502\n",
            "Tr: 10.00,Beam: 17, Iteration: 752, Q value: 0.2703, Reward: -1.0000, BF Gain pred: -7.09, BF Gain: -7.59, BFtrue Gain pred: -5.25, BFtrue Gain: -8.09,Critic Loss: 0.39, Policy Loss: -0.26\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1504\n",
            "Tr: 10.00,Beam: 17, Iteration: 753, Q value: 0.2708, Reward: 1.0000, BF Gain pred: -6.94, BF Gain: -4.06, BFtrue Gain pred: -4.59, BFtrue Gain: -3.89,Critic Loss: 0.41, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1506\n",
            "Tr: 10.00,Beam: 17, Iteration: 754, Q value: 0.2714, Reward: -1.0000, BF Gain pred: -7.11, BF Gain: -6.84, BFtrue Gain pred: -4.90, BFtrue Gain: -7.08,Critic Loss: 0.40, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1508\n",
            "Tr: 10.00,Beam: 17, Iteration: 755, Q value: 0.2719, Reward: -1.0000, BF Gain pred: -8.87, BF Gain: -9.81, BFtrue Gain pred: -4.32, BFtrue Gain: -9.27,Critic Loss: 0.36, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1510\n",
            "Tr: 10.00,Beam: 17, Iteration: 756, Q value: 0.2725, Reward: 1.0000, BF Gain pred: -4.58, BF Gain: -4.12, BFtrue Gain pred: -3.92, BFtrue Gain: -5.15,Critic Loss: 0.41, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1512\n",
            "Tr: 10.00,Beam: 17, Iteration: 757, Q value: 0.2730, Reward: -1.0000, BF Gain pred: -10.79, BF Gain: -7.67, BFtrue Gain pred: -5.90, BFtrue Gain: -7.77,Critic Loss: 0.43, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1514\n",
            "Tr: 10.00,Beam: 17, Iteration: 758, Q value: 0.2735, Reward: 1.0000, BF Gain pred: -3.72, BF Gain: -10.99, BFtrue Gain pred: -6.34, BFtrue Gain: -11.17,Critic Loss: 0.42, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1516\n",
            "Tr: 10.00,Beam: 17, Iteration: 759, Q value: 0.2740, Reward: -1.0000, BF Gain pred: -14.07, BF Gain: -14.95, BFtrue Gain pred: -12.42, BFtrue Gain: -13.96,Critic Loss: 0.37, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1518\n",
            "Tr: 10.00,Beam: 17, Iteration: 760, Q value: 0.2746, Reward: 1.0000, BF Gain pred: -4.67, BF Gain: -6.53, BFtrue Gain pred: -6.30, BFtrue Gain: -6.62,Critic Loss: 0.39, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1520\n",
            "Tr: 10.00,Beam: 17, Iteration: 761, Q value: 0.2751, Reward: -1.0000, BF Gain pred: -9.61, BF Gain: -5.64, BFtrue Gain pred: -15.85, BFtrue Gain: -5.17,Critic Loss: 0.39, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1522\n",
            "Tr: 10.00,Beam: 17, Iteration: 762, Q value: 0.2756, Reward: 1.0000, BF Gain pred: -2.45, BF Gain: -8.84, BFtrue Gain pred: -6.03, BFtrue Gain: -9.56,Critic Loss: 0.37, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1524\n",
            "Tr: 10.00,Beam: 17, Iteration: 763, Q value: 0.2761, Reward: -1.0000, BF Gain pred: -5.03, BF Gain: -4.40, BFtrue Gain pred: -23.28, BFtrue Gain: -4.35,Critic Loss: 0.40, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1526\n",
            "Tr: 10.00,Beam: 17, Iteration: 764, Q value: 0.2766, Reward: 1.0000, BF Gain pred: -4.65, BF Gain: -9.68, BFtrue Gain pred: -6.19, BFtrue Gain: -10.54,Critic Loss: 0.43, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1528\n",
            "Tr: 10.00,Beam: 17, Iteration: 765, Q value: 0.2772, Reward: -1.0000, BF Gain pred: -5.04, BF Gain: -2.73, BFtrue Gain pred: -7.63, BFtrue Gain: -3.05,Critic Loss: 0.37, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1530\n",
            "Tr: 10.00,Beam: 17, Iteration: 766, Q value: 0.2777, Reward: 1.0000, BF Gain pred: -3.49, BF Gain: -5.10, BFtrue Gain pred: -5.87, BFtrue Gain: -6.08,Critic Loss: 0.38, Policy Loss: -0.27\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1532\n",
            "Tr: 10.00,Beam: 17, Iteration: 767, Q value: 0.2783, Reward: -1.0000, BF Gain pred: -4.57, BF Gain: -0.07, BFtrue Gain pred: -4.33, BFtrue Gain: -0.07,Critic Loss: 0.40, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1534\n",
            "Tr: 10.00,Beam: 17, Iteration: 768, Q value: 0.2744, Reward: 1.0000, BF Gain pred: -1.55, BF Gain: -3.93, BFtrue Gain pred: -2.95, BFtrue Gain: -4.88,Critic Loss: 0.39, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1536\n",
            "Tr: 10.00,Beam: 17, Iteration: 769, Q value: 0.2794, Reward: 1.0000, BF Gain pred: 0.58, BF Gain: -6.49, BFtrue Gain pred: -3.15, BFtrue Gain: -6.03,Critic Loss: 0.38, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1538\n",
            "Tr: 10.00,Beam: 17, Iteration: 770, Q value: 0.2799, Reward: -1.0000, BF Gain pred: -0.95, BF Gain: -16.73, BFtrue Gain pred: 2.24, BFtrue Gain: -15.00,Critic Loss: 0.36, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1540\n",
            "Tr: 10.00,Beam: 17, Iteration: 771, Q value: 0.2596, Reward: -1.0000, BF Gain pred: -4.59, BF Gain: -6.54, BFtrue Gain pred: -3.03, BFtrue Gain: -7.39,Critic Loss: 0.40, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1542\n",
            "Tr: 10.00,Beam: 17, Iteration: 772, Q value: 0.2809, Reward: -1.0000, BF Gain pred: -8.21, BF Gain: 9.07, BFtrue Gain pred: -4.31, BFtrue Gain: 9.12,Critic Loss: 0.39, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1544\n",
            "Tr: 10.00,Beam: 17, Iteration: 773, Q value: 0.2814, Reward: 1.0000, BF Gain pred: -4.20, BF Gain: -1.42, BFtrue Gain pred: -6.36, BFtrue Gain: -1.83,Critic Loss: 0.38, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1546\n",
            "Tr: 10.00,Beam: 17, Iteration: 774, Q value: 0.2819, Reward: 1.0000, BF Gain pred: -4.06, BF Gain: -14.25, BFtrue Gain pred: -6.26, BFtrue Gain: -14.09,Critic Loss: 0.38, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1548\n",
            "Tr: 10.00,Beam: 17, Iteration: 775, Q value: 0.2676, Reward: -1.0000, BF Gain pred: -5.41, BF Gain: -16.50, BFtrue Gain pred: -5.27, BFtrue Gain: -18.45,Critic Loss: 0.37, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1550\n",
            "Tr: 10.00,Beam: 17, Iteration: 776, Q value: 0.2669, Reward: 1.0000, BF Gain pred: -4.46, BF Gain: -6.42, BFtrue Gain pred: -6.26, BFtrue Gain: -6.66,Critic Loss: 0.42, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1552\n",
            "Tr: 10.00,Beam: 17, Iteration: 777, Q value: 0.2834, Reward: 1.0000, BF Gain pred: -4.05, BF Gain: -21.11, BFtrue Gain pred: -14.41, BFtrue Gain: -21.55,Critic Loss: 0.37, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1554\n",
            "Tr: 10.00,Beam: 17, Iteration: 778, Q value: 0.2840, Reward: 1.0000, BF Gain pred: -2.28, BF Gain: -9.29, BFtrue Gain pred: -5.41, BFtrue Gain: -8.74,Critic Loss: 0.42, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1556\n",
            "Tr: 10.00,Beam: 17, Iteration: 779, Q value: 0.2845, Reward: -1.0000, BF Gain pred: -9.22, BF Gain: -13.07, BFtrue Gain pred: -7.87, BFtrue Gain: -13.65,Critic Loss: 0.37, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1558\n",
            "Tr: 10.00,Beam: 17, Iteration: 780, Q value: 0.1542, Reward: -1.0000, BF Gain pred: -16.63, BF Gain: -9.19, BFtrue Gain pred: -26.26, BFtrue Gain: -9.49,Critic Loss: 0.39, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1560\n",
            "Tr: 10.00,Beam: 17, Iteration: 781, Q value: 0.2857, Reward: 1.0000, BF Gain pred: -2.57, BF Gain: -1.37, BFtrue Gain pred: -4.59, BFtrue Gain: -1.78,Critic Loss: 0.38, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1562\n",
            "Tr: 10.00,Beam: 17, Iteration: 782, Q value: 0.2862, Reward: -1.0000, BF Gain pred: -2.93, BF Gain: -12.17, BFtrue Gain pred: 4.74, BFtrue Gain: -10.93,Critic Loss: 0.40, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1564\n",
            "Tr: 10.00,Beam: 17, Iteration: 783, Q value: 0.2524, Reward: -1.0000, BF Gain pred: -12.22, BF Gain: -4.14, BFtrue Gain pred: -12.21, BFtrue Gain: -4.48,Critic Loss: 0.38, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1566\n",
            "Tr: 10.00,Beam: 17, Iteration: 784, Q value: 0.2873, Reward: 1.0000, BF Gain pred: -0.74, BF Gain: -5.52, BFtrue Gain pred: -2.73, BFtrue Gain: -5.28,Critic Loss: 0.40, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1568\n",
            "Tr: 10.00,Beam: 17, Iteration: 785, Q value: 0.2879, Reward: -1.0000, BF Gain pred: -14.92, BF Gain: -10.82, BFtrue Gain pred: -10.23, BFtrue Gain: -9.88,Critic Loss: 0.40, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1570\n",
            "Tr: 10.00,Beam: 17, Iteration: 786, Q value: 0.2884, Reward: 1.0000, BF Gain pred: -3.78, BF Gain: -7.12, BFtrue Gain pred: -7.59, BFtrue Gain: -7.98,Critic Loss: 0.38, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1572\n",
            "Tr: 10.00,Beam: 17, Iteration: 787, Q value: 0.2890, Reward: -1.0000, BF Gain pred: -8.81, BF Gain: -19.27, BFtrue Gain pred: -7.52, BFtrue Gain: -18.63,Critic Loss: 0.41, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1574\n",
            "Tr: 10.00,Beam: 17, Iteration: 788, Q value: 0.2896, Reward: -1.0000, BF Gain pred: -16.49, BF Gain: -11.90, BFtrue Gain pred: -25.36, BFtrue Gain: -12.26,Critic Loss: 0.39, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1576\n",
            "Tr: 10.00,Beam: 17, Iteration: 789, Q value: 0.2704, Reward: 1.0000, BF Gain pred: -5.29, BF Gain: -6.78, BFtrue Gain pred: -2.10, BFtrue Gain: -7.40,Critic Loss: 0.40, Policy Loss: -0.28\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1578\n",
            "Tr: 10.00,Beam: 17, Iteration: 790, Q value: 0.2871, Reward: -1.0000, BF Gain pred: -15.03, BF Gain: -8.93, BFtrue Gain pred: -10.81, BFtrue Gain: -8.88,Critic Loss: 0.38, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1580\n",
            "Tr: 10.00,Beam: 17, Iteration: 791, Q value: 0.2913, Reward: 1.0000, BF Gain pred: -5.77, BF Gain: -10.27, BFtrue Gain pred: -7.40, BFtrue Gain: -10.97,Critic Loss: 0.35, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1582\n",
            "Tr: 10.00,Beam: 17, Iteration: 792, Q value: -0.3877, Reward: 1.0000, BF Gain pred: -3.97, BF Gain: -15.11, BFtrue Gain pred: -6.04, BFtrue Gain: -14.73,Critic Loss: 0.36, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1584\n",
            "Tr: 10.00,Beam: 17, Iteration: 793, Q value: 0.2925, Reward: -1.0000, BF Gain pred: -9.55, BF Gain: -7.08, BFtrue Gain pred: -1.00, BFtrue Gain: -6.47,Critic Loss: 0.37, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1586\n",
            "Tr: 10.00,Beam: 17, Iteration: 794, Q value: 0.2930, Reward: 1.0000, BF Gain pred: -3.64, BF Gain: -5.15, BFtrue Gain pred: -5.68, BFtrue Gain: -5.03,Critic Loss: 0.42, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1588\n",
            "Tr: 10.00,Beam: 17, Iteration: 795, Q value: -1.3957, Reward: -1.0000, BF Gain pred: -20.59, BF Gain: -3.82, BFtrue Gain pred: -8.48, BFtrue Gain: -3.45,Critic Loss: 0.36, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1590\n",
            "Tr: 10.00,Beam: 17, Iteration: 796, Q value: 0.2942, Reward: 1.0000, BF Gain pred: -6.37, BF Gain: -6.26, BFtrue Gain pred: -1.66, BFtrue Gain: -5.62,Critic Loss: 0.36, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1592\n",
            "Tr: 10.00,Beam: 17, Iteration: 797, Q value: 0.0986, Reward: -1.0000, BF Gain pred: -8.75, BF Gain: 0.12, BFtrue Gain pred: -4.26, BFtrue Gain: 0.94,Critic Loss: 0.35, Policy Loss: -0.29\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1594\n",
            "Tr: 10.00,Beam: 17, Iteration: 798, Q value: -0.6501, Reward: -1.0000, BF Gain pred: -11.00, BF Gain: -12.97, BFtrue Gain pred: -9.04, BFtrue Gain: -12.65,Critic Loss: 0.37, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1596\n",
            "Tr: 10.00,Beam: 17, Iteration: 799, Q value: 0.2959, Reward: 1.0000, BF Gain pred: -3.91, BF Gain: 1.26, BFtrue Gain pred: -0.83, BFtrue Gain: 1.80,Critic Loss: 0.36, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1598\n",
            "Tr: 10.00,Beam: 17, Iteration: 800, Q value: 0.1536, Reward: 1.0000, BF Gain pred: 2.60, BF Gain: -24.15, BFtrue Gain pred: -5.38, BFtrue Gain: -28.58,Critic Loss: 0.39, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1600\n",
            "Tr: 10.00,Beam: 17, Iteration: 801, Q value: 0.2965, Reward: -1.0000, BF Gain pred: -4.13, BF Gain: -14.57, BFtrue Gain pred: 3.31, BFtrue Gain: -12.89,Critic Loss: 0.36, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1602\n",
            "Tr: 10.00,Beam: 17, Iteration: 802, Q value: 0.2976, Reward: 1.0000, BF Gain pred: -3.58, BF Gain: -26.89, BFtrue Gain pred: -3.89, BFtrue Gain: -27.54,Critic Loss: 0.38, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1604\n",
            "Tr: 10.00,Beam: 17, Iteration: 803, Q value: 0.0973, Reward: -1.0000, BF Gain pred: -7.86, BF Gain: -5.07, BFtrue Gain pred: -9.94, BFtrue Gain: -5.34,Critic Loss: 0.40, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1606\n",
            "Tr: 10.00,Beam: 17, Iteration: 804, Q value: 0.0107, Reward: 1.0000, BF Gain pred: -3.62, BF Gain: -5.97, BFtrue Gain pred: 0.82, BFtrue Gain: -6.44,Critic Loss: 0.36, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1608\n",
            "Tr: 10.00,Beam: 17, Iteration: 805, Q value: 0.2993, Reward: -1.0000, BF Gain pred: -5.30, BF Gain: -5.12, BFtrue Gain pred: -2.58, BFtrue Gain: -4.44,Critic Loss: 0.38, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1610\n",
            "Tr: 10.00,Beam: 17, Iteration: 806, Q value: -0.5761, Reward: 1.0000, BF Gain pred: -3.17, BF Gain: -8.83, BFtrue Gain pred: -9.99, BFtrue Gain: -9.91,Critic Loss: 0.40, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1612\n",
            "Tr: 10.00,Beam: 17, Iteration: 807, Q value: 0.1705, Reward: -1.0000, BF Gain pred: -5.57, BF Gain: -8.73, BFtrue Gain pred: 3.03, BFtrue Gain: -8.47,Critic Loss: 0.36, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1614\n",
            "Tr: 10.00,Beam: 17, Iteration: 808, Q value: 0.3010, Reward: -1.0000, BF Gain pred: -7.09, BF Gain: -12.70, BFtrue Gain pred: -6.73, BFtrue Gain: -13.60,Critic Loss: 0.35, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1616\n",
            "Tr: 10.00,Beam: 17, Iteration: 809, Q value: -0.4697, Reward: 1.0000, BF Gain pred: -4.87, BF Gain: -1.42, BFtrue Gain pred: -4.76, BFtrue Gain: -1.19,Critic Loss: 0.39, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1618\n",
            "Tr: 10.00,Beam: 17, Iteration: 810, Q value: -0.0046, Reward: -1.0000, BF Gain pred: -8.20, BF Gain: -7.00, BFtrue Gain pred: -4.36, BFtrue Gain: -6.82,Critic Loss: 0.34, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1620\n",
            "Tr: 10.00,Beam: 17, Iteration: 811, Q value: 0.3026, Reward: 1.0000, BF Gain pred: -2.56, BF Gain: -5.20, BFtrue Gain pred: -2.67, BFtrue Gain: -5.35,Critic Loss: 0.36, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1622\n",
            "Tr: 10.00,Beam: 17, Iteration: 812, Q value: 0.2092, Reward: 1.0000, BF Gain pred: -0.13, BF Gain: -5.46, BFtrue Gain pred: -1.41, BFtrue Gain: -5.09,Critic Loss: 0.36, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1624\n",
            "Tr: 10.00,Beam: 17, Iteration: 813, Q value: 0.3036, Reward: -1.0000, BF Gain pred: -0.83, BF Gain: -6.20, BFtrue Gain pred: -1.55, BFtrue Gain: -6.59,Critic Loss: 0.40, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1626\n",
            "Tr: 10.00,Beam: 17, Iteration: 814, Q value: 0.2739, Reward: 1.0000, BF Gain pred: -0.33, BF Gain: -7.08, BFtrue Gain pred: -1.58, BFtrue Gain: -7.83,Critic Loss: 0.38, Policy Loss: -0.30\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1628\n",
            "Tr: 10.00,Beam: 17, Iteration: 815, Q value: 0.3048, Reward: -1.0000, BF Gain pred: -3.61, BF Gain: -19.44, BFtrue Gain pred: -3.44, BFtrue Gain: -21.58,Critic Loss: 0.38, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1630\n",
            "Tr: 10.00,Beam: 17, Iteration: 816, Q value: 0.0293, Reward: 1.0000, BF Gain pred: -2.14, BF Gain: -10.48, BFtrue Gain pred: 0.03, BFtrue Gain: -12.10,Critic Loss: 0.35, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1632\n",
            "Tr: 10.00,Beam: 17, Iteration: 817, Q value: 0.1065, Reward: -1.0000, BF Gain pred: -2.70, BF Gain: -7.64, BFtrue Gain pred: -1.94, BFtrue Gain: -8.65,Critic Loss: 0.38, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1634\n",
            "Tr: 10.00,Beam: 17, Iteration: 818, Q value: 0.2988, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -21.51, BFtrue Gain pred: -1.25, BFtrue Gain: -22.39,Critic Loss: 0.35, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1636\n",
            "Tr: 10.00,Beam: 17, Iteration: 819, Q value: -0.3968, Reward: 1.0000, BF Gain pred: -5.29, BF Gain: -7.03, BFtrue Gain pred: -4.49, BFtrue Gain: -6.99,Critic Loss: 0.34, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1638\n",
            "Tr: 10.00,Beam: 17, Iteration: 820, Q value: 0.2787, Reward: 1.0000, BF Gain pred: -4.15, BF Gain: -17.34, BFtrue Gain pred: -1.87, BFtrue Gain: -15.44,Critic Loss: 0.35, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1640\n",
            "Tr: 10.00,Beam: 17, Iteration: 821, Q value: -0.5260, Reward: -1.0000, BF Gain pred: -4.94, BF Gain: -13.51, BFtrue Gain pred: -0.48, BFtrue Gain: -13.62,Critic Loss: 0.32, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1642\n",
            "Tr: 10.00,Beam: 17, Iteration: 822, Q value: 0.2367, Reward: 1.0000, BF Gain pred: -4.35, BF Gain: -11.59, BFtrue Gain pred: -2.83, BFtrue Gain: -11.09,Critic Loss: 0.33, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1644\n",
            "Tr: 10.00,Beam: 17, Iteration: 823, Q value: -0.6456, Reward: -1.0000, BF Gain pred: -4.51, BF Gain: -9.44, BFtrue Gain pred: 2.74, BFtrue Gain: -9.95,Critic Loss: 0.31, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1646\n",
            "Tr: 10.00,Beam: 17, Iteration: 824, Q value: 0.2980, Reward: -1.0000, BF Gain pred: -7.91, BF Gain: -5.85, BFtrue Gain pred: -5.68, BFtrue Gain: -7.25,Critic Loss: 0.37, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1648\n",
            "Tr: 10.00,Beam: 17, Iteration: 825, Q value: 0.0619, Reward: -1.0000, BF Gain pred: -9.27, BF Gain: -14.01, BFtrue Gain pred: -12.45, BFtrue Gain: -15.25,Critic Loss: 0.36, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1650\n",
            "Tr: 10.00,Beam: 17, Iteration: 826, Q value: 0.2299, Reward: -1.0000, BF Gain pred: -9.85, BF Gain: -5.67, BFtrue Gain pred: -13.63, BFtrue Gain: -6.85,Critic Loss: 0.34, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1652\n",
            "Tr: 10.00,Beam: 17, Iteration: 827, Q value: -0.1455, Reward: 1.0000, BF Gain pred: -8.66, BF Gain: -1.91, BFtrue Gain pred: -14.68, BFtrue Gain: -3.66,Critic Loss: 0.32, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1654\n",
            "Tr: 10.00,Beam: 17, Iteration: 828, Q value: -0.1734, Reward: -1.0000, BF Gain pred: -9.99, BF Gain: -1.15, BFtrue Gain pred: -18.14, BFtrue Gain: -2.04,Critic Loss: 0.41, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1656\n",
            "Tr: 10.00,Beam: 17, Iteration: 829, Q value: -0.1116, Reward: 1.0000, BF Gain pred: -1.23, BF Gain: 0.76, BFtrue Gain pred: -10.06, BFtrue Gain: -0.88,Critic Loss: 0.34, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1658\n",
            "Tr: 10.00,Beam: 17, Iteration: 830, Q value: 0.0533, Reward: -1.0000, BF Gain pred: -6.67, BF Gain: -9.61, BFtrue Gain pred: -14.42, BFtrue Gain: -10.02,Critic Loss: 0.37, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1660\n",
            "Tr: 10.00,Beam: 17, Iteration: 831, Q value: 0.1966, Reward: -1.0000, BF Gain pred: -8.96, BF Gain: -1.74, BFtrue Gain pred: -14.35, BFtrue Gain: -1.96,Critic Loss: 0.39, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1662\n",
            "Tr: 10.00,Beam: 17, Iteration: 832, Q value: 0.1527, Reward: -1.0000, BF Gain pred: -11.63, BF Gain: 1.36, BFtrue Gain pred: -42.72, BFtrue Gain: 1.61,Critic Loss: 0.36, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1664\n",
            "Tr: 10.00,Beam: 17, Iteration: 833, Q value: 0.1534, Reward: 1.0000, BF Gain pred: -10.10, BF Gain: -2.21, BFtrue Gain pred: -18.21, BFtrue Gain: -3.56,Critic Loss: 0.37, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1666\n",
            "Tr: 10.00,Beam: 17, Iteration: 834, Q value: 0.1397, Reward: -1.0000, BF Gain pred: -13.83, BF Gain: -12.97, BFtrue Gain pred: -12.87, BFtrue Gain: -13.73,Critic Loss: 0.35, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1668\n",
            "Tr: 10.00,Beam: 17, Iteration: 835, Q value: 0.1855, Reward: -1.0000, BF Gain pred: -13.83, BF Gain: -7.71, BFtrue Gain pred: -11.41, BFtrue Gain: -9.00,Critic Loss: 0.35, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1670\n",
            "Tr: 10.00,Beam: 17, Iteration: 836, Q value: 0.2267, Reward: -1.0000, BF Gain pred: -13.83, BF Gain: -13.07, BFtrue Gain pred: -12.48, BFtrue Gain: -13.32,Critic Loss: 0.38, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1672\n",
            "Tr: 10.00,Beam: 17, Iteration: 837, Q value: 0.2483, Reward: 1.0000, BF Gain pred: -10.30, BF Gain: -6.81, BFtrue Gain pred: -12.03, BFtrue Gain: -7.22,Critic Loss: 0.35, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1674\n",
            "Tr: 10.00,Beam: 17, Iteration: 838, Q value: 0.2796, Reward: -1.0000, BF Gain pred: -10.30, BF Gain: -11.22, BFtrue Gain pred: -10.12, BFtrue Gain: -12.49,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1676\n",
            "Tr: 10.00,Beam: 17, Iteration: 839, Q value: 0.2630, Reward: 1.0000, BF Gain pred: -10.14, BF Gain: -3.67, BFtrue Gain pred: -11.01, BFtrue Gain: -4.08,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1678\n",
            "Tr: 10.00,Beam: 17, Iteration: 840, Q value: 0.2743, Reward: -1.0000, BF Gain pred: -10.50, BF Gain: -6.37, BFtrue Gain pred: -10.03, BFtrue Gain: -7.01,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1680\n",
            "Tr: 10.00,Beam: 17, Iteration: 841, Q value: 0.2807, Reward: 1.0000, BF Gain pred: -7.03, BF Gain: -15.46, BFtrue Gain pred: -8.70, BFtrue Gain: -15.19,Critic Loss: 0.35, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1682\n",
            "Tr: 10.00,Beam: 17, Iteration: 842, Q value: 0.2706, Reward: -1.0000, BF Gain pred: -7.03, BF Gain: -18.32, BFtrue Gain pred: -8.73, BFtrue Gain: -21.51,Critic Loss: 0.32, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1684\n",
            "Tr: 10.00,Beam: 17, Iteration: 843, Q value: 0.2841, Reward: -1.0000, BF Gain pred: -12.17, BF Gain: -9.29, BFtrue Gain pred: -9.42, BFtrue Gain: -8.42,Critic Loss: 0.33, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1686\n",
            "Tr: 10.00,Beam: 17, Iteration: 844, Q value: 0.3214, Reward: -1.0000, BF Gain pred: -12.17, BF Gain: -10.23, BFtrue Gain pred: -10.97, BFtrue Gain: -9.78,Critic Loss: 0.39, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1688\n",
            "Tr: 10.00,Beam: 17, Iteration: 845, Q value: 0.3220, Reward: 1.0000, BF Gain pred: -8.94, BF Gain: -21.20, BFtrue Gain pred: -12.00, BFtrue Gain: -21.95,Critic Loss: 0.35, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1690\n",
            "Tr: 10.00,Beam: 17, Iteration: 846, Q value: 0.2913, Reward: -1.0000, BF Gain pred: -8.94, BF Gain: -10.73, BFtrue Gain pred: -13.37, BFtrue Gain: -10.42,Critic Loss: 0.34, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1692\n",
            "Tr: 10.00,Beam: 17, Iteration: 847, Q value: 0.2400, Reward: 1.0000, BF Gain pred: -6.15, BF Gain: -11.93, BFtrue Gain pred: -14.02, BFtrue Gain: -11.43,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1694\n",
            "Tr: 10.00,Beam: 17, Iteration: 848, Q value: 0.3236, Reward: -1.0000, BF Gain pred: -6.15, BF Gain: -12.34, BFtrue Gain pred: -13.38, BFtrue Gain: -11.25,Critic Loss: 0.37, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1696\n",
            "Tr: 10.00,Beam: 17, Iteration: 849, Q value: 0.3241, Reward: -1.0000, BF Gain pred: -6.15, BF Gain: -30.85, BFtrue Gain pred: -14.16, BFtrue Gain: -27.19,Critic Loss: 0.41, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1698\n",
            "Tr: 10.00,Beam: 17, Iteration: 850, Q value: 0.3247, Reward: -1.0000, BF Gain pred: -10.43, BF Gain: -9.27, BFtrue Gain pred: -14.16, BFtrue Gain: -8.74,Critic Loss: 0.41, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1700\n",
            "Tr: 10.00,Beam: 17, Iteration: 851, Q value: 0.3161, Reward: -1.0000, BF Gain pred: -10.43, BF Gain: -8.31, BFtrue Gain pred: -14.96, BFtrue Gain: -8.30,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1702\n",
            "Tr: 10.00,Beam: 17, Iteration: 852, Q value: 0.2410, Reward: -1.0000, BF Gain pred: -20.99, BF Gain: -18.02, BFtrue Gain pred: -14.06, BFtrue Gain: -16.36,Critic Loss: 0.37, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1704\n",
            "Tr: 10.00,Beam: 17, Iteration: 853, Q value: -0.3212, Reward: 1.0000, BF Gain pred: -11.20, BF Gain: 0.80, BFtrue Gain pred: -9.86, BFtrue Gain: 1.89,Critic Loss: 0.34, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1706\n",
            "Tr: 10.00,Beam: 17, Iteration: 854, Q value: -0.8937, Reward: -1.0000, BF Gain pred: -11.68, BF Gain: -3.78, BFtrue Gain pred: -4.74, BFtrue Gain: -3.28,Critic Loss: 0.37, Policy Loss: -0.31\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1708\n",
            "Tr: 10.00,Beam: 17, Iteration: 855, Q value: -0.9109, Reward: 1.0000, BF Gain pred: -7.14, BF Gain: -20.23, BFtrue Gain pred: -9.93, BFtrue Gain: -21.39,Critic Loss: 0.39, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1710\n",
            "Tr: 10.00,Beam: 17, Iteration: 856, Q value: -0.2493, Reward: -1.0000, BF Gain pred: -7.94, BF Gain: -7.56, BFtrue Gain pred: -14.41, BFtrue Gain: -7.09,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1712\n",
            "Tr: 10.00,Beam: 17, Iteration: 857, Q value: -0.9113, Reward: 1.0000, BF Gain pred: -4.55, BF Gain: 0.27, BFtrue Gain pred: -10.29, BFtrue Gain: -0.10,Critic Loss: 0.36, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1714\n",
            "Tr: 10.00,Beam: 17, Iteration: 858, Q value: -0.6429, Reward: 1.0000, BF Gain pred: -0.17, BF Gain: -1.21, BFtrue Gain pred: -0.06, BFtrue Gain: -1.49,Critic Loss: 0.34, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1716\n",
            "Tr: 10.00,Beam: 17, Iteration: 859, Q value: -0.0397, Reward: 1.0000, BF Gain pred: 0.09, BF Gain: 1.58, BFtrue Gain pred: -0.03, BFtrue Gain: 2.60,Critic Loss: 0.35, Policy Loss: -0.32\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1718\n",
            "Tr: 10.00,Beam: 17, Iteration: 860, Q value: 0.3304, Reward: 1.0000, BF Gain pred: 7.55, BF Gain: -7.72, BFtrue Gain pred: 1.06, BFtrue Gain: -7.71,Critic Loss: 0.36, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1720\n",
            "Tr: 10.00,Beam: 17, Iteration: 861, Q value: 0.3169, Reward: -1.0000, BF Gain pred: 5.72, BF Gain: -7.11, BFtrue Gain pred: -0.24, BFtrue Gain: -7.42,Critic Loss: 0.34, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1722\n",
            "Tr: 10.00,Beam: 17, Iteration: 862, Q value: 0.3315, Reward: 1.0000, BF Gain pred: 8.21, BF Gain: -8.33, BFtrue Gain pred: 0.32, BFtrue Gain: -8.43,Critic Loss: 0.37, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1724\n",
            "Tr: 10.00,Beam: 17, Iteration: 863, Q value: 0.3321, Reward: -1.0000, BF Gain pred: 2.66, BF Gain: -3.18, BFtrue Gain pred: -4.01, BFtrue Gain: -2.77,Critic Loss: 0.36, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1726\n",
            "Tr: 10.00,Beam: 17, Iteration: 864, Q value: 0.3327, Reward: 1.0000, BF Gain pred: 5.91, BF Gain: -10.98, BFtrue Gain pred: -3.02, BFtrue Gain: -12.63,Critic Loss: 0.35, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1728\n",
            "Tr: 10.00,Beam: 17, Iteration: 865, Q value: 0.3332, Reward: -1.0000, BF Gain pred: -0.35, BF Gain: -3.46, BFtrue Gain pred: -3.43, BFtrue Gain: -3.32,Critic Loss: 0.40, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1730\n",
            "Tr: 10.00,Beam: 17, Iteration: 866, Q value: 0.3338, Reward: 1.0000, BF Gain pred: 7.53, BF Gain: -0.50, BFtrue Gain pred: -1.69, BFtrue Gain: 0.89,Critic Loss: 0.35, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1732\n",
            "Tr: 10.00,Beam: 17, Iteration: 867, Q value: 0.3343, Reward: -1.0000, BF Gain pred: 3.26, BF Gain: 8.07, BFtrue Gain pred: -1.38, BFtrue Gain: 9.24,Critic Loss: 0.33, Policy Loss: -0.33\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1734\n",
            "Tr: 10.00,Beam: 17, Iteration: 868, Q value: 0.3349, Reward: -1.0000, BF Gain pred: 1.19, BF Gain: 0.64, BFtrue Gain pred: -2.88, BFtrue Gain: 1.66,Critic Loss: 0.35, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1736\n",
            "Tr: 10.00,Beam: 17, Iteration: 869, Q value: 0.3354, Reward: -1.0000, BF Gain pred: 0.23, BF Gain: -5.71, BFtrue Gain pred: -4.34, BFtrue Gain: -4.98,Critic Loss: 0.39, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1738\n",
            "Tr: 10.00,Beam: 17, Iteration: 870, Q value: 0.3359, Reward: -1.0000, BF Gain pred: -3.45, BF Gain: -12.11, BFtrue Gain pred: 0.41, BFtrue Gain: -11.53,Critic Loss: 0.35, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1740\n",
            "Tr: 10.00,Beam: 17, Iteration: 871, Q value: 0.3365, Reward: 1.0000, BF Gain pred: -2.66, BF Gain: -1.81, BFtrue Gain pred: -4.31, BFtrue Gain: -1.59,Critic Loss: 0.35, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1742\n",
            "Tr: 10.00,Beam: 17, Iteration: 872, Q value: 0.3370, Reward: -1.0000, BF Gain pred: -4.29, BF Gain: -8.86, BFtrue Gain pred: -7.17, BFtrue Gain: -8.66,Critic Loss: 0.39, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1744\n",
            "Tr: 10.00,Beam: 17, Iteration: 873, Q value: 0.3375, Reward: -1.0000, BF Gain pred: -4.74, BF Gain: -2.45, BFtrue Gain pred: -7.83, BFtrue Gain: -2.51,Critic Loss: 0.34, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1746\n",
            "Tr: 10.00,Beam: 17, Iteration: 874, Q value: 0.3380, Reward: -1.0000, BF Gain pred: -9.06, BF Gain: -7.46, BFtrue Gain pred: -9.91, BFtrue Gain: -7.30,Critic Loss: 0.35, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1748\n",
            "Tr: 10.00,Beam: 17, Iteration: 875, Q value: 0.3385, Reward: 1.0000, BF Gain pred: -6.91, BF Gain: -5.02, BFtrue Gain pred: -8.90, BFtrue Gain: -5.38,Critic Loss: 0.37, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1750\n",
            "Tr: 10.00,Beam: 17, Iteration: 876, Q value: 0.3391, Reward: -1.0000, BF Gain pred: -7.14, BF Gain: -8.00, BFtrue Gain pred: -9.35, BFtrue Gain: -8.16,Critic Loss: 0.30, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1752\n",
            "Tr: 10.00,Beam: 17, Iteration: 877, Q value: 0.3395, Reward: -1.0000, BF Gain pred: -8.85, BF Gain: -9.03, BFtrue Gain pred: -8.96, BFtrue Gain: -8.87,Critic Loss: 0.38, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1754\n",
            "Tr: 10.00,Beam: 17, Iteration: 878, Q value: 0.3400, Reward: -1.0000, BF Gain pred: -8.85, BF Gain: -9.22, BFtrue Gain pred: -8.31, BFtrue Gain: -8.98,Critic Loss: 0.38, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1756\n",
            "Tr: 10.00,Beam: 17, Iteration: 879, Q value: 0.3405, Reward: 1.0000, BF Gain pred: -7.46, BF Gain: -2.02, BFtrue Gain pred: -8.08, BFtrue Gain: -2.25,Critic Loss: 0.37, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1758\n",
            "Tr: 10.00,Beam: 17, Iteration: 880, Q value: 0.3410, Reward: 1.0000, BF Gain pred: 1.54, BF Gain: 0.03, BFtrue Gain pred: -5.01, BFtrue Gain: -0.92,Critic Loss: 0.36, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1760\n",
            "Tr: 10.00,Beam: 17, Iteration: 881, Q value: 0.3415, Reward: -1.0000, BF Gain pred: 1.04, BF Gain: -0.53, BFtrue Gain pred: -2.74, BFtrue Gain: -0.74,Critic Loss: 0.34, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1762\n",
            "Tr: 10.00,Beam: 17, Iteration: 882, Q value: 0.3419, Reward: -1.0000, BF Gain pred: 0.41, BF Gain: -11.01, BFtrue Gain pred: -2.04, BFtrue Gain: -9.75,Critic Loss: 0.36, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1764\n",
            "Tr: 10.00,Beam: 17, Iteration: 883, Q value: 0.3319, Reward: -1.0000, BF Gain pred: -1.21, BF Gain: -8.99, BFtrue Gain pred: -2.13, BFtrue Gain: -8.27,Critic Loss: 0.36, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1766\n",
            "Tr: 10.00,Beam: 17, Iteration: 884, Q value: 0.2943, Reward: 1.0000, BF Gain pred: 3.15, BF Gain: -11.49, BFtrue Gain pred: 3.56, BFtrue Gain: -12.04,Critic Loss: 0.37, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1768\n",
            "Tr: 10.00,Beam: 17, Iteration: 885, Q value: 0.2172, Reward: -1.0000, BF Gain pred: 1.84, BF Gain: -9.60, BFtrue Gain pred: 1.03, BFtrue Gain: -9.28,Critic Loss: 0.33, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1770\n",
            "Tr: 10.00,Beam: 17, Iteration: 886, Q value: -0.5737, Reward: -1.0000, BF Gain pred: 0.97, BF Gain: -7.83, BFtrue Gain pred: 1.87, BFtrue Gain: -7.21,Critic Loss: 0.36, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1772\n",
            "Tr: 10.00,Beam: 17, Iteration: 887, Q value: -0.3188, Reward: -1.0000, BF Gain pred: 0.30, BF Gain: -0.34, BFtrue Gain pred: 2.46, BFtrue Gain: -0.36,Critic Loss: 0.41, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1774\n",
            "Tr: 10.00,Beam: 17, Iteration: 888, Q value: 0.1002, Reward: -1.0000, BF Gain pred: -2.92, BF Gain: -6.34, BFtrue Gain pred: 0.18, BFtrue Gain: -6.17,Critic Loss: 0.37, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1776\n",
            "Tr: 10.00,Beam: 17, Iteration: 889, Q value: -0.1986, Reward: -1.0000, BF Gain pred: -2.92, BF Gain: -4.54, BFtrue Gain pred: -4.55, BFtrue Gain: -5.20,Critic Loss: 0.38, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1778\n",
            "Tr: 10.00,Beam: 17, Iteration: 890, Q value: -0.1806, Reward: 1.0000, BF Gain pred: -2.30, BF Gain: -3.95, BFtrue Gain pred: -10.02, BFtrue Gain: -4.77,Critic Loss: 0.35, Policy Loss: -0.34\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1780\n",
            "Tr: 10.00,Beam: 17, Iteration: 891, Q value: 0.0473, Reward: -1.0000, BF Gain pred: -3.67, BF Gain: -0.70, BFtrue Gain pred: -12.20, BFtrue Gain: -1.35,Critic Loss: 0.36, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1782\n",
            "Tr: 10.00,Beam: 17, Iteration: 892, Q value: 0.1078, Reward: 1.0000, BF Gain pred: 0.43, BF Gain: -8.56, BFtrue Gain pred: -12.40, BFtrue Gain: -8.11,Critic Loss: 0.44, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1784\n",
            "Tr: 10.00,Beam: 17, Iteration: 893, Q value: -0.0026, Reward: -1.0000, BF Gain pred: -5.39, BF Gain: -6.72, BFtrue Gain pred: -13.05, BFtrue Gain: -6.40,Critic Loss: 0.35, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1786\n",
            "Tr: 10.00,Beam: 17, Iteration: 894, Q value: 0.1071, Reward: 1.0000, BF Gain pred: -2.98, BF Gain: 8.03, BFtrue Gain pred: -10.11, BFtrue Gain: 6.46,Critic Loss: 0.38, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1788\n",
            "Tr: 10.00,Beam: 17, Iteration: 895, Q value: 0.2098, Reward: 1.0000, BF Gain pred: -1.56, BF Gain: -1.01, BFtrue Gain pred: -10.99, BFtrue Gain: -0.25,Critic Loss: 0.34, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1790\n",
            "Tr: 10.00,Beam: 17, Iteration: 896, Q value: 0.2250, Reward: -1.0000, BF Gain pred: -3.69, BF Gain: -2.93, BFtrue Gain pred: -8.23, BFtrue Gain: -3.29,Critic Loss: 0.40, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1792\n",
            "Tr: 10.00,Beam: 17, Iteration: 897, Q value: 0.1904, Reward: 1.0000, BF Gain pred: 0.60, BF Gain: 3.43, BFtrue Gain pred: 0.65, BFtrue Gain: 3.31,Critic Loss: 0.41, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1794\n",
            "Tr: 10.00,Beam: 17, Iteration: 898, Q value: 0.1694, Reward: -1.0000, BF Gain pred: -3.94, BF Gain: -5.14, BFtrue Gain pred: -12.19, BFtrue Gain: -4.97,Critic Loss: 0.38, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1796\n",
            "Tr: 10.00,Beam: 17, Iteration: 899, Q value: 0.2466, Reward: 1.0000, BF Gain pred: 3.08, BF Gain: -10.47, BFtrue Gain pred: 1.45, BFtrue Gain: -9.94,Critic Loss: 0.37, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1798\n",
            "Tr: 10.00,Beam: 17, Iteration: 900, Q value: 0.3185, Reward: -1.0000, BF Gain pred: -3.99, BF Gain: -3.34, BFtrue Gain pred: -7.90, BFtrue Gain: -3.48,Critic Loss: 0.42, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1800\n",
            "Tr: 10.00,Beam: 17, Iteration: 901, Q value: 0.2342, Reward: -1.0000, BF Gain pred: -9.89, BF Gain: -14.50, BFtrue Gain pred: -16.77, BFtrue Gain: -14.27,Critic Loss: 0.35, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1802\n",
            "Tr: 10.00,Beam: 17, Iteration: 902, Q value: 0.3030, Reward: 1.0000, BF Gain pred: -1.35, BF Gain: -3.29, BFtrue Gain pred: -2.71, BFtrue Gain: -3.93,Critic Loss: 0.44, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1804\n",
            "Tr: 10.00,Beam: 17, Iteration: 903, Q value: 0.2612, Reward: -1.0000, BF Gain pred: -5.88, BF Gain: -16.82, BFtrue Gain pred: -13.00, BFtrue Gain: -15.86,Critic Loss: 0.37, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1806\n",
            "Tr: 10.00,Beam: 17, Iteration: 904, Q value: 0.2705, Reward: -1.0000, BF Gain pred: -8.04, BF Gain: -4.97, BFtrue Gain pred: -7.45, BFtrue Gain: -5.40,Critic Loss: 0.40, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1808\n",
            "Tr: 10.00,Beam: 17, Iteration: 905, Q value: 0.2904, Reward: 1.0000, BF Gain pred: -1.12, BF Gain: -3.67, BFtrue Gain pred: -7.65, BFtrue Gain: -3.88,Critic Loss: 0.35, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1810\n",
            "Tr: 10.00,Beam: 17, Iteration: 906, Q value: 0.3254, Reward: -1.0000, BF Gain pred: -9.67, BF Gain: -8.79, BFtrue Gain pred: -19.51, BFtrue Gain: -10.06,Critic Loss: 0.35, Policy Loss: -0.35\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1812\n",
            "Tr: 10.00,Beam: 17, Iteration: 907, Q value: 0.2650, Reward: 1.0000, BF Gain pred: 1.88, BF Gain: -1.43, BFtrue Gain pred: 5.30, BFtrue Gain: -1.40,Critic Loss: 0.32, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1814\n",
            "Tr: 10.00,Beam: 17, Iteration: 908, Q value: 0.3559, Reward: -1.0000, BF Gain pred: -7.03, BF Gain: -6.57, BFtrue Gain pred: -18.18, BFtrue Gain: -7.17,Critic Loss: 0.35, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1816\n",
            "Tr: 10.00,Beam: 17, Iteration: 909, Q value: 0.3564, Reward: 1.0000, BF Gain pred: 2.70, BF Gain: -7.18, BFtrue Gain pred: -6.36, BFtrue Gain: -8.32,Critic Loss: 0.34, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1818\n",
            "Tr: 10.00,Beam: 17, Iteration: 910, Q value: 0.2098, Reward: -1.0000, BF Gain pred: -5.90, BF Gain: -5.29, BFtrue Gain pred: -13.45, BFtrue Gain: -5.99,Critic Loss: 0.34, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1820\n",
            "Tr: 10.00,Beam: 17, Iteration: 911, Q value: 0.3575, Reward: 1.0000, BF Gain pred: -1.48, BF Gain: -4.48, BFtrue Gain pred: -7.44, BFtrue Gain: -5.61,Critic Loss: 0.34, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1822\n",
            "Tr: 10.00,Beam: 17, Iteration: 912, Q value: 0.3580, Reward: -1.0000, BF Gain pred: -2.80, BF Gain: -3.08, BFtrue Gain pred: -6.49, BFtrue Gain: -3.80,Critic Loss: 0.38, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1824\n",
            "Tr: 10.00,Beam: 17, Iteration: 913, Q value: 0.3585, Reward: 1.0000, BF Gain pred: -2.08, BF Gain: -13.44, BFtrue Gain pred: -4.98, BFtrue Gain: -14.21,Critic Loss: 0.33, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1826\n",
            "Tr: 10.00,Beam: 17, Iteration: 914, Q value: 0.3590, Reward: -1.0000, BF Gain pred: -2.08, BF Gain: -5.77, BFtrue Gain pred: -4.31, BFtrue Gain: -7.17,Critic Loss: 0.33, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1828\n",
            "Tr: 10.00,Beam: 17, Iteration: 915, Q value: 0.3595, Reward: -1.0000, BF Gain pred: -2.08, BF Gain: -1.55, BFtrue Gain pred: -3.76, BFtrue Gain: -1.91,Critic Loss: 0.38, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1830\n",
            "Tr: 10.00,Beam: 17, Iteration: 916, Q value: 0.3600, Reward: -1.0000, BF Gain pred: -2.90, BF Gain: -9.96, BFtrue Gain pred: -3.37, BFtrue Gain: -9.60,Critic Loss: 0.38, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1832\n",
            "Tr: 10.00,Beam: 17, Iteration: 917, Q value: 0.3605, Reward: -1.0000, BF Gain pred: -2.99, BF Gain: -11.28, BFtrue Gain pred: -2.79, BFtrue Gain: -10.35,Critic Loss: 0.36, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1834\n",
            "Tr: 10.00,Beam: 17, Iteration: 918, Q value: 0.3610, Reward: 1.0000, BF Gain pred: -1.65, BF Gain: -5.35, BFtrue Gain pred: -2.58, BFtrue Gain: -5.12,Critic Loss: 0.36, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1836\n",
            "Tr: 10.00,Beam: 17, Iteration: 919, Q value: 0.3616, Reward: 1.0000, BF Gain pred: -1.20, BF Gain: -4.18, BFtrue Gain pred: -1.40, BFtrue Gain: -1.61,Critic Loss: 0.38, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1838\n",
            "Tr: 10.00,Beam: 17, Iteration: 920, Q value: 0.2964, Reward: -1.0000, BF Gain pred: -3.06, BF Gain: 0.81, BFtrue Gain pred: -2.75, BFtrue Gain: -0.35,Critic Loss: 0.36, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1840\n",
            "Tr: 10.00,Beam: 17, Iteration: 921, Q value: 0.3626, Reward: 1.0000, BF Gain pred: -0.50, BF Gain: 0.43, BFtrue Gain pred: 0.25, BFtrue Gain: -0.45,Critic Loss: 0.36, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1842\n",
            "Tr: 10.00,Beam: 17, Iteration: 922, Q value: 0.3631, Reward: -1.0000, BF Gain pred: -1.12, BF Gain: -11.77, BFtrue Gain pred: -2.09, BFtrue Gain: -10.81,Critic Loss: 0.37, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1844\n",
            "Tr: 10.00,Beam: 17, Iteration: 923, Q value: 0.3636, Reward: -1.0000, BF Gain pred: -1.26, BF Gain: -0.84, BFtrue Gain pred: 0.67, BFtrue Gain: -1.40,Critic Loss: 0.33, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1846\n",
            "Tr: 10.00,Beam: 17, Iteration: 924, Q value: 0.2818, Reward: -1.0000, BF Gain pred: -2.86, BF Gain: -6.99, BFtrue Gain pred: -3.43, BFtrue Gain: -6.88,Critic Loss: 0.35, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1848\n",
            "Tr: 10.00,Beam: 17, Iteration: 925, Q value: 0.3404, Reward: -1.0000, BF Gain pred: -3.78, BF Gain: -15.03, BFtrue Gain pred: -1.76, BFtrue Gain: -14.98,Critic Loss: 0.36, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1850\n",
            "Tr: 10.00,Beam: 17, Iteration: 926, Q value: 0.3652, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: -3.60, BFtrue Gain pred: -1.77, BFtrue Gain: -3.18,Critic Loss: 0.35, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1852\n",
            "Tr: 10.00,Beam: 17, Iteration: 927, Q value: 0.1449, Reward: -1.0000, BF Gain pred: -2.64, BF Gain: -6.22, BFtrue Gain pred: -4.17, BFtrue Gain: -6.03,Critic Loss: 0.41, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1854\n",
            "Tr: 10.00,Beam: 17, Iteration: 928, Q value: 0.1972, Reward: -1.0000, BF Gain pred: -4.95, BF Gain: -2.81, BFtrue Gain pred: -5.73, BFtrue Gain: -2.02,Critic Loss: 0.35, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1856\n",
            "Tr: 10.00,Beam: 17, Iteration: 929, Q value: 0.3351, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: -3.87, BFtrue Gain pred: -5.86, BFtrue Gain: -3.44,Critic Loss: 0.38, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1858\n",
            "Tr: 10.00,Beam: 17, Iteration: 930, Q value: -0.0193, Reward: -1.0000, BF Gain pred: -7.34, BF Gain: -11.32, BFtrue Gain pred: -4.62, BFtrue Gain: -10.16,Critic Loss: 0.34, Policy Loss: -0.36\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1860\n",
            "Tr: 10.00,Beam: 17, Iteration: 931, Q value: 0.2713, Reward: 1.0000, BF Gain pred: -6.74, BF Gain: -4.71, BFtrue Gain pred: 0.11, BFtrue Gain: -4.68,Critic Loss: 0.36, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1862\n",
            "Tr: 10.00,Beam: 17, Iteration: 932, Q value: -0.0011, Reward: 1.0000, BF Gain pred: -4.92, BF Gain: -9.35, BFtrue Gain pred: -6.06, BFtrue Gain: -10.40,Critic Loss: 0.40, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1864\n",
            "Tr: 10.00,Beam: 17, Iteration: 933, Q value: 0.2462, Reward: 1.0000, BF Gain pred: -2.96, BF Gain: -1.69, BFtrue Gain pred: -7.07, BFtrue Gain: -0.31,Critic Loss: 0.32, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1866\n",
            "Tr: 10.00,Beam: 17, Iteration: 934, Q value: -0.2152, Reward: 1.0000, BF Gain pred: 9.96, BF Gain: 0.16, BFtrue Gain pred: -2.77, BFtrue Gain: 0.31,Critic Loss: 0.34, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1868\n",
            "Tr: 10.00,Beam: 17, Iteration: 935, Q value: 0.2370, Reward: -1.0000, BF Gain pred: -5.48, BF Gain: 0.38, BFtrue Gain pred: -8.90, BFtrue Gain: 0.70,Critic Loss: 0.39, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1870\n",
            "Tr: 10.00,Beam: 17, Iteration: 936, Q value: -0.1480, Reward: 1.0000, BF Gain pred: 11.96, BF Gain: -2.01, BFtrue Gain pred: -5.14, BFtrue Gain: -2.26,Critic Loss: 0.35, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1872\n",
            "Tr: 10.00,Beam: 17, Iteration: 937, Q value: 0.3102, Reward: -1.0000, BF Gain pred: -2.85, BF Gain: -14.48, BFtrue Gain pred: -8.79, BFtrue Gain: -13.49,Critic Loss: 0.35, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1874\n",
            "Tr: 10.00,Beam: 17, Iteration: 938, Q value: -0.0480, Reward: 1.0000, BF Gain pred: 0.19, BF Gain: -7.03, BFtrue Gain pred: -5.58, BFtrue Gain: -7.35,Critic Loss: 0.34, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1876\n",
            "Tr: 10.00,Beam: 17, Iteration: 939, Q value: 0.3556, Reward: -1.0000, BF Gain pred: -3.99, BF Gain: -4.54, BFtrue Gain pred: -11.63, BFtrue Gain: -3.28,Critic Loss: 0.36, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1878\n",
            "Tr: 10.00,Beam: 17, Iteration: 940, Q value: 0.2859, Reward: 1.0000, BF Gain pred: 6.48, BF Gain: -3.06, BFtrue Gain pred: -3.42, BFtrue Gain: -2.80,Critic Loss: 0.37, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1880\n",
            "Tr: 10.00,Beam: 17, Iteration: 941, Q value: 0.3274, Reward: -1.0000, BF Gain pred: 1.45, BF Gain: -3.15, BFtrue Gain pred: -11.85, BFtrue Gain: -1.96,Critic Loss: 0.30, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1882\n",
            "Tr: 10.00,Beam: 17, Iteration: 942, Q value: 0.3228, Reward: 1.0000, BF Gain pred: 1.76, BF Gain: -4.80, BFtrue Gain pred: 0.06, BFtrue Gain: -3.92,Critic Loss: 0.38, Policy Loss: -0.37\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1884\n",
            "Tr: 10.00,Beam: 17, Iteration: 943, Q value: 0.3747, Reward: 1.0000, BF Gain pred: 2.91, BF Gain: -0.08, BFtrue Gain pred: -7.26, BFtrue Gain: 1.40,Critic Loss: 0.39, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1886\n",
            "Tr: 10.00,Beam: 17, Iteration: 944, Q value: 0.2658, Reward: 1.0000, BF Gain pred: 3.91, BF Gain: -3.25, BFtrue Gain pred: -0.34, BFtrue Gain: -3.69,Critic Loss: 0.35, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1888\n",
            "Tr: 10.00,Beam: 17, Iteration: 945, Q value: 0.3759, Reward: 1.0000, BF Gain pred: 4.41, BF Gain: -12.49, BFtrue Gain pred: -5.98, BFtrue Gain: -11.49,Critic Loss: 0.38, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1890\n",
            "Tr: 10.00,Beam: 17, Iteration: 946, Q value: 0.2249, Reward: 1.0000, BF Gain pred: 5.01, BF Gain: -1.74, BFtrue Gain pred: 0.29, BFtrue Gain: -1.31,Critic Loss: 0.38, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1892\n",
            "Tr: 10.00,Beam: 17, Iteration: 947, Q value: 0.3643, Reward: -1.0000, BF Gain pred: 2.65, BF Gain: -2.53, BFtrue Gain pred: -5.90, BFtrue Gain: -2.05,Critic Loss: 0.34, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1894\n",
            "Tr: 10.00,Beam: 17, Iteration: 948, Q value: 0.3593, Reward: -1.0000, BF Gain pred: -4.87, BF Gain: -6.40, BFtrue Gain pred: -10.68, BFtrue Gain: -5.82,Critic Loss: 0.37, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1896\n",
            "Tr: 10.00,Beam: 17, Iteration: 949, Q value: 0.3727, Reward: 1.0000, BF Gain pred: -4.84, BF Gain: -1.84, BFtrue Gain pred: -8.04, BFtrue Gain: -0.87,Critic Loss: 0.35, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1898\n",
            "Tr: 10.00,Beam: 17, Iteration: 950, Q value: 0.2379, Reward: -1.0000, BF Gain pred: -5.03, BF Gain: -0.83, BFtrue Gain pred: -11.10, BFtrue Gain: -1.00,Critic Loss: 0.36, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1900\n",
            "Tr: 10.00,Beam: 17, Iteration: 951, Q value: 0.3298, Reward: 1.0000, BF Gain pred: -1.06, BF Gain: -3.24, BFtrue Gain pred: -4.06, BFtrue Gain: -2.55,Critic Loss: 0.36, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1902\n",
            "Tr: 10.00,Beam: 17, Iteration: 952, Q value: 0.3304, Reward: -1.0000, BF Gain pred: -9.96, BF Gain: -2.20, BFtrue Gain pred: -12.84, BFtrue Gain: -1.96,Critic Loss: 0.32, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1904\n",
            "Tr: 10.00,Beam: 17, Iteration: 953, Q value: 0.3045, Reward: 1.0000, BF Gain pred: -4.29, BF Gain: -18.94, BFtrue Gain pred: -7.19, BFtrue Gain: -18.19,Critic Loss: 0.34, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1906\n",
            "Tr: 10.00,Beam: 17, Iteration: 954, Q value: 0.3246, Reward: -1.0000, BF Gain pred: -4.82, BF Gain: 4.22, BFtrue Gain pred: -8.75, BFtrue Gain: 4.90,Critic Loss: 0.33, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1908\n",
            "Tr: 10.00,Beam: 17, Iteration: 955, Q value: 0.1995, Reward: 1.0000, BF Gain pred: -4.16, BF Gain: -14.92, BFtrue Gain pred: -9.66, BFtrue Gain: -14.48,Critic Loss: 0.36, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1910\n",
            "Tr: 10.00,Beam: 17, Iteration: 956, Q value: 0.3717, Reward: -1.0000, BF Gain pred: -4.82, BF Gain: -8.18, BFtrue Gain pred: -9.00, BFtrue Gain: -7.65,Critic Loss: 0.41, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1912\n",
            "Tr: 10.00,Beam: 17, Iteration: 957, Q value: 0.3743, Reward: 1.0000, BF Gain pred: -2.12, BF Gain: 1.43, BFtrue Gain pred: -6.34, BFtrue Gain: 1.88,Critic Loss: 0.33, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1914\n",
            "Tr: 10.00,Beam: 17, Iteration: 958, Q value: 0.3834, Reward: -1.0000, BF Gain pred: -4.70, BF Gain: 0.60, BFtrue Gain pred: -5.37, BFtrue Gain: 0.98,Critic Loss: 0.33, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1916\n",
            "Tr: 10.00,Beam: 17, Iteration: 959, Q value: 0.3840, Reward: 1.0000, BF Gain pred: 0.82, BF Gain: -1.40, BFtrue Gain pred: -6.30, BFtrue Gain: -1.11,Critic Loss: 0.34, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1918\n",
            "Tr: 10.00,Beam: 17, Iteration: 960, Q value: 0.3138, Reward: -1.0000, BF Gain pred: -0.38, BF Gain: -0.29, BFtrue Gain pred: -1.01, BFtrue Gain: -1.03,Critic Loss: 0.38, Policy Loss: -0.38\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1920\n",
            "Tr: 10.00,Beam: 17, Iteration: 961, Q value: 0.3851, Reward: 1.0000, BF Gain pred: 1.09, BF Gain: 0.07, BFtrue Gain pred: -3.19, BFtrue Gain: 1.81,Critic Loss: 0.35, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1922\n",
            "Tr: 10.00,Beam: 17, Iteration: 962, Q value: 0.2895, Reward: 1.0000, BF Gain pred: 6.89, BF Gain: -1.43, BFtrue Gain pred: 1.63, BFtrue Gain: -0.75,Critic Loss: 0.37, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1924\n",
            "Tr: 10.00,Beam: 17, Iteration: 963, Q value: 0.3862, Reward: -1.0000, BF Gain pred: -0.73, BF Gain: -8.75, BFtrue Gain pred: -5.01, BFtrue Gain: -8.32,Critic Loss: 0.36, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1926\n",
            "Tr: 10.00,Beam: 17, Iteration: 964, Q value: -0.0981, Reward: 1.0000, BF Gain pred: 2.05, BF Gain: -2.86, BFtrue Gain pred: -1.15, BFtrue Gain: -2.27,Critic Loss: 0.32, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1928\n",
            "Tr: 10.00,Beam: 17, Iteration: 965, Q value: 0.3873, Reward: -1.0000, BF Gain pred: -1.76, BF Gain: -9.97, BFtrue Gain pred: -5.23, BFtrue Gain: -10.58,Critic Loss: 0.36, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1930\n",
            "Tr: 10.00,Beam: 17, Iteration: 966, Q value: 0.0385, Reward: -1.0000, BF Gain pred: -2.57, BF Gain: -4.50, BFtrue Gain pred: -2.73, BFtrue Gain: -3.84,Critic Loss: 0.34, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1932\n",
            "Tr: 10.00,Beam: 17, Iteration: 967, Q value: 0.3839, Reward: 1.0000, BF Gain pred: -1.01, BF Gain: -11.33, BFtrue Gain pred: -0.78, BFtrue Gain: -11.40,Critic Loss: 0.35, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1934\n",
            "Tr: 10.00,Beam: 17, Iteration: 968, Q value: 0.3889, Reward: -1.0000, BF Gain pred: -1.02, BF Gain: -8.45, BFtrue Gain pred: -1.53, BFtrue Gain: -7.56,Critic Loss: 0.36, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1936\n",
            "Tr: 10.00,Beam: 17, Iteration: 969, Q value: 0.2922, Reward: -1.0000, BF Gain pred: -1.90, BF Gain: -10.68, BFtrue Gain pred: -4.82, BFtrue Gain: -10.03,Critic Loss: 0.33, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1938\n",
            "Tr: 10.00,Beam: 17, Iteration: 970, Q value: 0.1793, Reward: -1.0000, BF Gain pred: -3.45, BF Gain: -14.22, BFtrue Gain pred: -4.04, BFtrue Gain: -14.01,Critic Loss: 0.35, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1940\n",
            "Tr: 10.00,Beam: 17, Iteration: 971, Q value: 0.2675, Reward: 1.0000, BF Gain pred: -0.79, BF Gain: -3.19, BFtrue Gain pred: 0.22, BFtrue Gain: -2.57,Critic Loss: 0.35, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1942\n",
            "Tr: 10.00,Beam: 17, Iteration: 972, Q value: 0.3297, Reward: 1.0000, BF Gain pred: -0.32, BF Gain: -11.34, BFtrue Gain pred: -1.82, BFtrue Gain: -10.83,Critic Loss: 0.33, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1944\n",
            "Tr: 10.00,Beam: 17, Iteration: 973, Q value: 0.3823, Reward: -1.0000, BF Gain pred: -2.71, BF Gain: -17.46, BFtrue Gain pred: -2.04, BFtrue Gain: -17.55,Critic Loss: 0.35, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1946\n",
            "Tr: 10.00,Beam: 17, Iteration: 974, Q value: 0.3922, Reward: 1.0000, BF Gain pred: -0.08, BF Gain: -5.40, BFtrue Gain pred: -0.61, BFtrue Gain: -4.60,Critic Loss: 0.31, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1948\n",
            "Tr: 10.00,Beam: 17, Iteration: 975, Q value: 0.3311, Reward: -1.0000, BF Gain pred: -3.44, BF Gain: -6.81, BFtrue Gain pred: -1.38, BFtrue Gain: -6.09,Critic Loss: 0.32, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1950\n",
            "Tr: 10.00,Beam: 17, Iteration: 976, Q value: 0.3296, Reward: -1.0000, BF Gain pred: -5.28, BF Gain: 0.51, BFtrue Gain pred: -3.78, BFtrue Gain: 0.43,Critic Loss: 0.31, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1952\n",
            "Tr: 10.00,Beam: 17, Iteration: 977, Q value: 0.3939, Reward: 1.0000, BF Gain pred: 1.46, BF Gain: -6.50, BFtrue Gain pred: -4.33, BFtrue Gain: -5.93,Critic Loss: 0.34, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1954\n",
            "Tr: 10.00,Beam: 17, Iteration: 978, Q value: 0.2802, Reward: -1.0000, BF Gain pred: -1.59, BF Gain: -5.28, BFtrue Gain pred: -6.65, BFtrue Gain: -5.21,Critic Loss: 0.31, Policy Loss: -0.39\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1956\n",
            "Tr: 10.00,Beam: 17, Iteration: 979, Q value: 0.3934, Reward: 1.0000, BF Gain pred: 4.41, BF Gain: -4.96, BFtrue Gain pred: -1.18, BFtrue Gain: -4.17,Critic Loss: 0.34, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1958\n",
            "Tr: 10.00,Beam: 17, Iteration: 980, Q value: -0.0764, Reward: -1.0000, BF Gain pred: 3.42, BF Gain: -7.92, BFtrue Gain pred: -4.39, BFtrue Gain: -7.88,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1960\n",
            "Tr: 10.00,Beam: 17, Iteration: 981, Q value: 0.3962, Reward: 1.0000, BF Gain pred: 7.29, BF Gain: 2.23, BFtrue Gain pred: 11.27, BFtrue Gain: 2.11,Critic Loss: 0.33, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1962\n",
            "Tr: 10.00,Beam: 17, Iteration: 982, Q value: 0.3148, Reward: -1.0000, BF Gain pred: -3.13, BF Gain: -2.27, BFtrue Gain pred: -5.41, BFtrue Gain: -1.39,Critic Loss: 0.33, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1964\n",
            "Tr: 10.00,Beam: 17, Iteration: 983, Q value: 0.3973, Reward: 1.0000, BF Gain pred: 6.04, BF Gain: 3.63, BFtrue Gain pred: 0.95, BFtrue Gain: 4.00,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1966\n",
            "Tr: 10.00,Beam: 17, Iteration: 984, Q value: 0.3551, Reward: -1.0000, BF Gain pred: 4.74, BF Gain: -5.83, BFtrue Gain pred: -1.10, BFtrue Gain: -5.54,Critic Loss: 0.32, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1968\n",
            "Tr: 10.00,Beam: 17, Iteration: 985, Q value: 0.3984, Reward: 1.0000, BF Gain pred: 6.94, BF Gain: 3.93, BFtrue Gain pred: -3.02, BFtrue Gain: 3.92,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1970\n",
            "Tr: 10.00,Beam: 17, Iteration: 986, Q value: 0.1227, Reward: 1.0000, BF Gain pred: 12.22, BF Gain: -0.43, BFtrue Gain pred: -0.59, BFtrue Gain: -0.32,Critic Loss: 0.30, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1972\n",
            "Tr: 10.00,Beam: 17, Iteration: 987, Q value: 0.3994, Reward: -1.0000, BF Gain pred: 5.46, BF Gain: -2.14, BFtrue Gain pred: 5.23, BFtrue Gain: 0.28,Critic Loss: 0.36, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1974\n",
            "Tr: 10.00,Beam: 17, Iteration: 988, Q value: 0.3586, Reward: 1.0000, BF Gain pred: 6.31, BF Gain: -1.08, BFtrue Gain pred: 0.47, BFtrue Gain: -1.60,Critic Loss: 0.29, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1976\n",
            "Tr: 10.00,Beam: 17, Iteration: 989, Q value: 0.4004, Reward: -1.0000, BF Gain pred: 1.48, BF Gain: 5.16, BFtrue Gain pred: 2.14, BFtrue Gain: 4.83,Critic Loss: 0.33, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1978\n",
            "Tr: 10.00,Beam: 17, Iteration: 990, Q value: 0.3905, Reward: 1.0000, BF Gain pred: 4.16, BF Gain: -1.48, BFtrue Gain pred: 1.43, BFtrue Gain: 0.23,Critic Loss: 0.35, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1980\n",
            "Tr: 10.00,Beam: 17, Iteration: 991, Q value: 0.3645, Reward: -1.0000, BF Gain pred: -0.35, BF Gain: -9.63, BFtrue Gain pred: -1.05, BFtrue Gain: -9.03,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1982\n",
            "Tr: 10.00,Beam: 17, Iteration: 992, Q value: 0.3926, Reward: -1.0000, BF Gain pred: -0.55, BF Gain: -7.56, BFtrue Gain pred: -0.25, BFtrue Gain: -7.33,Critic Loss: 0.34, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1984\n",
            "Tr: 10.00,Beam: 17, Iteration: 993, Q value: 0.4024, Reward: 1.0000, BF Gain pred: 3.05, BF Gain: -6.61, BFtrue Gain pred: -2.19, BFtrue Gain: -6.74,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1986\n",
            "Tr: 10.00,Beam: 17, Iteration: 994, Q value: 0.3990, Reward: -1.0000, BF Gain pred: -0.90, BF Gain: -6.63, BFtrue Gain pred: -2.90, BFtrue Gain: -5.49,Critic Loss: 0.32, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1988\n",
            "Tr: 10.00,Beam: 17, Iteration: 995, Q value: 0.4033, Reward: 1.0000, BF Gain pred: -0.05, BF Gain: -4.29, BFtrue Gain pred: -1.55, BFtrue Gain: -5.86,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1990\n",
            "Tr: 10.00,Beam: 17, Iteration: 996, Q value: 0.3716, Reward: 1.0000, BF Gain pred: 2.00, BF Gain: -10.28, BFtrue Gain pred: -3.32, BFtrue Gain: -10.95,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1992\n",
            "Tr: 10.00,Beam: 17, Iteration: 997, Q value: 0.4043, Reward: -1.0000, BF Gain pred: 1.26, BF Gain: -2.39, BFtrue Gain pred: -2.43, BFtrue Gain: -2.74,Critic Loss: 0.31, Policy Loss: -0.40\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1994\n",
            "Tr: 10.00,Beam: 17, Iteration: 998, Q value: 0.4048, Reward: -1.0000, BF Gain pred: -3.34, BF Gain: -2.10, BFtrue Gain pred: -2.65, BFtrue Gain: -2.61,Critic Loss: 0.30, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1996\n",
            "Tr: 10.00,Beam: 17, Iteration: 999, Q value: 0.3813, Reward: 1.0000, BF Gain pred: 4.94, BF Gain: -7.52, BFtrue Gain pred: -2.10, BFtrue Gain: -6.79,Critic Loss: 0.31, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 1998\n",
            "Tr: 10.00,Beam: 17, Iteration: 1000, Q value: 0.3542, Reward: -1.0000, BF Gain pred: -1.36, BF Gain: -4.94, BFtrue Gain pred: -4.64, BFtrue Gain: -3.26,Critic Loss: 0.33, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2000\n",
            "Tr: 10.00,Beam: 17, Iteration: 1001, Q value: 0.4064, Reward: 1.0000, BF Gain pred: 0.56, BF Gain: -7.40, BFtrue Gain pred: -0.39, BFtrue Gain: -6.09,Critic Loss: 0.30, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2002\n",
            "Tr: 10.00,Beam: 17, Iteration: 1002, Q value: -0.1815, Reward: 1.0000, BF Gain pred: 6.60, BF Gain: -7.53, BFtrue Gain pred: 0.98, BFtrue Gain: -7.86,Critic Loss: 0.33, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2004\n",
            "Tr: 10.00,Beam: 17, Iteration: 1003, Q value: 0.3940, Reward: -1.0000, BF Gain pred: 0.87, BF Gain: -14.84, BFtrue Gain pred: -1.73, BFtrue Gain: -13.27,Critic Loss: 0.32, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2006\n",
            "Tr: 10.00,Beam: 17, Iteration: 1004, Q value: 0.3393, Reward: -1.0000, BF Gain pred: -2.71, BF Gain: -10.15, BFtrue Gain pred: -4.74, BFtrue Gain: -8.38,Critic Loss: 0.32, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2008\n",
            "Tr: 10.00,Beam: 17, Iteration: 1005, Q value: 0.3909, Reward: 1.0000, BF Gain pred: 3.49, BF Gain: -3.16, BFtrue Gain pred: 3.28, BFtrue Gain: -2.73,Critic Loss: 0.33, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2010\n",
            "Tr: 10.00,Beam: 17, Iteration: 1006, Q value: 0.3523, Reward: -1.0000, BF Gain pred: -1.02, BF Gain: -0.61, BFtrue Gain pred: -3.92, BFtrue Gain: -0.50,Critic Loss: 0.30, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2012\n",
            "Tr: 10.00,Beam: 17, Iteration: 1007, Q value: 0.4097, Reward: -1.0000, BF Gain pred: -2.42, BF Gain: 0.97, BFtrue Gain pred: 1.71, BFtrue Gain: 1.45,Critic Loss: 0.35, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2014\n",
            "Tr: 10.00,Beam: 17, Iteration: 1008, Q value: 0.4103, Reward: -1.0000, BF Gain pred: -3.70, BF Gain: -2.83, BFtrue Gain pred: -2.16, BFtrue Gain: -3.37,Critic Loss: 0.29, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2016\n",
            "Tr: 10.00,Beam: 17, Iteration: 1009, Q value: 0.4109, Reward: 1.0000, BF Gain pred: -0.34, BF Gain: 13.33, BFtrue Gain pred: 1.01, BFtrue Gain: 17.20,Critic Loss: 0.32, Policy Loss: -0.41\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2018\n",
            "Tr: 10.00,Beam: 17, Iteration: 1010, Q value: 0.4049, Reward: 1.0000, BF Gain pred: -0.24, BF Gain: 7.79, BFtrue Gain pred: 0.13, BFtrue Gain: 9.28,Critic Loss: 0.34, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2020\n",
            "Tr: 10.00,Beam: 17, Iteration: 1011, Q value: 0.3605, Reward: -1.0000, BF Gain pred: -0.24, BF Gain: 4.15, BFtrue Gain pred: -0.52, BFtrue Gain: 5.83,Critic Loss: 0.30, Policy Loss: -0.41\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2022\n",
            "Tr: 10.00,Beam: 17, Iteration: 1012, Q value: 0.3993, Reward: 1.0000, BF Gain pred: 2.42, BF Gain: -6.37, BFtrue Gain pred: -1.02, BFtrue Gain: -5.80,Critic Loss: 0.34, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2024\n",
            "Tr: 10.00,Beam: 17, Iteration: 1013, Q value: 0.3833, Reward: 1.0000, BF Gain pred: 4.81, BF Gain: -7.60, BFtrue Gain pred: -1.31, BFtrue Gain: -6.16,Critic Loss: 0.31, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2026\n",
            "Tr: 10.00,Beam: 17, Iteration: 1014, Q value: 0.3430, Reward: -1.0000, BF Gain pred: 2.45, BF Gain: 1.64, BFtrue Gain pred: -1.62, BFtrue Gain: 2.06,Critic Loss: 0.30, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2028\n",
            "Tr: 10.00,Beam: 17, Iteration: 1015, Q value: 0.0095, Reward: 1.0000, BF Gain pred: 4.81, BF Gain: -3.47, BFtrue Gain pred: -2.16, BFtrue Gain: -2.86,Critic Loss: 0.34, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2030\n",
            "Tr: 10.00,Beam: 17, Iteration: 1016, Q value: 0.2732, Reward: -1.0000, BF Gain pred: 3.06, BF Gain: -3.69, BFtrue Gain pred: -1.41, BFtrue Gain: -3.62,Critic Loss: 0.33, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2032\n",
            "Tr: 10.00,Beam: 17, Iteration: 1017, Q value: 0.3196, Reward: 1.0000, BF Gain pred: 5.10, BF Gain: -1.31, BFtrue Gain pred: 1.62, BFtrue Gain: -0.94,Critic Loss: 0.32, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2034\n",
            "Tr: 10.00,Beam: 17, Iteration: 1018, Q value: -0.1961, Reward: -1.0000, BF Gain pred: 0.59, BF Gain: -2.80, BFtrue Gain pred: 3.78, BFtrue Gain: -2.41,Critic Loss: 0.31, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2036\n",
            "Tr: 10.00,Beam: 17, Iteration: 1019, Q value: -0.1985, Reward: 1.0000, BF Gain pred: 7.42, BF Gain: 3.89, BFtrue Gain pred: 4.55, BFtrue Gain: 2.53,Critic Loss: 0.31, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2038\n",
            "Tr: 10.00,Beam: 17, Iteration: 1020, Q value: -0.9276, Reward: 1.0000, BF Gain pred: 13.43, BF Gain: 3.63, BFtrue Gain pred: 2.60, BFtrue Gain: 2.85,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2040\n",
            "Tr: 10.00,Beam: 17, Iteration: 1021, Q value: -1.0733, Reward: -1.0000, BF Gain pred: 12.89, BF Gain: -1.07, BFtrue Gain pred: 2.13, BFtrue Gain: -0.74,Critic Loss: 0.30, Policy Loss: -0.42\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2042\n",
            "Tr: 10.00,Beam: 17, Iteration: 1022, Q value: -0.4916, Reward: -1.0000, BF Gain pred: 4.55, BF Gain: -5.30, BFtrue Gain pred: -0.76, BFtrue Gain: -6.47,Critic Loss: 0.33, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2044\n",
            "Tr: 10.00,Beam: 17, Iteration: 1023, Q value: -0.0063, Reward: -1.0000, BF Gain pred: 2.01, BF Gain: -1.62, BFtrue Gain pred: -4.50, BFtrue Gain: -3.37,Critic Loss: 0.30, Policy Loss: -0.42\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2046\n",
            "Tr: 10.00,Beam: 17, Iteration: 1024, Q value: 0.0226, Reward: -1.0000, BF Gain pred: -6.39, BF Gain: -4.97, BFtrue Gain pred: -9.57, BFtrue Gain: -6.45,Critic Loss: 0.30, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2048\n",
            "Tr: 10.00,Beam: 17, Iteration: 1025, Q value: 0.1925, Reward: 1.0000, BF Gain pred: -0.69, BF Gain: -9.70, BFtrue Gain pred: -11.34, BFtrue Gain: -9.70,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2050\n",
            "Tr: 10.00,Beam: 17, Iteration: 1026, Q value: 0.2232, Reward: -1.0000, BF Gain pred: -2.19, BF Gain: -13.01, BFtrue Gain pred: -5.59, BFtrue Gain: -13.49,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2052\n",
            "Tr: 10.00,Beam: 17, Iteration: 1027, Q value: 0.2665, Reward: -1.0000, BF Gain pred: -2.75, BF Gain: -15.24, BFtrue Gain pred: -2.95, BFtrue Gain: -13.83,Critic Loss: 0.32, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2054\n",
            "Tr: 10.00,Beam: 17, Iteration: 1028, Q value: 0.2257, Reward: 1.0000, BF Gain pred: -0.80, BF Gain: -9.62, BFtrue Gain pred: -3.49, BFtrue Gain: -8.75,Critic Loss: 0.34, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2056\n",
            "Tr: 10.00,Beam: 17, Iteration: 1029, Q value: 0.1748, Reward: -1.0000, BF Gain pred: -1.63, BF Gain: -9.99, BFtrue Gain pred: -3.88, BFtrue Gain: -8.53,Critic Loss: 0.27, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2058\n",
            "Tr: 10.00,Beam: 17, Iteration: 1030, Q value: 0.1831, Reward: -1.0000, BF Gain pred: -1.63, BF Gain: -11.81, BFtrue Gain pred: -2.69, BFtrue Gain: -11.07,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2060\n",
            "Tr: 10.00,Beam: 17, Iteration: 1031, Q value: 0.1378, Reward: -1.0000, BF Gain pred: -1.63, BF Gain: -10.88, BFtrue Gain pred: -0.25, BFtrue Gain: -10.56,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2062\n",
            "Tr: 10.00,Beam: 17, Iteration: 1032, Q value: -0.0209, Reward: -1.0000, BF Gain pred: -1.63, BF Gain: -16.94, BFtrue Gain pred: -1.42, BFtrue Gain: -17.51,Critic Loss: 0.30, Policy Loss: -0.41\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2064\n",
            "Tr: 10.00,Beam: 17, Iteration: 1033, Q value: -0.4954, Reward: -1.0000, BF Gain pred: -1.63, BF Gain: -17.68, BFtrue Gain pred: -3.30, BFtrue Gain: -17.14,Critic Loss: 0.31, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2066\n",
            "Tr: 10.00,Beam: 17, Iteration: 1034, Q value: -1.1946, Reward: -1.0000, BF Gain pred: -5.59, BF Gain: -16.77, BFtrue Gain pred: -5.85, BFtrue Gain: -16.05,Critic Loss: 0.29, Policy Loss: -0.41\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2068\n",
            "Tr: 10.00,Beam: 17, Iteration: 1035, Q value: -1.2531, Reward: -1.0000, BF Gain pred: -5.59, BF Gain: -8.81, BFtrue Gain pred: -13.62, BFtrue Gain: -8.84,Critic Loss: 0.33, Policy Loss: -0.41\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2070\n",
            "Tr: 10.00,Beam: 17, Iteration: 1036, Q value: -1.2263, Reward: -1.0000, BF Gain pred: -8.20, BF Gain: -25.91, BFtrue Gain pred: -15.85, BFtrue Gain: -30.83,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2072\n",
            "Tr: 10.00,Beam: 17, Iteration: 1037, Q value: -0.7927, Reward: 1.0000, BF Gain pred: -5.67, BF Gain: -16.44, BFtrue Gain pred: -4.86, BFtrue Gain: -17.05,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2074\n",
            "Tr: 10.00,Beam: 17, Iteration: 1038, Q value: -0.5421, Reward: 1.0000, BF Gain pred: -3.57, BF Gain: -9.60, BFtrue Gain pred: -2.78, BFtrue Gain: -10.41,Critic Loss: 0.32, Policy Loss: -0.42\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2076\n",
            "Tr: 10.00,Beam: 17, Iteration: 1039, Q value: -0.6197, Reward: 1.0000, BF Gain pred: -1.15, BF Gain: -11.32, BFtrue Gain pred: -3.41, BFtrue Gain: -12.06,Critic Loss: 0.27, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2078\n",
            "Tr: 10.00,Beam: 17, Iteration: 1040, Q value: -0.4522, Reward: -1.0000, BF Gain pred: -1.85, BF Gain: -3.23, BFtrue Gain pred: -3.43, BFtrue Gain: -3.32,Critic Loss: 0.32, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2080\n",
            "Tr: 10.00,Beam: 17, Iteration: 1041, Q value: 0.2656, Reward: 1.0000, BF Gain pred: 0.14, BF Gain: -6.24, BFtrue Gain pred: -0.77, BFtrue Gain: -7.09,Critic Loss: 0.29, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2082\n",
            "Tr: 10.00,Beam: 17, Iteration: 1042, Q value: 0.1944, Reward: 1.0000, BF Gain pred: 1.03, BF Gain: -17.07, BFtrue Gain pred: 0.81, BFtrue Gain: -16.10,Critic Loss: 0.27, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2084\n",
            "Tr: 10.00,Beam: 17, Iteration: 1043, Q value: 0.2104, Reward: -1.0000, BF Gain pred: -5.57, BF Gain: -3.90, BFtrue Gain pred: -5.28, BFtrue Gain: -3.65,Critic Loss: 0.30, Policy Loss: -0.42\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2086\n",
            "Tr: 10.00,Beam: 17, Iteration: 1044, Q value: 0.1196, Reward: -1.0000, BF Gain pred: -6.25, BF Gain: -9.74, BFtrue Gain pred: -7.28, BFtrue Gain: -9.85,Critic Loss: 0.28, Policy Loss: -0.42\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2088\n",
            "Tr: 10.00,Beam: 17, Iteration: 1045, Q value: 0.2772, Reward: 1.0000, BF Gain pred: -3.30, BF Gain: -6.89, BFtrue Gain pred: -4.40, BFtrue Gain: -6.70,Critic Loss: 0.30, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2090\n",
            "Tr: 10.00,Beam: 17, Iteration: 1046, Q value: 0.1150, Reward: 1.0000, BF Gain pred: -3.21, BF Gain: -7.07, BFtrue Gain pred: 0.35, BFtrue Gain: -7.34,Critic Loss: 0.28, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2092\n",
            "Tr: 10.00,Beam: 17, Iteration: 1047, Q value: -0.1165, Reward: -1.0000, BF Gain pred: -7.02, BF Gain: -5.56, BFtrue Gain pred: -0.03, BFtrue Gain: -5.63,Critic Loss: 0.29, Policy Loss: -0.43\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2094\n",
            "Tr: 10.00,Beam: 17, Iteration: 1048, Q value: -0.0974, Reward: 1.0000, BF Gain pred: -5.55, BF Gain: -1.86, BFtrue Gain pred: 3.31, BFtrue Gain: -1.21,Critic Loss: 0.30, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2096\n",
            "Tr: 10.00,Beam: 17, Iteration: 1049, Q value: -0.2878, Reward: 1.0000, BF Gain pred: -5.39, BF Gain: -20.29, BFtrue Gain pred: 0.65, BFtrue Gain: -20.61,Critic Loss: 0.28, Policy Loss: -0.43\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2098\n",
            "Tr: 10.00,Beam: 17, Iteration: 1050, Q value: -0.1737, Reward: -1.0000, BF Gain pred: -7.35, BF Gain: -7.66, BFtrue Gain pred: -1.25, BFtrue Gain: -7.30,Critic Loss: 0.29, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2100\n",
            "Tr: 10.00,Beam: 17, Iteration: 1051, Q value: -0.1134, Reward: -1.0000, BF Gain pred: -9.29, BF Gain: -20.87, BFtrue Gain pred: -1.10, BFtrue Gain: -19.73,Critic Loss: 0.29, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2102\n",
            "Tr: 10.00,Beam: 17, Iteration: 1052, Q value: -0.0488, Reward: -1.0000, BF Gain pred: -10.03, BF Gain: -11.27, BFtrue Gain pred: -1.95, BFtrue Gain: -11.10,Critic Loss: 0.26, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2104\n",
            "Tr: 10.00,Beam: 17, Iteration: 1053, Q value: 0.1262, Reward: 1.0000, BF Gain pred: -9.29, BF Gain: -12.57, BFtrue Gain pred: -3.38, BFtrue Gain: -11.87,Critic Loss: 0.30, Policy Loss: -0.43\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2106\n",
            "Tr: 10.00,Beam: 17, Iteration: 1054, Q value: 0.0432, Reward: -1.0000, BF Gain pred: -12.58, BF Gain: -9.07, BFtrue Gain pred: -5.87, BFtrue Gain: -8.08,Critic Loss: 0.26, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2108\n",
            "Tr: 10.00,Beam: 17, Iteration: 1055, Q value: 0.1752, Reward: 1.0000, BF Gain pred: -10.71, BF Gain: -10.12, BFtrue Gain pred: -10.01, BFtrue Gain: -9.56,Critic Loss: 0.30, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2110\n",
            "Tr: 10.00,Beam: 17, Iteration: 1056, Q value: -0.0301, Reward: -1.0000, BF Gain pred: -10.92, BF Gain: -5.76, BFtrue Gain pred: -10.07, BFtrue Gain: -4.94,Critic Loss: 0.31, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2112\n",
            "Tr: 10.00,Beam: 17, Iteration: 1057, Q value: -0.0675, Reward: 1.0000, BF Gain pred: -8.91, BF Gain: -9.38, BFtrue Gain pred: -10.09, BFtrue Gain: -9.05,Critic Loss: 0.31, Policy Loss: -0.43\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2114\n",
            "Tr: 10.00,Beam: 17, Iteration: 1058, Q value: -0.1437, Reward: 1.0000, BF Gain pred: -7.69, BF Gain: -7.96, BFtrue Gain pred: -7.61, BFtrue Gain: -7.84,Critic Loss: 0.30, Policy Loss: -0.44\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2116\n",
            "Tr: 10.00,Beam: 17, Iteration: 1059, Q value: 0.0558, Reward: 1.0000, BF Gain pred: -5.99, BF Gain: -20.40, BFtrue Gain pred: -5.88, BFtrue Gain: -21.39,Critic Loss: 0.31, Policy Loss: -0.44\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2118\n",
            "Tr: 10.00,Beam: 17, Iteration: 1060, Q value: 0.3316, Reward: -1.0000, BF Gain pred: -6.02, BF Gain: -4.90, BFtrue Gain pred: -7.79, BFtrue Gain: -5.89,Critic Loss: 0.30, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2120\n",
            "Tr: 10.00,Beam: 17, Iteration: 1061, Q value: 0.4347, Reward: 1.0000, BF Gain pred: -4.66, BF Gain: -3.71, BFtrue Gain pred: -5.94, BFtrue Gain: -4.01,Critic Loss: 0.34, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2122\n",
            "Tr: 10.00,Beam: 17, Iteration: 1062, Q value: 0.3378, Reward: -1.0000, BF Gain pred: -4.66, BF Gain: -5.66, BFtrue Gain pred: -6.72, BFtrue Gain: -4.89,Critic Loss: 0.31, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2124\n",
            "Tr: 10.00,Beam: 17, Iteration: 1063, Q value: 0.2916, Reward: -1.0000, BF Gain pred: -4.66, BF Gain: 4.60, BFtrue Gain pred: -6.18, BFtrue Gain: 2.95,Critic Loss: 0.31, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2126\n",
            "Tr: 10.00,Beam: 17, Iteration: 1064, Q value: 0.1709, Reward: 1.0000, BF Gain pred: -4.04, BF Gain: -5.28, BFtrue Gain pred: -4.53, BFtrue Gain: -5.74,Critic Loss: 0.31, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2128\n",
            "Tr: 10.00,Beam: 17, Iteration: 1065, Q value: 0.0431, Reward: 1.0000, BF Gain pred: -3.92, BF Gain: -8.07, BFtrue Gain pred: -3.02, BFtrue Gain: -8.00,Critic Loss: 0.28, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2130\n",
            "Tr: 10.00,Beam: 17, Iteration: 1066, Q value: 0.1777, Reward: -1.0000, BF Gain pred: -8.77, BF Gain: -9.72, BFtrue Gain pred: -3.17, BFtrue Gain: -10.55,Critic Loss: 0.30, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2132\n",
            "Tr: 10.00,Beam: 17, Iteration: 1067, Q value: 0.1605, Reward: 1.0000, BF Gain pred: -5.21, BF Gain: -3.07, BFtrue Gain pred: -0.85, BFtrue Gain: -2.69,Critic Loss: 0.33, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2134\n",
            "Tr: 10.00,Beam: 17, Iteration: 1068, Q value: 0.4241, Reward: 1.0000, BF Gain pred: -0.26, BF Gain: -6.15, BFtrue Gain pred: -7.68, BFtrue Gain: -6.46,Critic Loss: 0.31, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2136\n",
            "Tr: 10.00,Beam: 17, Iteration: 1069, Q value: 0.2745, Reward: -1.0000, BF Gain pred: -1.04, BF Gain: -5.99, BFtrue Gain pred: -7.78, BFtrue Gain: -6.24,Critic Loss: 0.33, Policy Loss: -0.44\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2138\n",
            "Tr: 10.00,Beam: 17, Iteration: 1070, Q value: 0.2455, Reward: -1.0000, BF Gain pred: -3.13, BF Gain: -21.31, BFtrue Gain pred: -15.11, BFtrue Gain: -19.63,Critic Loss: 0.31, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2140\n",
            "Tr: 10.00,Beam: 17, Iteration: 1071, Q value: 0.1383, Reward: -1.0000, BF Gain pred: -8.05, BF Gain: -8.94, BFtrue Gain pred: -13.18, BFtrue Gain: -9.06,Critic Loss: 0.37, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2142\n",
            "Tr: 10.00,Beam: 17, Iteration: 1072, Q value: -0.0615, Reward: 1.0000, BF Gain pred: -5.22, BF Gain: -6.02, BFtrue Gain pred: -9.02, BFtrue Gain: -6.62,Critic Loss: 0.31, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2144\n",
            "Tr: 10.00,Beam: 17, Iteration: 1073, Q value: -0.7572, Reward: -1.0000, BF Gain pred: -7.58, BF Gain: -7.25, BFtrue Gain pred: -9.32, BFtrue Gain: -7.39,Critic Loss: 0.30, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2146\n",
            "Tr: 10.00,Beam: 17, Iteration: 1074, Q value: -0.5640, Reward: 1.0000, BF Gain pred: -2.26, BF Gain: -6.07, BFtrue Gain pred: -11.23, BFtrue Gain: -5.81,Critic Loss: 0.31, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2148\n",
            "Tr: 10.00,Beam: 17, Iteration: 1075, Q value: -1.7276, Reward: -1.0000, BF Gain pred: -2.64, BF Gain: -1.85, BFtrue Gain pred: -12.30, BFtrue Gain: -1.36,Critic Loss: 0.32, Policy Loss: -0.43\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2150\n",
            "Tr: 10.00,Beam: 17, Iteration: 1076, Q value: -1.2066, Reward: 1.0000, BF Gain pred: -2.24, BF Gain: 1.73, BFtrue Gain pred: -8.71, BFtrue Gain: 2.05,Critic Loss: 0.33, Policy Loss: -0.43\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2152\n",
            "Tr: 10.00,Beam: 17, Iteration: 1077, Q value: -1.4987, Reward: -1.0000, BF Gain pred: -4.49, BF Gain: 6.84, BFtrue Gain pred: -4.68, BFtrue Gain: 6.54,Critic Loss: 0.31, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2154\n",
            "Tr: 10.00,Beam: 17, Iteration: 1078, Q value: -1.9274, Reward: -1.0000, BF Gain pred: -5.65, BF Gain: 1.37, BFtrue Gain pred: -1.22, BFtrue Gain: 1.31,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2156\n",
            "Tr: 10.00,Beam: 17, Iteration: 1079, Q value: -0.3609, Reward: 1.0000, BF Gain pred: -4.26, BF Gain: 0.75, BFtrue Gain pred: 1.07, BFtrue Gain: 1.06,Critic Loss: 0.32, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2158\n",
            "Tr: 10.00,Beam: 17, Iteration: 1080, Q value: -2.0005, Reward: -1.0000, BF Gain pred: -6.04, BF Gain: -5.00, BFtrue Gain pred: -2.21, BFtrue Gain: -4.61,Critic Loss: 0.32, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2160\n",
            "Tr: 10.00,Beam: 17, Iteration: 1081, Q value: -0.6164, Reward: 1.0000, BF Gain pred: -4.88, BF Gain: -3.74, BFtrue Gain pred: -0.11, BFtrue Gain: -3.45,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2162\n",
            "Tr: 10.00,Beam: 17, Iteration: 1082, Q value: -0.7762, Reward: 1.0000, BF Gain pred: -4.14, BF Gain: 4.08, BFtrue Gain pred: -3.78, BFtrue Gain: 6.24,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2164\n",
            "Tr: 10.00,Beam: 17, Iteration: 1083, Q value: -0.2353, Reward: 1.0000, BF Gain pred: -1.45, BF Gain: -4.11, BFtrue Gain pred: -0.02, BFtrue Gain: -3.65,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2166\n",
            "Tr: 10.00,Beam: 17, Iteration: 1084, Q value: -0.1810, Reward: -1.0000, BF Gain pred: -2.89, BF Gain: -8.14, BFtrue Gain pred: -7.21, BFtrue Gain: -8.20,Critic Loss: 0.33, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2168\n",
            "Tr: 10.00,Beam: 17, Iteration: 1085, Q value: -0.3825, Reward: -1.0000, BF Gain pred: -4.01, BF Gain: -3.59, BFtrue Gain pred: -3.91, BFtrue Gain: -2.81,Critic Loss: 0.33, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2170\n",
            "Tr: 10.00,Beam: 17, Iteration: 1086, Q value: 0.2293, Reward: 1.0000, BF Gain pred: -3.00, BF Gain: -7.45, BFtrue Gain pred: -7.69, BFtrue Gain: -7.98,Critic Loss: 0.35, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2172\n",
            "Tr: 10.00,Beam: 17, Iteration: 1087, Q value: 0.1666, Reward: 1.0000, BF Gain pred: 0.60, BF Gain: -4.89, BFtrue Gain pred: -12.83, BFtrue Gain: -6.22,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2174\n",
            "Tr: 10.00,Beam: 17, Iteration: 1088, Q value: 0.2131, Reward: -1.0000, BF Gain pred: -12.37, BF Gain: -5.11, BFtrue Gain pred: -8.65, BFtrue Gain: -5.05,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2176\n",
            "Tr: 10.00,Beam: 17, Iteration: 1089, Q value: 0.2590, Reward: 1.0000, BF Gain pred: 3.08, BF Gain: -1.75, BFtrue Gain pred: -17.57, BFtrue Gain: -2.26,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2178\n",
            "Tr: 10.00,Beam: 17, Iteration: 1090, Q value: 0.2006, Reward: -1.0000, BF Gain pred: 3.08, BF Gain: -10.56, BFtrue Gain pred: -10.93, BFtrue Gain: -11.95,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2180\n",
            "Tr: 10.00,Beam: 17, Iteration: 1091, Q value: 0.2566, Reward: -1.0000, BF Gain pred: 3.08, BF Gain: -9.16, BFtrue Gain pred: -15.10, BFtrue Gain: -9.65,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2182\n",
            "Tr: 10.00,Beam: 17, Iteration: 1092, Q value: 0.2968, Reward: -1.0000, BF Gain pred: 3.08, BF Gain: -3.72, BFtrue Gain pred: -18.70, BFtrue Gain: -2.26,Critic Loss: 0.31, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2184\n",
            "Tr: 10.00,Beam: 17, Iteration: 1093, Q value: 0.3640, Reward: -1.0000, BF Gain pred: -0.02, BF Gain: -8.03, BFtrue Gain pred: -19.56, BFtrue Gain: -7.23,Critic Loss: 0.38, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2186\n",
            "Tr: 10.00,Beam: 17, Iteration: 1094, Q value: 0.2292, Reward: -1.0000, BF Gain pred: -3.25, BF Gain: 0.26, BFtrue Gain pred: -18.67, BFtrue Gain: 1.78,Critic Loss: 0.32, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2188\n",
            "Tr: 10.00,Beam: 17, Iteration: 1095, Q value: -0.0302, Reward: -1.0000, BF Gain pred: -3.25, BF Gain: -4.38, BFtrue Gain pred: -18.35, BFtrue Gain: -3.54,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2190\n",
            "Tr: 10.00,Beam: 17, Iteration: 1096, Q value: -0.6839, Reward: -1.0000, BF Gain pred: -3.25, BF Gain: -4.69, BFtrue Gain pred: -20.11, BFtrue Gain: -4.48,Critic Loss: 0.29, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2192\n",
            "Tr: 10.00,Beam: 17, Iteration: 1097, Q value: -1.2594, Reward: 1.0000, BF Gain pred: 1.56, BF Gain: -6.03, BFtrue Gain pred: -8.16, BFtrue Gain: -6.83,Critic Loss: 0.30, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2194\n",
            "Tr: 10.00,Beam: 17, Iteration: 1098, Q value: -1.1159, Reward: -1.0000, BF Gain pred: -2.87, BF Gain: -7.95, BFtrue Gain pred: 1.40, BFtrue Gain: -8.19,Critic Loss: 0.32, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2196\n",
            "Tr: 10.00,Beam: 17, Iteration: 1099, Q value: -0.6362, Reward: -1.0000, BF Gain pred: -3.80, BF Gain: -12.51, BFtrue Gain pred: -3.35, BFtrue Gain: -13.04,Critic Loss: 0.32, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2198\n",
            "Tr: 10.00,Beam: 17, Iteration: 1100, Q value: -0.3698, Reward: 1.0000, BF Gain pred: 0.58, BF Gain: -8.34, BFtrue Gain pred: 0.79, BFtrue Gain: -8.14,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2200\n",
            "Tr: 10.00,Beam: 17, Iteration: 1101, Q value: -0.1361, Reward: -1.0000, BF Gain pred: 0.22, BF Gain: -9.52, BFtrue Gain pred: 4.69, BFtrue Gain: -9.23,Critic Loss: 0.31, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2202\n",
            "Tr: 10.00,Beam: 17, Iteration: 1102, Q value: 0.2019, Reward: -1.0000, BF Gain pred: -0.56, BF Gain: -18.15, BFtrue Gain pred: 3.86, BFtrue Gain: -18.17,Critic Loss: 0.30, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2204\n",
            "Tr: 10.00,Beam: 17, Iteration: 1103, Q value: 0.0829, Reward: -1.0000, BF Gain pred: -3.45, BF Gain: -15.86, BFtrue Gain pred: 1.36, BFtrue Gain: -15.63,Critic Loss: 0.32, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2206\n",
            "Tr: 10.00,Beam: 17, Iteration: 1104, Q value: 0.1465, Reward: -1.0000, BF Gain pred: -4.03, BF Gain: -16.55, BFtrue Gain pred: -1.69, BFtrue Gain: -17.89,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2208\n",
            "Tr: 10.00,Beam: 17, Iteration: 1105, Q value: 0.1166, Reward: 1.0000, BF Gain pred: -0.79, BF Gain: -8.73, BFtrue Gain pred: -0.13, BFtrue Gain: -9.09,Critic Loss: 0.34, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2210\n",
            "Tr: 10.00,Beam: 17, Iteration: 1106, Q value: 0.0220, Reward: -1.0000, BF Gain pred: -2.14, BF Gain: -6.48, BFtrue Gain pred: -2.77, BFtrue Gain: -7.34,Critic Loss: 0.35, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2212\n",
            "Tr: 10.00,Beam: 17, Iteration: 1107, Q value: 0.0050, Reward: 1.0000, BF Gain pred: -0.16, BF Gain: -9.43, BFtrue Gain pred: 2.66, BFtrue Gain: -8.70,Critic Loss: 0.30, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2214\n",
            "Tr: 10.00,Beam: 17, Iteration: 1108, Q value: -0.0583, Reward: 1.0000, BF Gain pred: 0.40, BF Gain: -3.13, BFtrue Gain pred: -1.08, BFtrue Gain: -3.31,Critic Loss: 0.32, Policy Loss: -0.45\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2216\n",
            "Tr: 10.00,Beam: 17, Iteration: 1109, Q value: 0.1157, Reward: -1.0000, BF Gain pred: -0.17, BF Gain: -8.72, BFtrue Gain pred: -1.36, BFtrue Gain: -8.69,Critic Loss: 0.30, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2218\n",
            "Tr: 10.00,Beam: 17, Iteration: 1110, Q value: 0.1033, Reward: -1.0000, BF Gain pred: -0.27, BF Gain: -6.23, BFtrue Gain pred: -4.24, BFtrue Gain: -7.02,Critic Loss: 0.29, Policy Loss: -0.44\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2220\n",
            "Tr: 10.00,Beam: 17, Iteration: 1111, Q value: 0.3451, Reward: 1.0000, BF Gain pred: 0.58, BF Gain: 3.16, BFtrue Gain pred: -4.58, BFtrue Gain: 2.08,Critic Loss: 0.34, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2222\n",
            "Tr: 10.00,Beam: 17, Iteration: 1112, Q value: 0.3714, Reward: -1.0000, BF Gain pred: -0.17, BF Gain: -4.70, BFtrue Gain pred: -5.25, BFtrue Gain: -4.91,Critic Loss: 0.34, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2224\n",
            "Tr: 10.00,Beam: 17, Iteration: 1113, Q value: 0.4173, Reward: -1.0000, BF Gain pred: -3.08, BF Gain: -0.33, BFtrue Gain pred: -4.72, BFtrue Gain: -0.03,Critic Loss: 0.30, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2226\n",
            "Tr: 10.00,Beam: 17, Iteration: 1114, Q value: 0.4499, Reward: -1.0000, BF Gain pred: -3.36, BF Gain: -3.43, BFtrue Gain pred: -2.59, BFtrue Gain: -2.36,Critic Loss: 0.32, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2228\n",
            "Tr: 10.00,Beam: 17, Iteration: 1115, Q value: 0.3001, Reward: 1.0000, BF Gain pred: -2.57, BF Gain: -3.61, BFtrue Gain pred: -4.10, BFtrue Gain: -3.93,Critic Loss: 0.29, Policy Loss: -0.47\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2230\n",
            "Tr: 10.00,Beam: 17, Iteration: 1116, Q value: 0.3648, Reward: -1.0000, BF Gain pred: -6.96, BF Gain: -9.29, BFtrue Gain pred: -2.89, BFtrue Gain: -9.54,Critic Loss: 0.30, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2232\n",
            "Tr: 10.00,Beam: 17, Iteration: 1117, Q value: 0.3018, Reward: 1.0000, BF Gain pred: -1.45, BF Gain: -1.84, BFtrue Gain pred: -0.48, BFtrue Gain: -1.63,Critic Loss: 0.31, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2234\n",
            "Tr: 10.00,Beam: 17, Iteration: 1118, Q value: 0.2939, Reward: -1.0000, BF Gain pred: -5.39, BF Gain: 1.07, BFtrue Gain pred: -2.99, BFtrue Gain: 1.21,Critic Loss: 0.31, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2236\n",
            "Tr: 10.00,Beam: 17, Iteration: 1119, Q value: -0.0596, Reward: 1.0000, BF Gain pred: -1.95, BF Gain: -8.66, BFtrue Gain pred: -3.75, BFtrue Gain: -8.51,Critic Loss: 0.31, Policy Loss: -0.45\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2238\n",
            "Tr: 10.00,Beam: 17, Iteration: 1120, Q value: 0.0472, Reward: -1.0000, BF Gain pred: -6.75, BF Gain: -8.30, BFtrue Gain pred: -12.65, BFtrue Gain: -7.51,Critic Loss: 0.34, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2240\n",
            "Tr: 10.00,Beam: 17, Iteration: 1121, Q value: 0.3414, Reward: -1.0000, BF Gain pred: -7.22, BF Gain: -3.53, BFtrue Gain pred: -2.54, BFtrue Gain: -3.30,Critic Loss: 0.27, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2242\n",
            "Tr: 10.00,Beam: 17, Iteration: 1122, Q value: 0.3040, Reward: 1.0000, BF Gain pred: 0.54, BF Gain: -3.23, BFtrue Gain pred: -0.13, BFtrue Gain: -3.44,Critic Loss: 0.32, Policy Loss: -0.46\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2244\n",
            "Tr: 10.00,Beam: 17, Iteration: 1123, Q value: 0.4118, Reward: -1.0000, BF Gain pred: 0.04, BF Gain: -6.08, BFtrue Gain pred: -2.39, BFtrue Gain: -7.68,Critic Loss: 0.33, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2246\n",
            "Tr: 10.00,Beam: 17, Iteration: 1124, Q value: 0.2552, Reward: 1.0000, BF Gain pred: 2.28, BF Gain: -5.11, BFtrue Gain pred: -0.17, BFtrue Gain: -5.31,Critic Loss: 0.29, Policy Loss: -0.46\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2248\n",
            "Tr: 10.00,Beam: 17, Iteration: 1125, Q value: 0.4707, Reward: -1.0000, BF Gain pred: -5.33, BF Gain: -1.88, BFtrue Gain pred: -7.10, BFtrue Gain: -1.60,Critic Loss: 0.30, Policy Loss: -0.47\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2250\n",
            "Tr: 10.00,Beam: 17, Iteration: 1126, Q value: 0.2841, Reward: 1.0000, BF Gain pred: 0.78, BF Gain: -11.80, BFtrue Gain pred: 2.52, BFtrue Gain: -11.01,Critic Loss: 0.27, Policy Loss: -0.47\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2252\n",
            "Tr: 10.00,Beam: 17, Iteration: 1127, Q value: 0.0990, Reward: -1.0000, BF Gain pred: -0.92, BF Gain: -5.86, BFtrue Gain pred: -1.05, BFtrue Gain: -5.87,Critic Loss: 0.28, Policy Loss: -0.47\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2254\n",
            "Tr: 10.00,Beam: 17, Iteration: 1128, Q value: 0.4719, Reward: -1.0000, BF Gain pred: -6.18, BF Gain: 0.43, BFtrue Gain pred: -3.05, BFtrue Gain: 2.22,Critic Loss: 0.29, Policy Loss: -0.47\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2256\n",
            "Tr: 10.00,Beam: 17, Iteration: 1129, Q value: 0.4592, Reward: 1.0000, BF Gain pred: -0.85, BF Gain: -3.87, BFtrue Gain pred: 5.54, BFtrue Gain: -3.66,Critic Loss: 0.32, Policy Loss: -0.47\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2258\n",
            "Tr: 10.00,Beam: 17, Iteration: 1130, Q value: -0.3282, Reward: -1.0000, BF Gain pred: -5.97, BF Gain: -4.42, BFtrue Gain pred: -2.91, BFtrue Gain: -5.11,Critic Loss: 0.30, Policy Loss: -0.47\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2260\n",
            "Tr: 10.00,Beam: 17, Iteration: 1131, Q value: 0.4733, Reward: -1.0000, BF Gain pred: -7.52, BF Gain: 3.13, BFtrue Gain pred: -2.24, BFtrue Gain: 3.81,Critic Loss: 0.33, Policy Loss: -0.47\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2262\n",
            "Tr: 10.00,Beam: 17, Iteration: 1132, Q value: 0.4738, Reward: 1.0000, BF Gain pred: 0.65, BF Gain: -8.23, BFtrue Gain pred: 4.01, BFtrue Gain: -8.72,Critic Loss: 0.30, Policy Loss: -0.47\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2264\n",
            "Tr: 10.00,Beam: 17, Iteration: 1133, Q value: 0.4743, Reward: -1.0000, BF Gain pred: -5.66, BF Gain: -11.26, BFtrue Gain pred: 0.16, BFtrue Gain: -12.31,Critic Loss: 0.31, Policy Loss: -0.47\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2266\n",
            "Tr: 10.00,Beam: 17, Iteration: 1134, Q value: 0.4748, Reward: 1.0000, BF Gain pred: -4.22, BF Gain: -14.09, BFtrue Gain pred: -2.35, BFtrue Gain: -14.93,Critic Loss: 0.28, Policy Loss: -0.47\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2268\n",
            "Tr: 10.00,Beam: 17, Iteration: 1135, Q value: 0.4754, Reward: 1.0000, BF Gain pred: -3.92, BF Gain: -11.99, BFtrue Gain pred: 5.44, BFtrue Gain: -8.92,Critic Loss: 0.36, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2270\n",
            "Tr: 10.00,Beam: 17, Iteration: 1136, Q value: 0.3487, Reward: -1.0000, BF Gain pred: -5.90, BF Gain: -16.28, BFtrue Gain pred: -3.24, BFtrue Gain: -16.95,Critic Loss: 0.27, Policy Loss: -0.48\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2272\n",
            "Tr: 10.00,Beam: 17, Iteration: 1137, Q value: 0.4765, Reward: -1.0000, BF Gain pred: -9.87, BF Gain: -14.53, BFtrue Gain pred: -6.17, BFtrue Gain: -15.41,Critic Loss: 0.34, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2274\n",
            "Tr: 10.00,Beam: 17, Iteration: 1138, Q value: 0.4405, Reward: 1.0000, BF Gain pred: -8.99, BF Gain: -15.65, BFtrue Gain pred: -8.04, BFtrue Gain: -14.94,Critic Loss: 0.32, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2276\n",
            "Tr: 10.00,Beam: 17, Iteration: 1139, Q value: 0.4776, Reward: 1.0000, BF Gain pred: -8.27, BF Gain: -13.93, BFtrue Gain pred: -6.68, BFtrue Gain: -13.19,Critic Loss: 0.32, Policy Loss: -0.48\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2278\n",
            "Tr: 10.00,Beam: 17, Iteration: 1140, Q value: -0.1052, Reward: -1.0000, BF Gain pred: -10.10, BF Gain: -7.19, BFtrue Gain pred: -11.49, BFtrue Gain: -5.92,Critic Loss: 0.34, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2280\n",
            "Tr: 10.00,Beam: 17, Iteration: 1141, Q value: 0.4786, Reward: 1.0000, BF Gain pred: -8.99, BF Gain: -6.50, BFtrue Gain pred: -13.43, BFtrue Gain: -5.47,Critic Loss: 0.34, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2282\n",
            "Tr: 10.00,Beam: 17, Iteration: 1142, Q value: 0.4720, Reward: 1.0000, BF Gain pred: -8.34, BF Gain: -4.31, BFtrue Gain pred: -9.61, BFtrue Gain: -3.96,Critic Loss: 0.31, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2284\n",
            "Tr: 10.00,Beam: 17, Iteration: 1143, Q value: 0.3805, Reward: -1.0000, BF Gain pred: -11.71, BF Gain: 0.82, BFtrue Gain pred: -16.56, BFtrue Gain: 0.24,Critic Loss: 0.31, Policy Loss: -0.48\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2286\n",
            "Tr: 10.00,Beam: 17, Iteration: 1144, Q value: 0.4524, Reward: -1.0000, BF Gain pred: -15.19, BF Gain: -2.20, BFtrue Gain pred: -24.27, BFtrue Gain: -1.33,Critic Loss: 0.31, Policy Loss: -0.48\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2288\n",
            "Tr: 10.00,Beam: 17, Iteration: 1145, Q value: 0.4467, Reward: 1.0000, BF Gain pred: -11.71, BF Gain: 0.20, BFtrue Gain pred: -14.37, BFtrue Gain: 0.74,Critic Loss: 0.30, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2290\n",
            "Tr: 10.00,Beam: 17, Iteration: 1146, Q value: 0.3588, Reward: -1.0000, BF Gain pred: -17.82, BF Gain: -5.91, BFtrue Gain pred: -19.02, BFtrue Gain: -6.53,Critic Loss: 0.32, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2292\n",
            "Tr: 10.00,Beam: 17, Iteration: 1147, Q value: 0.4490, Reward: 1.0000, BF Gain pred: -14.55, BF Gain: -10.88, BFtrue Gain pred: -13.69, BFtrue Gain: -11.36,Critic Loss: 0.30, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2294\n",
            "Tr: 10.00,Beam: 17, Iteration: 1148, Q value: 0.4824, Reward: 1.0000, BF Gain pred: -8.86, BF Gain: -9.85, BFtrue Gain pred: -12.89, BFtrue Gain: -9.36,Critic Loss: 0.29, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2296\n",
            "Tr: 10.00,Beam: 17, Iteration: 1149, Q value: 0.4829, Reward: -1.0000, BF Gain pred: -18.48, BF Gain: -11.74, BFtrue Gain pred: -17.48, BFtrue Gain: -11.15,Critic Loss: 0.30, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2298\n",
            "Tr: 10.00,Beam: 17, Iteration: 1150, Q value: 0.4776, Reward: 1.0000, BF Gain pred: -14.80, BF Gain: -9.99, BFtrue Gain pred: -20.62, BFtrue Gain: -9.56,Critic Loss: 0.33, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2300\n",
            "Tr: 10.00,Beam: 17, Iteration: 1151, Q value: 0.4840, Reward: 1.0000, BF Gain pred: -9.42, BF Gain: -6.12, BFtrue Gain pred: -12.48, BFtrue Gain: -6.03,Critic Loss: 0.30, Policy Loss: -0.48\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2302\n",
            "Tr: 10.00,Beam: 17, Iteration: 1152, Q value: 0.4846, Reward: -1.0000, BF Gain pred: -18.27, BF Gain: -2.74, BFtrue Gain pred: -19.92, BFtrue Gain: -3.98,Critic Loss: 0.32, Policy Loss: -0.49\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2304\n",
            "Tr: 10.00,Beam: 17, Iteration: 1153, Q value: 0.2795, Reward: 1.0000, BF Gain pred: -9.28, BF Gain: -3.41, BFtrue Gain pred: -13.01, BFtrue Gain: -2.92,Critic Loss: 0.30, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2306\n",
            "Tr: 10.00,Beam: 17, Iteration: 1154, Q value: 0.4857, Reward: -1.0000, BF Gain pred: -12.38, BF Gain: -4.15, BFtrue Gain pred: -15.21, BFtrue Gain: -4.41,Critic Loss: 0.31, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2308\n",
            "Tr: 10.00,Beam: 17, Iteration: 1155, Q value: 0.4862, Reward: -1.0000, BF Gain pred: -19.36, BF Gain: -2.74, BFtrue Gain pred: -17.12, BFtrue Gain: -1.90,Critic Loss: 0.33, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2310\n",
            "Tr: 10.00,Beam: 17, Iteration: 1156, Q value: 0.4867, Reward: 1.0000, BF Gain pred: -10.87, BF Gain: -3.34, BFtrue Gain pred: -13.72, BFtrue Gain: -2.17,Critic Loss: 0.28, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2312\n",
            "Tr: 10.00,Beam: 17, Iteration: 1157, Q value: 0.4871, Reward: 1.0000, BF Gain pred: -8.98, BF Gain: -2.68, BFtrue Gain pred: -12.90, BFtrue Gain: -1.89,Critic Loss: 0.30, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2314\n",
            "Tr: 10.00,Beam: 17, Iteration: 1158, Q value: 0.4876, Reward: -1.0000, BF Gain pred: -19.11, BF Gain: -4.54, BFtrue Gain pred: -27.16, BFtrue Gain: -4.64,Critic Loss: 0.29, Policy Loss: -0.49\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2316\n",
            "Tr: 10.00,Beam: 17, Iteration: 1159, Q value: 0.4881, Reward: 1.0000, BF Gain pred: -9.05, BF Gain: -6.04, BFtrue Gain pred: -6.27, BFtrue Gain: -6.27,Critic Loss: 0.29, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2318\n",
            "Tr: 10.00,Beam: 17, Iteration: 1160, Q value: 0.4886, Reward: -1.0000, BF Gain pred: -14.05, BF Gain: -9.86, BFtrue Gain pred: -21.87, BFtrue Gain: -9.34,Critic Loss: 0.29, Policy Loss: -0.49\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2320\n",
            "Tr: 10.00,Beam: 17, Iteration: 1161, Q value: 0.4891, Reward: 1.0000, BF Gain pred: -10.54, BF Gain: -10.66, BFtrue Gain pred: -15.86, BFtrue Gain: -9.89,Critic Loss: 0.30, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2322\n",
            "Tr: 10.00,Beam: 17, Iteration: 1162, Q value: 0.4897, Reward: 1.0000, BF Gain pred: -10.28, BF Gain: -8.93, BFtrue Gain pred: -12.76, BFtrue Gain: -7.50,Critic Loss: 0.29, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2324\n",
            "Tr: 10.00,Beam: 17, Iteration: 1163, Q value: 0.4902, Reward: -1.0000, BF Gain pred: -14.43, BF Gain: -3.11, BFtrue Gain pred: -19.48, BFtrue Gain: -3.07,Critic Loss: 0.31, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2326\n",
            "Tr: 10.00,Beam: 17, Iteration: 1164, Q value: 0.4907, Reward: 1.0000, BF Gain pred: -10.54, BF Gain: -9.92, BFtrue Gain pred: -9.34, BFtrue Gain: -9.30,Critic Loss: 0.25, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2328\n",
            "Tr: 10.00,Beam: 17, Iteration: 1165, Q value: 0.4912, Reward: -1.0000, BF Gain pred: -14.50, BF Gain: 1.10, BFtrue Gain pred: -23.45, BFtrue Gain: 0.61,Critic Loss: 0.29, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2330\n",
            "Tr: 10.00,Beam: 17, Iteration: 1166, Q value: 0.4917, Reward: 1.0000, BF Gain pred: -8.27, BF Gain: -2.94, BFtrue Gain pred: -13.79, BFtrue Gain: -2.27,Critic Loss: 0.28, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2332\n",
            "Tr: 10.00,Beam: 17, Iteration: 1167, Q value: 0.4922, Reward: -1.0000, BF Gain pred: -8.82, BF Gain: -14.19, BFtrue Gain pred: -12.16, BFtrue Gain: -13.81,Critic Loss: 0.28, Policy Loss: -0.49\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2334\n",
            "Tr: 10.00,Beam: 17, Iteration: 1168, Q value: 0.4911, Reward: -1.0000, BF Gain pred: -14.43, BF Gain: -3.57, BFtrue Gain pred: -27.84, BFtrue Gain: -3.33,Critic Loss: 0.27, Policy Loss: -0.49\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2336\n",
            "Tr: 10.00,Beam: 17, Iteration: 1169, Q value: 0.4932, Reward: 1.0000, BF Gain pred: -10.54, BF Gain: -3.98, BFtrue Gain pred: -9.85, BFtrue Gain: -3.50,Critic Loss: 0.28, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2338\n",
            "Tr: 10.00,Beam: 17, Iteration: 1170, Q value: 0.4937, Reward: -1.0000, BF Gain pred: -12.94, BF Gain: 0.26, BFtrue Gain pred: -23.71, BFtrue Gain: 1.05,Critic Loss: 0.32, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2340\n",
            "Tr: 10.00,Beam: 17, Iteration: 1171, Q value: 0.4942, Reward: 1.0000, BF Gain pred: -9.49, BF Gain: 3.60, BFtrue Gain pred: -15.52, BFtrue Gain: 4.06,Critic Loss: 0.33, Policy Loss: -0.49\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2342\n",
            "Tr: 10.00,Beam: 17, Iteration: 1172, Q value: 0.4947, Reward: -1.0000, BF Gain pred: -10.62, BF Gain: -4.46, BFtrue Gain pred: -12.66, BFtrue Gain: -4.59,Critic Loss: 0.28, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2344\n",
            "Tr: 10.00,Beam: 17, Iteration: 1173, Q value: 0.4952, Reward: -1.0000, BF Gain pred: -14.84, BF Gain: -5.79, BFtrue Gain pred: -32.26, BFtrue Gain: -5.81,Critic Loss: 0.29, Policy Loss: -0.50\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2346\n",
            "Tr: 10.00,Beam: 17, Iteration: 1174, Q value: 0.4957, Reward: 1.0000, BF Gain pred: -10.91, BF Gain: -0.80, BFtrue Gain pred: -9.40, BFtrue Gain: 0.07,Critic Loss: 0.27, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2348\n",
            "Tr: 10.00,Beam: 17, Iteration: 1175, Q value: 0.4962, Reward: 1.0000, BF Gain pred: -10.65, BF Gain: -4.94, BFtrue Gain pred: -19.76, BFtrue Gain: -3.63,Critic Loss: 0.31, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2350\n",
            "Tr: 10.00,Beam: 17, Iteration: 1176, Q value: 0.4967, Reward: 1.0000, BF Gain pred: -9.06, BF Gain: -11.35, BFtrue Gain pred: -15.04, BFtrue Gain: -11.04,Critic Loss: 0.28, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2352\n",
            "Tr: 10.00,Beam: 17, Iteration: 1177, Q value: 0.4973, Reward: -1.0000, BF Gain pred: -12.08, BF Gain: -12.80, BFtrue Gain pred: -13.00, BFtrue Gain: -12.78,Critic Loss: 0.30, Policy Loss: -0.50\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2354\n",
            "Tr: 10.00,Beam: 17, Iteration: 1178, Q value: 0.4978, Reward: -1.0000, BF Gain pred: -12.68, BF Gain: -8.76, BFtrue Gain pred: -24.28, BFtrue Gain: -8.72,Critic Loss: 0.30, Policy Loss: -0.50\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2356\n",
            "Tr: 10.00,Beam: 17, Iteration: 1179, Q value: 0.4984, Reward: 1.0000, BF Gain pred: -10.02, BF Gain: -14.75, BFtrue Gain pred: -8.86, BFtrue Gain: -12.69,Critic Loss: 0.29, Policy Loss: -0.50\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2358\n",
            "Tr: 10.00,Beam: 17, Iteration: 1180, Q value: 0.4990, Reward: -1.0000, BF Gain pred: -10.65, BF Gain: -18.91, BFtrue Gain pred: -17.77, BFtrue Gain: -16.26,Critic Loss: 0.30, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2360\n",
            "Tr: 10.00,Beam: 17, Iteration: 1181, Q value: 0.4996, Reward: 1.0000, BF Gain pred: -10.62, BF Gain: -13.00, BFtrue Gain pred: -15.00, BFtrue Gain: -12.21,Critic Loss: 0.28, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2362\n",
            "Tr: 10.00,Beam: 17, Iteration: 1182, Q value: 0.5001, Reward: 1.0000, BF Gain pred: -9.67, BF Gain: -7.29, BFtrue Gain pred: -15.59, BFtrue Gain: -6.00,Critic Loss: 0.28, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2364\n",
            "Tr: 10.00,Beam: 17, Iteration: 1183, Q value: 0.5006, Reward: -1.0000, BF Gain pred: -10.53, BF Gain: -2.75, BFtrue Gain pred: -19.82, BFtrue Gain: -1.58,Critic Loss: 0.31, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2366\n",
            "Tr: 10.00,Beam: 17, Iteration: 1184, Q value: 0.5012, Reward: -1.0000, BF Gain pred: -14.12, BF Gain: -2.04, BFtrue Gain pred: -15.69, BFtrue Gain: -2.85,Critic Loss: 0.30, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2368\n",
            "Tr: 10.00,Beam: 17, Iteration: 1185, Q value: 0.5017, Reward: 1.0000, BF Gain pred: -11.93, BF Gain: -6.97, BFtrue Gain pred: -14.42, BFtrue Gain: -6.75,Critic Loss: 0.27, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2370\n",
            "Tr: 10.00,Beam: 17, Iteration: 1186, Q value: 0.5022, Reward: 1.0000, BF Gain pred: -8.08, BF Gain: -7.08, BFtrue Gain pred: -16.55, BFtrue Gain: -7.07,Critic Loss: 0.31, Policy Loss: -0.50\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2372\n",
            "Tr: 10.00,Beam: 17, Iteration: 1187, Q value: 0.5027, Reward: -1.0000, BF Gain pred: -15.63, BF Gain: -5.44, BFtrue Gain pred: -13.91, BFtrue Gain: -5.41,Critic Loss: 0.31, Policy Loss: -0.50\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2374\n",
            "Tr: 10.00,Beam: 17, Iteration: 1188, Q value: 0.5032, Reward: 1.0000, BF Gain pred: -8.46, BF Gain: -8.02, BFtrue Gain pred: -17.35, BFtrue Gain: -8.32,Critic Loss: 0.27, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2376\n",
            "Tr: 10.00,Beam: 17, Iteration: 1189, Q value: 0.5037, Reward: -1.0000, BF Gain pred: -10.62, BF Gain: -11.32, BFtrue Gain pred: -13.94, BFtrue Gain: -10.31,Critic Loss: 0.29, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2378\n",
            "Tr: 10.00,Beam: 17, Iteration: 1190, Q value: 0.5042, Reward: 1.0000, BF Gain pred: -9.67, BF Gain: 1.15, BFtrue Gain pred: -15.53, BFtrue Gain: 0.97,Critic Loss: 0.29, Policy Loss: -0.50\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2380\n",
            "Tr: 10.00,Beam: 17, Iteration: 1191, Q value: 0.5047, Reward: -1.0000, BF Gain pred: -12.99, BF Gain: -9.27, BFtrue Gain pred: -17.91, BFtrue Gain: -8.81,Critic Loss: 0.29, Policy Loss: -0.51\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2382\n",
            "Tr: 10.00,Beam: 17, Iteration: 1192, Q value: 0.5052, Reward: 1.0000, BF Gain pred: -11.32, BF Gain: -6.19, BFtrue Gain pred: -13.97, BFtrue Gain: -6.08,Critic Loss: 0.31, Policy Loss: -0.51\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2384\n",
            "Tr: 10.00,Beam: 17, Iteration: 1193, Q value: 0.5057, Reward: 1.0000, BF Gain pred: -10.98, BF Gain: -6.96, BFtrue Gain pred: -12.58, BFtrue Gain: -5.16,Critic Loss: 0.31, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2386\n",
            "Tr: 10.00,Beam: 17, Iteration: 1194, Q value: 0.5062, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -8.50, BFtrue Gain pred: -12.11, BFtrue Gain: -8.86,Critic Loss: 0.30, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2388\n",
            "Tr: 10.00,Beam: 17, Iteration: 1195, Q value: 0.5067, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -17.58, BFtrue Gain pred: -14.06, BFtrue Gain: -18.35,Critic Loss: 0.32, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2390\n",
            "Tr: 10.00,Beam: 17, Iteration: 1196, Q value: 0.5071, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -2.53, BFtrue Gain pred: -14.12, BFtrue Gain: -2.86,Critic Loss: 0.31, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2392\n",
            "Tr: 10.00,Beam: 17, Iteration: 1197, Q value: 0.5076, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -10.40, BFtrue Gain pred: -14.07, BFtrue Gain: -9.81,Critic Loss: 0.28, Policy Loss: -0.51\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2394\n",
            "Tr: 10.00,Beam: 17, Iteration: 1198, Q value: 0.5081, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -16.84, BFtrue Gain pred: -13.93, BFtrue Gain: -16.92,Critic Loss: 0.32, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2396\n",
            "Tr: 10.00,Beam: 17, Iteration: 1199, Q value: 0.5086, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -10.69, BFtrue Gain pred: -13.82, BFtrue Gain: -9.90,Critic Loss: 0.29, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2398\n",
            "Tr: 10.00,Beam: 17, Iteration: 1200, Q value: 0.5091, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -10.36, BFtrue Gain pred: -13.74, BFtrue Gain: -10.62,Critic Loss: 0.33, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2400\n",
            "Tr: 10.00,Beam: 17, Iteration: 1201, Q value: 0.5096, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -11.56, BFtrue Gain pred: -13.58, BFtrue Gain: -11.78,Critic Loss: 0.29, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2402\n",
            "Tr: 10.00,Beam: 17, Iteration: 1202, Q value: 0.5100, Reward: -1.0000, BF Gain pred: -11.95, BF Gain: -4.84, BFtrue Gain pred: -13.55, BFtrue Gain: -4.38,Critic Loss: 0.35, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2404\n",
            "Tr: 10.00,Beam: 17, Iteration: 1203, Q value: 0.5105, Reward: 1.0000, BF Gain pred: -11.84, BF Gain: -9.60, BFtrue Gain pred: -13.40, BFtrue Gain: -8.96,Critic Loss: 0.34, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2406\n",
            "Tr: 10.00,Beam: 17, Iteration: 1204, Q value: 0.5110, Reward: 1.0000, BF Gain pred: -9.13, BF Gain: -3.16, BFtrue Gain pred: -10.06, BFtrue Gain: -2.99,Critic Loss: 0.32, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2408\n",
            "Tr: 10.00,Beam: 17, Iteration: 1205, Q value: 0.5116, Reward: -1.0000, BF Gain pred: -12.44, BF Gain: -4.48, BFtrue Gain pred: -10.55, BFtrue Gain: -4.31,Critic Loss: 0.35, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2410\n",
            "Tr: 10.00,Beam: 17, Iteration: 1206, Q value: 0.4748, Reward: 1.0000, BF Gain pred: -7.22, BF Gain: -5.43, BFtrue Gain pred: -9.21, BFtrue Gain: -5.92,Critic Loss: 0.36, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2412\n",
            "Tr: 10.00,Beam: 17, Iteration: 1207, Q value: 0.4230, Reward: 1.0000, BF Gain pred: -5.37, BF Gain: -10.21, BFtrue Gain pred: -7.83, BFtrue Gain: -9.79,Critic Loss: 0.31, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2414\n",
            "Tr: 10.00,Beam: 17, Iteration: 1208, Q value: 0.5131, Reward: -1.0000, BF Gain pred: -18.64, BF Gain: -6.43, BFtrue Gain pred: -11.61, BFtrue Gain: -5.86,Critic Loss: 0.26, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2416\n",
            "Tr: 10.00,Beam: 17, Iteration: 1209, Q value: 0.5136, Reward: 1.0000, BF Gain pred: -9.73, BF Gain: -5.29, BFtrue Gain pred: -9.54, BFtrue Gain: -4.73,Critic Loss: 0.30, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2418\n",
            "Tr: 10.00,Beam: 17, Iteration: 1210, Q value: 0.5141, Reward: 1.0000, BF Gain pred: -9.59, BF Gain: -11.77, BFtrue Gain pred: -9.02, BFtrue Gain: -11.31,Critic Loss: 0.34, Policy Loss: -0.51\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2420\n",
            "Tr: 10.00,Beam: 17, Iteration: 1211, Q value: 0.5146, Reward: -1.0000, BF Gain pred: -15.93, BF Gain: -10.27, BFtrue Gain pred: -10.63, BFtrue Gain: -9.88,Critic Loss: 0.34, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2422\n",
            "Tr: 10.00,Beam: 17, Iteration: 1212, Q value: 0.5152, Reward: 1.0000, BF Gain pred: -12.57, BF Gain: -7.28, BFtrue Gain pred: -9.08, BFtrue Gain: -6.53,Critic Loss: 0.37, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2424\n",
            "Tr: 10.00,Beam: 17, Iteration: 1213, Q value: 0.5156, Reward: -1.0000, BF Gain pred: -13.03, BF Gain: -1.87, BFtrue Gain pred: -11.28, BFtrue Gain: -1.70,Critic Loss: 0.33, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2426\n",
            "Tr: 10.00,Beam: 17, Iteration: 1214, Q value: 0.5161, Reward: 1.0000, BF Gain pred: -11.90, BF Gain: -4.56, BFtrue Gain pred: -11.00, BFtrue Gain: -5.05,Critic Loss: 0.32, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2428\n",
            "Tr: 10.00,Beam: 17, Iteration: 1215, Q value: 0.5166, Reward: 1.0000, BF Gain pred: -2.64, BF Gain: -4.29, BFtrue Gain pred: -2.44, BFtrue Gain: -4.24,Critic Loss: 0.33, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2430\n",
            "Tr: 10.00,Beam: 17, Iteration: 1216, Q value: 0.5171, Reward: -1.0000, BF Gain pred: -10.36, BF Gain: -4.10, BFtrue Gain pred: -7.29, BFtrue Gain: -3.79,Critic Loss: 0.29, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2432\n",
            "Tr: 10.00,Beam: 17, Iteration: 1217, Q value: 0.5176, Reward: 1.0000, BF Gain pred: -9.13, BF Gain: -7.35, BFtrue Gain pred: -5.37, BFtrue Gain: -7.40,Critic Loss: 0.29, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2434\n",
            "Tr: 10.00,Beam: 17, Iteration: 1218, Q value: 0.5180, Reward: 1.0000, BF Gain pred: -4.55, BF Gain: -9.64, BFtrue Gain pred: -5.18, BFtrue Gain: -9.73,Critic Loss: 0.31, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2436\n",
            "Tr: 10.00,Beam: 17, Iteration: 1219, Q value: 0.5184, Reward: -1.0000, BF Gain pred: -12.40, BF Gain: -6.19, BFtrue Gain pred: -9.80, BFtrue Gain: -5.83,Critic Loss: 0.28, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2438\n",
            "Tr: 10.00,Beam: 17, Iteration: 1220, Q value: 0.5189, Reward: 1.0000, BF Gain pred: -5.18, BF Gain: -7.09, BFtrue Gain pred: -2.73, BFtrue Gain: -6.50,Critic Loss: 0.32, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2440\n",
            "Tr: 10.00,Beam: 17, Iteration: 1221, Q value: 0.5193, Reward: -1.0000, BF Gain pred: -5.42, BF Gain: -18.39, BFtrue Gain pred: -6.39, BFtrue Gain: -18.60,Critic Loss: 0.31, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2442\n",
            "Tr: 10.00,Beam: 17, Iteration: 1222, Q value: 0.5198, Reward: -1.0000, BF Gain pred: -12.20, BF Gain: -10.37, BFtrue Gain pred: -7.79, BFtrue Gain: -10.47,Critic Loss: 0.33, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2444\n",
            "Tr: 10.00,Beam: 17, Iteration: 1223, Q value: 0.5202, Reward: 1.0000, BF Gain pred: -2.18, BF Gain: -26.21, BFtrue Gain pred: -3.90, BFtrue Gain: -23.63,Critic Loss: 0.29, Policy Loss: -0.52\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2446\n",
            "Tr: 10.00,Beam: 17, Iteration: 1224, Q value: 0.5207, Reward: -1.0000, BF Gain pred: -6.16, BF Gain: -12.57, BFtrue Gain pred: -6.81, BFtrue Gain: -12.37,Critic Loss: 0.29, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2448\n",
            "Tr: 10.00,Beam: 17, Iteration: 1225, Q value: 0.5211, Reward: -1.0000, BF Gain pred: -7.71, BF Gain: -13.42, BFtrue Gain pred: -6.20, BFtrue Gain: -14.04,Critic Loss: 0.28, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2450\n",
            "Tr: 10.00,Beam: 17, Iteration: 1226, Q value: 0.5215, Reward: 1.0000, BF Gain pred: -6.52, BF Gain: -10.25, BFtrue Gain pred: -5.07, BFtrue Gain: -10.38,Critic Loss: 0.28, Policy Loss: -0.52\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2452\n",
            "Tr: 10.00,Beam: 17, Iteration: 1227, Q value: 0.5175, Reward: -1.0000, BF Gain pred: -7.68, BF Gain: -11.98, BFtrue Gain pred: -7.31, BFtrue Gain: -12.87,Critic Loss: 0.33, Policy Loss: -0.52\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2454\n",
            "Tr: 10.00,Beam: 17, Iteration: 1228, Q value: 0.5223, Reward: 1.0000, BF Gain pred: -6.98, BF Gain: -5.59, BFtrue Gain pred: -4.88, BFtrue Gain: -5.90,Critic Loss: 0.27, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2456\n",
            "Tr: 10.00,Beam: 17, Iteration: 1229, Q value: 0.5227, Reward: 1.0000, BF Gain pred: -6.97, BF Gain: -10.76, BFtrue Gain pred: -3.27, BFtrue Gain: -10.58,Critic Loss: 0.28, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2458\n",
            "Tr: 10.00,Beam: 17, Iteration: 1230, Q value: 0.5231, Reward: 1.0000, BF Gain pred: -6.32, BF Gain: -6.20, BFtrue Gain pred: -3.16, BFtrue Gain: -6.51,Critic Loss: 0.31, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2460\n",
            "Tr: 10.00,Beam: 17, Iteration: 1231, Q value: 0.5235, Reward: 1.0000, BF Gain pred: -5.36, BF Gain: -5.31, BFtrue Gain pred: -3.18, BFtrue Gain: -4.61,Critic Loss: 0.29, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2462\n",
            "Tr: 10.00,Beam: 17, Iteration: 1232, Q value: 0.5239, Reward: 1.0000, BF Gain pred: -3.80, BF Gain: -4.00, BFtrue Gain pred: -0.95, BFtrue Gain: -3.22,Critic Loss: 0.30, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2464\n",
            "Tr: 10.00,Beam: 17, Iteration: 1233, Q value: 0.5242, Reward: 1.0000, BF Gain pred: -2.46, BF Gain: -2.78, BFtrue Gain pred: 2.62, BFtrue Gain: -3.15,Critic Loss: 0.26, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2466\n",
            "Tr: 10.00,Beam: 17, Iteration: 1234, Q value: 0.5246, Reward: -1.0000, BF Gain pred: -5.89, BF Gain: -4.63, BFtrue Gain pred: -0.59, BFtrue Gain: -5.38,Critic Loss: 0.25, Policy Loss: -0.52\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2468\n",
            "Tr: 10.00,Beam: 17, Iteration: 1235, Q value: 0.5250, Reward: -1.0000, BF Gain pred: -6.11, BF Gain: -2.75, BFtrue Gain pred: -0.44, BFtrue Gain: -3.85,Critic Loss: 0.31, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2470\n",
            "Tr: 10.00,Beam: 17, Iteration: 1236, Q value: 0.5254, Reward: -1.0000, BF Gain pred: -7.04, BF Gain: -14.31, BFtrue Gain pred: -0.18, BFtrue Gain: -16.69,Critic Loss: 0.28, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2472\n",
            "Tr: 10.00,Beam: 17, Iteration: 1237, Q value: 0.5258, Reward: 1.0000, BF Gain pred: -5.19, BF Gain: -21.60, BFtrue Gain pred: -1.91, BFtrue Gain: -21.49,Critic Loss: 0.28, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2474\n",
            "Tr: 10.00,Beam: 17, Iteration: 1238, Q value: 0.5136, Reward: -1.0000, BF Gain pred: -6.26, BF Gain: -11.09, BFtrue Gain pred: -1.22, BFtrue Gain: -10.27,Critic Loss: 0.29, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2476\n",
            "Tr: 10.00,Beam: 17, Iteration: 1239, Q value: 0.1950, Reward: 1.0000, BF Gain pred: -2.80, BF Gain: -8.50, BFtrue Gain pred: -2.47, BFtrue Gain: -7.46,Critic Loss: 0.26, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2478\n",
            "Tr: 10.00,Beam: 17, Iteration: 1240, Q value: 0.5271, Reward: -1.0000, BF Gain pred: -6.85, BF Gain: -10.29, BFtrue Gain pred: -5.19, BFtrue Gain: -10.55,Critic Loss: 0.28, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2480\n",
            "Tr: 10.00,Beam: 17, Iteration: 1241, Q value: 0.5276, Reward: 1.0000, BF Gain pred: 2.32, BF Gain: -2.58, BFtrue Gain pred: -1.16, BFtrue Gain: -2.12,Critic Loss: 0.35, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2482\n",
            "Tr: 10.00,Beam: 17, Iteration: 1242, Q value: 0.4690, Reward: -1.0000, BF Gain pred: -2.80, BF Gain: -7.22, BFtrue Gain pred: -1.81, BFtrue Gain: -7.46,Critic Loss: 0.26, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2484\n",
            "Tr: 10.00,Beam: 17, Iteration: 1243, Q value: 0.5287, Reward: -1.0000, BF Gain pred: -5.55, BF Gain: -4.90, BFtrue Gain pred: -5.46, BFtrue Gain: -4.67,Critic Loss: 0.30, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2486\n",
            "Tr: 10.00,Beam: 17, Iteration: 1244, Q value: 0.5293, Reward: 1.0000, BF Gain pred: 2.73, BF Gain: 2.69, BFtrue Gain pred: -2.18, BFtrue Gain: 2.54,Critic Loss: 0.33, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2488\n",
            "Tr: 10.00,Beam: 17, Iteration: 1245, Q value: 0.5207, Reward: -1.0000, BF Gain pred: -2.94, BF Gain: 0.16, BFtrue Gain pred: 0.77, BFtrue Gain: 0.14,Critic Loss: 0.27, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2490\n",
            "Tr: 10.00,Beam: 17, Iteration: 1246, Q value: 0.4928, Reward: -1.0000, BF Gain pred: -3.04, BF Gain: -5.17, BFtrue Gain pred: 1.60, BFtrue Gain: -5.24,Critic Loss: 0.30, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2492\n",
            "Tr: 10.00,Beam: 17, Iteration: 1247, Q value: 0.5311, Reward: 1.0000, BF Gain pred: -2.98, BF Gain: -19.10, BFtrue Gain pred: -0.26, BFtrue Gain: -19.36,Critic Loss: 0.29, Policy Loss: -0.53\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2494\n",
            "Tr: 10.00,Beam: 17, Iteration: 1248, Q value: 0.4606, Reward: -1.0000, BF Gain pred: -5.22, BF Gain: -1.84, BFtrue Gain pred: -1.65, BFtrue Gain: -1.93,Critic Loss: 0.30, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2496\n",
            "Tr: 10.00,Beam: 17, Iteration: 1249, Q value: 0.5323, Reward: 1.0000, BF Gain pred: 5.95, BF Gain: -0.91, BFtrue Gain pred: 0.66, BFtrue Gain: -2.01,Critic Loss: 0.24, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2498\n",
            "Tr: 10.00,Beam: 17, Iteration: 1250, Q value: 0.5329, Reward: -1.0000, BF Gain pred: -4.04, BF Gain: -0.54, BFtrue Gain pred: -3.63, BFtrue Gain: -0.77,Critic Loss: 0.33, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2500\n",
            "Tr: 10.00,Beam: 17, Iteration: 1251, Q value: 0.5335, Reward: 1.0000, BF Gain pred: -2.17, BF Gain: -0.69, BFtrue Gain pred: -2.57, BFtrue Gain: -1.78,Critic Loss: 0.30, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2502\n",
            "Tr: 10.00,Beam: 17, Iteration: 1252, Q value: 0.5297, Reward: 1.0000, BF Gain pred: -1.64, BF Gain: 0.35, BFtrue Gain pred: -3.00, BFtrue Gain: -1.30,Critic Loss: 0.27, Policy Loss: -0.53\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2504\n",
            "Tr: 10.00,Beam: 17, Iteration: 1253, Q value: 0.5238, Reward: 1.0000, BF Gain pred: 1.50, BF Gain: 3.23, BFtrue Gain pred: -0.11, BFtrue Gain: 2.71,Critic Loss: 0.34, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2506\n",
            "Tr: 10.00,Beam: 17, Iteration: 1254, Q value: 0.4885, Reward: -1.0000, BF Gain pred: 1.11, BF Gain: 6.34, BFtrue Gain pred: -1.13, BFtrue Gain: 6.62,Critic Loss: 0.31, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2508\n",
            "Tr: 10.00,Beam: 17, Iteration: 1255, Q value: 0.5355, Reward: -1.0000, BF Gain pred: 0.26, BF Gain: -4.90, BFtrue Gain pred: -0.88, BFtrue Gain: -5.34,Critic Loss: 0.29, Policy Loss: -0.54\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2510\n",
            "Tr: 10.00,Beam: 17, Iteration: 1256, Q value: 0.5360, Reward: -1.0000, BF Gain pred: -3.75, BF Gain: -20.76, BFtrue Gain pred: -3.16, BFtrue Gain: -18.08,Critic Loss: 0.29, Policy Loss: -0.54\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2512\n",
            "Tr: 10.00,Beam: 17, Iteration: 1257, Q value: 0.5365, Reward: 1.0000, BF Gain pred: -0.94, BF Gain: -6.88, BFtrue Gain pred: -1.89, BFtrue Gain: -7.79,Critic Loss: 0.33, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2514\n",
            "Tr: 10.00,Beam: 17, Iteration: 1258, Q value: 0.5282, Reward: 1.0000, BF Gain pred: 0.49, BF Gain: -5.17, BFtrue Gain pred: -1.83, BFtrue Gain: -6.17,Critic Loss: 0.31, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2516\n",
            "Tr: 10.00,Beam: 17, Iteration: 1259, Q value: 0.5375, Reward: -1.0000, BF Gain pred: -1.56, BF Gain: -7.13, BFtrue Gain pred: -2.75, BFtrue Gain: -7.33,Critic Loss: 0.28, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2518\n",
            "Tr: 10.00,Beam: 17, Iteration: 1260, Q value: 0.5380, Reward: 1.0000, BF Gain pred: 0.44, BF Gain: -17.56, BFtrue Gain pred: 1.37, BFtrue Gain: -15.23,Critic Loss: 0.29, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2520\n",
            "Tr: 10.00,Beam: 17, Iteration: 1261, Q value: 0.5385, Reward: 1.0000, BF Gain pred: 3.07, BF Gain: -15.66, BFtrue Gain pred: -2.08, BFtrue Gain: -16.13,Critic Loss: 0.28, Policy Loss: -0.54\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2522\n",
            "Tr: 10.00,Beam: 17, Iteration: 1262, Q value: 0.5390, Reward: -1.0000, BF Gain pred: -0.24, BF Gain: -11.13, BFtrue Gain pred: -0.24, BFtrue Gain: -11.67,Critic Loss: 0.30, Policy Loss: -0.54\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2524\n",
            "Tr: 10.00,Beam: 17, Iteration: 1263, Q value: 0.5395, Reward: 1.0000, BF Gain pred: 0.63, BF Gain: -7.97, BFtrue Gain pred: -0.12, BFtrue Gain: -9.02,Critic Loss: 0.28, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2526\n",
            "Tr: 10.00,Beam: 17, Iteration: 1264, Q value: 0.5400, Reward: 1.0000, BF Gain pred: 1.17, BF Gain: -4.95, BFtrue Gain pred: -0.69, BFtrue Gain: -4.75,Critic Loss: 0.31, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2528\n",
            "Tr: 10.00,Beam: 17, Iteration: 1265, Q value: 0.5404, Reward: 1.0000, BF Gain pred: 7.89, BF Gain: -5.47, BFtrue Gain pred: 2.99, BFtrue Gain: -5.43,Critic Loss: 0.27, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2530\n",
            "Tr: 10.00,Beam: 17, Iteration: 1266, Q value: 0.5409, Reward: -1.0000, BF Gain pred: -2.04, BF Gain: -10.84, BFtrue Gain pred: -2.39, BFtrue Gain: -10.80,Critic Loss: 0.29, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2532\n",
            "Tr: 10.00,Beam: 17, Iteration: 1267, Q value: 0.5414, Reward: 1.0000, BF Gain pred: 5.42, BF Gain: -7.69, BFtrue Gain pred: 2.09, BFtrue Gain: -8.40,Critic Loss: 0.28, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2534\n",
            "Tr: 10.00,Beam: 17, Iteration: 1268, Q value: 0.4065, Reward: -1.0000, BF Gain pred: -1.42, BF Gain: -18.32, BFtrue Gain pred: -1.71, BFtrue Gain: -19.65,Critic Loss: 0.29, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2536\n",
            "Tr: 10.00,Beam: 17, Iteration: 1269, Q value: 0.5387, Reward: 1.0000, BF Gain pred: 2.19, BF Gain: -6.38, BFtrue Gain pred: 0.71, BFtrue Gain: -6.07,Critic Loss: 0.25, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2538\n",
            "Tr: 10.00,Beam: 17, Iteration: 1270, Q value: 0.4502, Reward: -1.0000, BF Gain pred: -2.38, BF Gain: -9.65, BFtrue Gain pred: 0.83, BFtrue Gain: -9.40,Critic Loss: 0.31, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2540\n",
            "Tr: 10.00,Beam: 17, Iteration: 1271, Q value: 0.5370, Reward: 1.0000, BF Gain pred: 2.77, BF Gain: -4.56, BFtrue Gain pred: 1.90, BFtrue Gain: -4.63,Critic Loss: 0.31, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2542\n",
            "Tr: 10.00,Beam: 17, Iteration: 1272, Q value: 0.4116, Reward: -1.0000, BF Gain pred: 0.06, BF Gain: -8.28, BFtrue Gain pred: 2.61, BFtrue Gain: -8.28,Critic Loss: 0.30, Policy Loss: -0.54\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2544\n",
            "Tr: 10.00,Beam: 17, Iteration: 1273, Q value: 0.5307, Reward: 1.0000, BF Gain pred: 1.78, BF Gain: -2.16, BFtrue Gain pred: 2.23, BFtrue Gain: -2.20,Critic Loss: 0.32, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2546\n",
            "Tr: 10.00,Beam: 17, Iteration: 1274, Q value: 0.5434, Reward: -1.0000, BF Gain pred: -1.99, BF Gain: 1.31, BFtrue Gain pred: 1.69, BFtrue Gain: 1.49,Critic Loss: 0.26, Policy Loss: -0.54\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2548\n",
            "Tr: 10.00,Beam: 17, Iteration: 1275, Q value: 0.5450, Reward: -1.0000, BF Gain pred: -2.28, BF Gain: -15.90, BFtrue Gain pred: 1.72, BFtrue Gain: -15.67,Critic Loss: 0.31, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2550\n",
            "Tr: 10.00,Beam: 17, Iteration: 1276, Q value: 0.5454, Reward: -1.0000, BF Gain pred: -3.97, BF Gain: -7.60, BFtrue Gain pred: -0.16, BFtrue Gain: -6.07,Critic Loss: 0.29, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2552\n",
            "Tr: 10.00,Beam: 17, Iteration: 1277, Q value: 0.5011, Reward: -1.0000, BF Gain pred: -3.97, BF Gain: 3.28, BFtrue Gain pred: -1.04, BFtrue Gain: 3.67,Critic Loss: 0.28, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2554\n",
            "Tr: 10.00,Beam: 17, Iteration: 1278, Q value: 0.5362, Reward: -1.0000, BF Gain pred: -4.58, BF Gain: -6.55, BFtrue Gain pred: -1.77, BFtrue Gain: -5.95,Critic Loss: 0.30, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2556\n",
            "Tr: 10.00,Beam: 17, Iteration: 1279, Q value: 0.5438, Reward: 1.0000, BF Gain pred: -2.23, BF Gain: 4.44, BFtrue Gain pred: -1.63, BFtrue Gain: 4.37,Critic Loss: 0.29, Policy Loss: -0.55\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2558\n",
            "Tr: 10.00,Beam: 17, Iteration: 1280, Q value: 0.5383, Reward: -1.0000, BF Gain pred: -6.66, BF Gain: -3.61, BFtrue Gain pred: -6.53, BFtrue Gain: -3.78,Critic Loss: 0.30, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2560\n",
            "Tr: 10.00,Beam: 17, Iteration: 1281, Q value: 0.5326, Reward: 1.0000, BF Gain pred: -1.56, BF Gain: 0.52, BFtrue Gain pred: -2.45, BFtrue Gain: 0.33,Critic Loss: 0.27, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2562\n",
            "Tr: 10.00,Beam: 17, Iteration: 1282, Q value: 0.4737, Reward: -1.0000, BF Gain pred: -7.89, BF Gain: -9.03, BFtrue Gain pred: -7.56, BFtrue Gain: -8.58,Critic Loss: 0.31, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2564\n",
            "Tr: 10.00,Beam: 17, Iteration: 1283, Q value: 0.5294, Reward: 1.0000, BF Gain pred: -6.23, BF Gain: -16.08, BFtrue Gain pred: -6.71, BFtrue Gain: -16.37,Critic Loss: 0.27, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2566\n",
            "Tr: 10.00,Beam: 17, Iteration: 1284, Q value: 0.5489, Reward: 1.0000, BF Gain pred: -6.11, BF Gain: -1.43, BFtrue Gain pred: -6.21, BFtrue Gain: -0.20,Critic Loss: 0.26, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2568\n",
            "Tr: 10.00,Beam: 17, Iteration: 1285, Q value: 0.5493, Reward: 1.0000, BF Gain pred: -5.27, BF Gain: -5.31, BFtrue Gain pred: -5.42, BFtrue Gain: -5.47,Critic Loss: 0.27, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2570\n",
            "Tr: 10.00,Beam: 17, Iteration: 1286, Q value: 0.5497, Reward: 1.0000, BF Gain pred: -3.48, BF Gain: -25.86, BFtrue Gain pred: -5.11, BFtrue Gain: -25.94,Critic Loss: 0.28, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2572\n",
            "Tr: 10.00,Beam: 17, Iteration: 1287, Q value: 0.5501, Reward: -1.0000, BF Gain pred: -6.09, BF Gain: -12.33, BFtrue Gain pred: -5.00, BFtrue Gain: -12.13,Critic Loss: 0.31, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2574\n",
            "Tr: 10.00,Beam: 17, Iteration: 1288, Q value: 0.4015, Reward: -1.0000, BF Gain pred: -7.16, BF Gain: -22.31, BFtrue Gain pred: -7.06, BFtrue Gain: -22.21,Critic Loss: 0.27, Policy Loss: -0.55\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2576\n",
            "Tr: 10.00,Beam: 17, Iteration: 1289, Q value: 0.5509, Reward: -1.0000, BF Gain pred: -7.43, BF Gain: -10.22, BFtrue Gain pred: -6.24, BFtrue Gain: -9.65,Critic Loss: 0.28, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2578\n",
            "Tr: 10.00,Beam: 17, Iteration: 1290, Q value: 0.5448, Reward: -1.0000, BF Gain pred: -8.05, BF Gain: -10.44, BFtrue Gain pred: -5.31, BFtrue Gain: -9.52,Critic Loss: 0.30, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2580\n",
            "Tr: 10.00,Beam: 17, Iteration: 1291, Q value: 0.4623, Reward: 1.0000, BF Gain pred: -3.87, BF Gain: -13.12, BFtrue Gain pred: -8.51, BFtrue Gain: -12.17,Critic Loss: 0.27, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2582\n",
            "Tr: 10.00,Beam: 17, Iteration: 1292, Q value: 0.4765, Reward: -1.0000, BF Gain pred: -6.45, BF Gain: -6.36, BFtrue Gain pred: -8.26, BFtrue Gain: -6.20,Critic Loss: 0.27, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2584\n",
            "Tr: 10.00,Beam: 17, Iteration: 1293, Q value: 0.5529, Reward: -1.0000, BF Gain pred: -8.08, BF Gain: -14.50, BFtrue Gain pred: -14.40, BFtrue Gain: -14.51,Critic Loss: 0.28, Policy Loss: -0.55\n",
            "torch.Size([5, 1]) torch.Size([1, 36])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Critic_(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Critic_, self).__init__()\n",
        "\n",
        "        self.scaling_factor = 16\n",
        "        self.fc1 = nn.Linear(input_size, self.scaling_factor * input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(self.scaling_factor * input_size)\n",
        "        self.fc2 = nn.Linear(self.scaling_factor * input_size, self.scaling_factor * output_size)\n",
        "        self.bn2 = nn.BatchNorm1d(self.scaling_factor * output_size)\n",
        "        self.fc3 = nn.Linear(self.scaling_factor * output_size, output_size)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        #print(\"state, action\", state, action)\n",
        "        x = torch.cat((state, action), 1)\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, ch, p_factor):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.M = int(input_size / 2)\n",
        "        self.output_size = output_size\n",
        "        self.penalty_factor = torch.tensor(p_factor).float().cuda()\n",
        "\n",
        "        # just for debugging\n",
        "        self.ch_r = torch.from_numpy(ch[:, :self.M].transpose()).float().cuda()\n",
        "        self.ch_i = torch.from_numpy(ch[:, self.M:].transpose()).float().cuda()\n",
        "\n",
        "        # self.H_r = nn.Parameter(torch.randn(self.M, 16))\n",
        "        self.H_r = torch.from_numpy(ch[:, :self.M].transpose()).float().cuda()\n",
        "        # self.H_i = nn.Parameter(torch.randn(self.M, 16))\n",
        "        self.H_i = torch.from_numpy(ch[:, self.M:].transpose()).float().cuda()\n",
        "\n",
        "        self.Thetas = nn.Parameter(torch.randn(self.M, 32))\n",
        "        self.R = nn.Parameter(torch.rand(self.M, 32))\n",
        "\n",
        "        self.fc1 = nn.Linear(522, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        # x = torch.cat((state, action), 1)\n",
        "\n",
        "        # self.H_r = (1 / torch.sqrt(torch.tensor(self.M))) * torch.cos(self.Thetas) * self.R\n",
        "        # self.H_i = (1 / torch.sqrt(torch.tensor(self.M))) * torch.sin(self.Thetas) * self.R\n",
        "\n",
        "        x_state = phase2bf(state)\n",
        "        x_state_r, x_state_i = x_state[:, :self.M], x_state[:, self.M:]\n",
        "\n",
        "        z_r = x_state_r @ self.H_r + x_state_i @ self.H_i\n",
        "        z_i = x_state_r @ self.H_i - x_state_i @ self.H_r\n",
        "\n",
        "        z = z_r ** 2 + z_i ** 2\n",
        "        z = z ** self.penalty_factor\n",
        "        # z_min = torch.min(z, dim=1).values.reshape(-1, 1)\n",
        "\n",
        "        x_action = phase2bf(action)\n",
        "        x_action_r, x_action_i = x_action[:, :self.M], x_action[:, self.M:]\n",
        "\n",
        "        u_r = x_action_r @ self.H_r + x_action_i @ self.H_i\n",
        "        u_i = x_action_r @ self.H_i - x_action_i @ self.H_r\n",
        "\n",
        "        u = u_r ** 2 + u_i ** 2\n",
        "        u = u ** self.penalty_factor\n",
        "        # u_min = torch.min(u, dim=1).values.reshape(-1, 1)\n",
        "\n",
        "        # ----------- up to this point ----------- #\n",
        "        # \"z\" is of size (batch, S)\n",
        "        # \"u\" is of size (batch, S)\n",
        "        # ---------------------------------------- #\n",
        "\n",
        "        feature = torch.cat((z, u), dim=1)\n",
        "        out = torch.relu(self.fc1(feature))\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        # out = 10 * torch.log10(torch.mean(u, dim=1).reshape(-1, 1)) - 10 * torch.log10(torch.mean(z, dim=1).reshape(-1, 1))\n",
        "\n",
        "        # ----------- this one works ----------- #\n",
        "        # out = torch.mean(10 * torch.log10(u), dim=1).reshape(-1, 1) - torch.mean(10 * torch.log10(z), dim=1).reshape(-1, 1)\n",
        "        # -------------------------------------- #\n",
        "\n",
        "        # out = -torch.mean(torch.divide(torch.tensor(1.), u), dim=1).reshape(-1, 1) + \\\n",
        "        #       torch.mean(torch.divide(torch.tensor(1.), z), dim=1).reshape(-1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.pi = torch.tensor(np.pi).float().cuda()\n",
        "        self.scaling_factor = 16\n",
        "        self.fc1 = nn.Linear(input_size, self.scaling_factor * input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(self.scaling_factor * input_size)\n",
        "        self.fc2 = nn.Linear(self.scaling_factor * input_size, self.scaling_factor * output_size)\n",
        "        self.bn2 = nn.BatchNorm1d(self.scaling_factor * output_size)\n",
        "        self.fc3 = nn.Linear(self.scaling_factor * output_size, output_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.bn1(self.fc1(state)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = torch.tanh(self.fc3(x)) * self.pi\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Actor_(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Actor_, self).__init__()\n",
        "        self.pi = torch.tensor(np.pi).float().cuda()\n",
        "        self.scaling_factor = 1\n",
        "        self.fc1 = nn.Linear(input_size, self.scaling_factor * input_size)\n",
        "        # self.bn1 = nn.BatchNorm1d(self.scaling_factor * input_size)\n",
        "        # self.fc2 = nn.Linear(self.scaling_factor * input_size, self.scaling_factor * output_size)\n",
        "        # self.bn2 = nn.BatchNorm1d(self.scaling_factor * output_size)\n",
        "        # self.fc3 = nn.Linear(self.scaling_factor * output_size, output_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # x = F.relu(self.bn1(self.fc1(state)))\n",
        "        # x = F.relu(self.bn2(self.fc2(x)))\n",
        "        # x = torch.tanh(self.fc3(x)) * self.pi\n",
        "        x = self.fc1(state)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "class OUNoise(object):\n",
        "    def __init__(self, action_shape, mu=0.0, theta=0.15, max_sigma=1, min_sigma=0.08, decay_period=10000):\n",
        "        self.mu = mu\n",
        "        self.theta = theta\n",
        "        self.sigma = max_sigma\n",
        "        self.max_sigma = max_sigma\n",
        "        self.min_sigma = min_sigma\n",
        "        self.decay_period = decay_period\n",
        "        self.action_dim = action_shape\n",
        "        self.low = -np.pi\n",
        "        self.high = np.pi\n",
        "        self.state = self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        state = torch.ones(self.action_dim) * self.mu\n",
        "        return state.float().cuda()\n",
        "\n",
        "    def evolve_state(self):\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * torch.normal(0, 1, size=self.action_dim).cuda()\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "\n",
        "    def get_action(self, action, t=0):\n",
        "        ou_state = self.evolve_state()\n",
        "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "        # return torch.clamp(action + ou_state, self.low, self.high)\n",
        "        return action + ou_state\n",
        "\n",
        "\n",
        "def phase2bf(ph_mat):\n",
        "    # ph_mat: (i) a tensor, (ii) B x M\n",
        "    # bf_mat: (i) a tensor, (ii) B x 2M\n",
        "    # B stands for batch size and M is the number of antenna\n",
        "\n",
        "    M = torch.tensor(ph_mat.shape[1]).to(ph_mat.device)\n",
        "    bf_mat = torch.exp(1j * ph_mat)\n",
        "    bf_mat_r = torch.real(bf_mat)\n",
        "    bf_mat_i = torch.imag(bf_mat)\n",
        "\n",
        "    bf_mat_ = (1 / torch.sqrt(M)) * torch.cat((bf_mat_r, bf_mat_i), dim=1)\n",
        "\n",
        "    return bf_mat_\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "#from DDPG_classes import Actor, Critic_, OUNoise, init_weights\n",
        "#from env_ddpg import envCB\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "def train(options, train_options, beam_id, exp_id, training_size):\n",
        "\n",
        "    with torch.cuda.device(options['gpu_idx']):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print('Beam', beam_id, 'training begins. GPU being used:', torch.cuda.current_device())\n",
        "        options['best_value'] = torch.tensor(-30).cuda()\n",
        "        options['best_state'] = torch.rand(1,25).cuda()\n",
        "        options['ph_table_rep'] = options['ph_table_rep'].cuda()\n",
        "        options['multi_step'] = options['multi_step'].cuda()\n",
        "        options['ph_table'] = options['ph_table'].cuda()\n",
        "        ch_bs_irs =torch.tensor( options['ch_bs_irs'][5:]).cuda() #torch.from_numpy(channel).float().cuda()\n",
        "        ch_bs_irs_i =torch.tensor( options['ch_bs_irs_i'][5:]).cuda()\n",
        "        ch_bs_irs_i2 =torch.tensor( options['ch_bs_irs_i2'][5:]).cuda()\n",
        "        channel1 = torch.tensor( options['ch_t3'][5:]).cuda()\n",
        "        if training_size==0.1:\n",
        "          model_file = '/content/drive/MyDrive/Colab_Notebooks/model_based_01M_params_nn_M_36_signal_tr_size_90000_repeat_02.pt'\n",
        "          min_SIR = 11\n",
        "          trsize = '0.1'\n",
        "        elif training_size==0.5:\n",
        "          model_file = '/content/drive/MyDrive/Colab_Notebooks/model_based_05M_params_nn_M_36_signal_02.pt'\n",
        "          min_SIR = 13\n",
        "          trsize= '0.5'\n",
        "        elif training_size==1:\n",
        "          model_file = '/content/drive/MyDrive/Colab_Notebooks/model_based_1M_params_nn_M_36_signal_tr_size_900000_repeat_0.pt'\n",
        "          min_SIR = 15\n",
        "          trsize='1'\n",
        "        elif training_size==5:\n",
        "          model_file = '/content/drive/MyDrive/model_based_5M_params_nn_M_36_signal_tr_size_4500000_repeat_02.pt'\n",
        "          min_SIR = 19\n",
        "          trsize ='5'\n",
        "        elif training_size==10:\n",
        "          model_file = '/content/drive/MyDrive/model_based_10M_params_nn_M_36_signal_tr_size_9000000_repeat_0.pt'\n",
        "          min_SIR = 23\n",
        "          trsize ='10'\n",
        "        else:\n",
        "          raise  ValueError(\"Invalid training size. Select a value from [0.5, 1, 5, 10]\")\n",
        "\n",
        "        model_signal =  GainPred_Dense(options['num_ant'])\n",
        "        model_signal.load_state_dict(torch.load(model_file, weights_only=True))\n",
        "\n",
        "\n",
        "\n",
        "        print(model_signal)\n",
        "        #model_signal.load_state_dict(torch.load('/content/model_based_05M_params_nn_M_36_signal_02.pt'))\n",
        "\n",
        "        model_signal.eval()\n",
        "        model_signal.cuda()\n",
        "\n",
        "\n",
        "        # options['ch_t'] = torch.from_numpy(options['ch_t']).float().cuda()\n",
        "        # options['ch_i'] = torch.from_numpy(options['ch_i']).float().cuda()\n",
        "\n",
        "        options['ph_table_rep'] = options['ph_table_rep'].cuda()\n",
        "        options['multi_step'] = options['multi_step'].cuda()\n",
        "        options['ph_table'] = options['ph_table'].cuda()\n",
        "\n",
        "\n",
        "        real_time_perf_true = np.zeros((train_options['num_iter'],))\n",
        "        real_time_perf_n_true = np.zeros((train_options['num_iter'],))\n",
        "\n",
        "        real_time_perf = np.zeros((train_options['num_iter'],))\n",
        "        real_time_perf_n = np.zeros((train_options['num_iter'],))\n",
        "\n",
        "        actor_net1 = Actor(options['num_ant'], options['num_ant'])\n",
        "        critic_net1 = Critic_(2 * options['num_ant'], 1)\n",
        "        actor_net1.cuda()\n",
        "        actor_net1.apply(init_weights)\n",
        "        actor_net1.apply(init_weights)\n",
        "        critic_net1.cuda()\n",
        "        critic_net1.apply(init_weights)\n",
        "        critic_net1.apply(init_weights)\n",
        "        ounoise = OUNoise((1, options['num_ant']))\n",
        "\n",
        "        env_1 = envCB_1(options['num_ant'], options['num_bits'], beam_id, options, model_signal, exp_id)\n",
        "        env_2 = envCB_1(options['num_ant'], options['num_bits'], beam_id, options, model_signal, exp_id)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        actor_net1.cuda()\n",
        "        critic_net1.cuda()\n",
        "        actor_net1.apply(init_weights)\n",
        "        critic_net1.apply(init_weights)\n",
        "\n",
        "\n",
        "        CB_Env_1 = env_1\n",
        "        CB_Env_2 = env_2\n",
        "        critic_criterion = nn.MSELoss()\n",
        "\n",
        "        critic_optimizer1 = optim.Adam(critic_net1.parameters(), lr=1e-3)\n",
        "        actor_optimizer1 = optim.Adam(actor_net1.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        if train_options['overall_iter'] == 1:\n",
        "            state = (torch.rand((1, options['num_ant'])).float().cuda() * 2 * torch.pi) - torch.pi  # vector of phases\n",
        "            print('Initial State Activated.')\n",
        "        else:\n",
        "            state = train_options['state']\n",
        "        epsi_ones = np.ones([1,95])*0.1\n",
        "        epsilon_ddpg = [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1] + epsi_ones.squeeze().tolist()\n",
        "\n",
        "\n",
        "        best_experiences_pred =  []\n",
        "        best_experiences =  []\n",
        "        # -------------- training -------------- #\n",
        "        replay_memory = train_options['replay_memory']\n",
        "        iteration = 0\n",
        "        num_of_iter = train_options['num_iter']\n",
        "\n",
        "        best_states, best_actions_pred, best_rewards_pred, best_values_pred = None, None, float('-inf'), float('-inf')\n",
        "        best_states, best_actions, best_rewards, best_values = None, None, float('-inf'),float('-inf')\n",
        "        revisit_frequency = 1000\n",
        "        count = 0\n",
        "        bf_gain_pred_prev = float('-inf')\n",
        "        while iteration < num_of_iter:\n",
        "\n",
        "            state = state.float().cuda()\n",
        "            actor_net1.eval()\n",
        "\n",
        "            user_num = 1\n",
        "            action_pred = torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            reward_pred =torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            action_quant_pred =torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            state_1_pred =torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            state_1  = torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            reward = torch.zeros(user_num,).cuda()\n",
        "            bf_gain = torch.zeros(user_num,).cuda()\n",
        "            bf_gain_pred = torch.zeros(user_num,).cuda()\n",
        "\n",
        "            random_number = np.random.rand()> epsilon_ddpg[count]\n",
        "\n",
        "            if random_number  and iteration>1000:  # 10% probability to revisit best state\n",
        "                if len(replay_memory) > train_options['replay_memory_size'] -len(best_experiences)-len(best_experiences_pred):\n",
        "                    replay_memory = replay_memory[len(best_experiences):]\n",
        "                    replay_memory = replay_memory[len(best_experiences_pred):]\n",
        "                print(\"\\n Added best experiences \\n\")\n",
        "                replay_memory = replay_memory+best_experiences\n",
        "                replay_memory = replay_memory+best_experiences_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            action_pred = actor_net1(state)\n",
        "            reward_pred, bf_gain_pred, action_quant_pred, state_1_pred = CB_Env_1.get_reward(action_pred) #!!!!!\n",
        "            critic_net1.eval()\n",
        "            q_pred1 = critic_net1(state, action_quant_pred)\n",
        "            action_pred_noisy = ounoise.get_action(action_pred, t=train_options['overall_iter'])  # torch.Size([1, action_dim])\n",
        "            mat_dist = torch.abs(action_pred_noisy.reshape(options['num_ant'], 1) - options['ph_table_rep'])\n",
        "            action_quant = options['ph_table_rep'][range(options['num_ant']), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
        "\n",
        "            state_1, reward, bf_gain, terminal1 = CB_Env_1.step(action_quant)  # get next state and reward\n",
        "            action = action_quant.reshape((1, -1)).float().cuda()  # best action according0ly\n",
        "            real_time_perf[iteration] = torch.Tensor.cpu(bf_gain_pred.detach()).numpy()\n",
        "            real_time_perf_n[iteration] = torch.Tensor.cpu(bf_gain.detach()).numpy()\n",
        "            bf_gain_pred_true = BF_GAIN_signal( action_pred,  ch_bs_irs, channel1 ) - BF_GAIN_interf( action_pred,  ch_bs_irs_i,ch_bs_irs_i2, channel1 )\n",
        "            bf_gain_true =  BF_GAIN_signal( action,  ch_bs_irs, channel1 ) - BF_GAIN_interf( action,  ch_bs_irs_i,ch_bs_irs_i2, channel1 )\n",
        "\n",
        "            real_time_perf_true[iteration] = bf_gain_pred_true\n",
        "            real_time_perf_n_true[iteration] =bf_gain_true\n",
        "\n",
        "            if torch.abs(bf_gain_pred -bf_gain_pred_prev)<0.01 and bf_gain_pred<30:\n",
        "                reward -= 2 # Add a penalty for being stuck\n",
        "            bf_gain_pred_prev = bf_gain_pred\n",
        "\n",
        "\n",
        "            if  bf_gain_pred > options['best_value']:\n",
        "\n",
        "                options['best_value'] = bf_gain_pred.clone()\n",
        "                options['best_state'] = state_1_pred.clone()\n",
        "\n",
        "            replay_memory.append((state, action, reward, state_1, terminal1))\n",
        "            replay_memory.append((state, action_pred,reward_pred, state_1_pred, terminal1))\n",
        "\n",
        "\n",
        "            if bf_gain_pred> min_SIR :\n",
        "\n",
        "                best_experiences_pred.append((state.clone(), action_pred.clone(),reward_pred.clone(), state_1_pred.clone(), terminal1))\n",
        "                print(\"best_experiences_pred length\", len(best_experiences_pred ))\n",
        "                if bf_gain_pred > best_values_pred:\n",
        "                    best_states = state\n",
        "                    best_actions_pred = action_quant_pred\n",
        "                    best_rewards_pred = reward_pred\n",
        "                    best_values_pred = bf_gain_pred\n",
        "\n",
        "            if bf_gain > min_SIR:\n",
        "                best_experiences.append((state.clone(), action.clone(),reward.clone(), state_1.clone(), terminal1))\n",
        "                print(\"best_experiences length\", len(best_experiences ))\n",
        "                if bf_gain > best_values:\n",
        "                    best_states = state\n",
        "                    best_actions = action\n",
        "                    best_rewards = reward\n",
        "                    best_values = bf_gain\n",
        "            best_experiences_pred = list(set(best_experiences_pred))\n",
        "            best_experiences = list(set(best_experiences))\n",
        "            print(\"Best gain pred\",best_values_pred,\"Best gain\", best_values)\n",
        "            print(\"Best gain pred and | length\",len(best_experiences)+len(best_experiences_pred))\n",
        "            print(\"Length of the buffer:\", len(replay_memory))\n",
        "            #print(\"Epsilon:\", epsilon_ddpg[count])\n",
        "            if iteration%1000 == 0:\n",
        "                count +=1\n",
        "            #print(\"MAG state\", mag_state.shape)\n",
        "            #print(\"State:\", state,\"\\nAction_pred_quant\", action_quant_pred,\"\\nAction_quant\", action)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            while len(replay_memory) > train_options['replay_memory_size']:\n",
        "                replay_memory.pop(0)\n",
        "\n",
        "            # -------------- Experience Replay -------------- #\n",
        "            minibatch = random.sample(replay_memory, min(len(replay_memory), train_options['minibatch_size']))\n",
        "\n",
        "            # unpack minibatch, since torch.cat is by default dim=0, which is the dimension of batch\n",
        "            state_batch = torch.cat(tuple(d[0] for d in minibatch))  # torch.Size([*, state_dim])\n",
        "            action_batch = torch.cat(tuple(d[1] for d in minibatch))  # torch.Size([*, action_dim])\n",
        "            reward_batch = torch.cat(tuple(d[2] for d in minibatch))  # torch.Size([*, 1])\n",
        "            state_1_batch = torch.cat(tuple(d[3] for d in minibatch))  # torch.Size([*, state_dim])\n",
        "\n",
        "            state_batch = state_batch.detach()\n",
        "            action_batch = action_batch.detach()\n",
        "            reward_batch = reward_batch.detach()\n",
        "            state_1_batch = state_1_batch.detach()\n",
        "\n",
        "            if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
        "                state_batch = state_batch.cuda()\n",
        "                action_batch = action_batch.cuda()\n",
        "                reward_batch = reward_batch.cuda()\n",
        "                state_1_batch = state_1_batch.cuda()\n",
        "\n",
        "            # loss calculation for Critic Network\n",
        "            critic_net1.train()\n",
        "            Q_prime1 = reward_batch\n",
        "            Q_pred1 = critic_net1(state_batch, action_batch)\n",
        "            critic_loss1 = critic_criterion(Q_pred1, Q_prime1.detach())\n",
        "\n",
        "            # Update Critic Network\n",
        "            critic_optimizer1.zero_grad()\n",
        "            critic_loss1.backward()\n",
        "            critic_optimizer1.step()\n",
        "\n",
        "            # loss calculation for Actor Network\n",
        "\n",
        "\n",
        "\n",
        "            actor_net1.train()\n",
        "            critic_net1.eval()\n",
        "            '''\n",
        "\n",
        "\n",
        "            '''\n",
        "            actor_loss1 = torch.mean(-critic_net1(state_batch, actor_net1(state_batch)))\n",
        "            #actor_loss1 = -torch.mean(target_critic_net1(state_batch, actor_net1(state_batch)))\n",
        "            actor_optimizer1.zero_grad()\n",
        "            actor_loss1.backward()\n",
        "            actor_optimizer1.step()\n",
        "\n",
        "            # UPDATE state, epsilon, target network, etc.\n",
        "            state = state_1_pred\n",
        "            iteration += 1\n",
        "            train_options['overall_iter'] += 1  # global counter\n",
        "\n",
        "            if train_options['overall_iter'] % options['save_freq'] == 0:\n",
        "                if not os.path.exists('pretrained_model/'):\n",
        "                    os.mkdir('pretrained_model/')\n",
        "                PATH = 'pretrained_model/beam' + str(beam_id) + '_iter' + str(train_options['overall_iter']) + '.pth'\n",
        "                torch.save(critic_net1.state_dict(), PATH)\n",
        "                torch.save(actor_net1.state_dict(), PATH)\n",
        "\n",
        "            # store: best beamforming vector so far\n",
        "            if train_options['overall_iter'] % options['pf_print'] == 0:\n",
        "                iter_id = np.array(train_options['overall_iter']).reshape(1, 1)\n",
        "                best_state = CB_Env_1.best_bf_vec.reshape(1, -1)\n",
        "                if os.path.exists('pfs/pf_' + str(beam_id) + '.txt'):\n",
        "                    with open('pfs/pf_' + str(beam_id) + '.txt', 'ab') as bm:\n",
        "                        np.savetxt(bm, iter_id, fmt='%d', delimiter='\\n')\n",
        "                    with open('pfs/pf_' + str(beam_id) + '.txt', 'ab') as bm:\n",
        "                        np.savetxt(bm, best_state, fmt='%.5f', delimiter=',')\n",
        "                else:\n",
        "                    np.savetxt('pfs/pf_' + str(beam_id) + '.txt', iter_id, fmt='%d', delimiter='\\n')\n",
        "                    with open('pfs/pf_' + str(beam_id) + '.txt', 'ab') as bm:\n",
        "                        np.savetxt(bm, best_state, fmt='%.5f', delimiter=',')\n",
        "\n",
        "            # plotting\n",
        "            # ax.plot(range(iteration), real_time_perf_true[:iteration], '-k', alpha=0.7, label='DRL Performance')\n",
        "            # ax.plot(range(iteration), real_time_perf_n_true[:iteration], '-b', alpha=0.3, label='Noise Performance')\n",
        "            # # ax.plot(range(iteration), np.ones(iteration) * options['target'], '--m', alpha=1.0, label='Victory Claimed')\n",
        "            # # ax.set_xscale('log')\n",
        "            # ax.grid(True)\n",
        "            # ax.legend()\n",
        "            # plt.draw()\n",
        "            # plt.pause(0.001)\n",
        "            # ax.cla()\n",
        "\n",
        "            print(\n",
        "                \"Tr: %.2f,Beam: %d, Iteration: %d, Q value: %.4f, Reward: %.4f, BF Gain pred: %.2f, BF Gain: %.2f, BFtrue Gain pred: %.2f, BFtrue Gain: %.2f,Critic Loss: %.2f, Policy Loss: %.2f\" % \\\n",
        "                (training_size,beam_id, train_options['overall_iter'],\n",
        "                 torch.Tensor.cpu(q_pred1.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(reward_pred).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(bf_gain_pred.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(bf_gain_pred_true.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(bf_gain_true.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(critic_loss1.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(actor_loss1.detach()).numpy().squeeze()))\n",
        "\n",
        "        # Training Communication Interface\n",
        "        train_options['replay_memory'] = replay_memory  # used for the next loop\n",
        "        train_options['state'] = state  # used for the next loop\n",
        "        train_options['best_state'] = CB_Env_1.best_bf_vec  # used for clustering and assignment\n",
        "\n",
        "        file_name = '/content/drive/MyDrive/Colab_Notebooks/rl_results/rl_result_'+trsize +'_' + str(exp_id) + '.mat'\n",
        "        scio.savemat(file_name,\n",
        "                     {'agent': real_time_perf_true,\n",
        "                      'noise': real_time_perf_n_true,\n",
        "                      'agent_pred': real_time_perf,\n",
        "                      'noise_pred': real_time_perf_n})\n",
        "\n",
        "    return train_options\n",
        "\n",
        "\n",
        "def BF_GAIN_signal(irs_phase,  ch_bs_irs, ch_signal ):\n",
        "\n",
        "    Theta = torch.exp(1j*irs_phase.reshape(1,-1)).cuda()\n",
        "    B = Theta.reshape(-1,1) * ch_bs_irs.reshape(-1,1)\n",
        "    H = ch_signal.cuda().conj().reshape(1,-1) @ B\n",
        "    #H_h = H.conj().T  # Hermitian\n",
        "    #f = torch.linalg.pinv(H_h @ H) @ H_h  # ZF precoding\n",
        "    #print(\"H and f\", H.shape, f.shape)\n",
        "    #print(\"h_interference\", h_interference, h_interference.shape)\n",
        "    #print(h for h in h_interference)\n",
        "    #signal_power = torch.abs(H @ f).pow(2).sum()  # Power of desired signal\n",
        "\n",
        "    #interference_power = torch.abs(h_interference[:,1] @ f).pow(2).sum() + torch.abs(h_interference[:,2] @ f).pow(2).sum() # Power of interference\n",
        "\n",
        "    #sir = signal_power / (interference_power)\n",
        "\n",
        "    channel_gain = 10*torch.log10(torch.abs(H)**2)\n",
        "\n",
        "    return channel_gain\n",
        "\n",
        "def BF_GAIN_interf(irs_phase,  ch_bs_irs1,ch_bs_irs2,  ch_signal ):\n",
        "\n",
        "    Theta = torch.exp(1j*irs_phase.reshape(1,-1)).cuda()\n",
        "    B1 = Theta.reshape(-1,1) * ch_bs_irs1.reshape(-1,1)\n",
        "    H1 = ch_signal.cuda().conj().reshape(1,-1) @ B1\n",
        "\n",
        "    B2 = Theta.reshape(-1,1) * ch_bs_irs2.reshape(-1,1)\n",
        "    H2 = ch_signal.cuda().conj().reshape(1,-1) @ B2\n",
        "    channel_gain = 10*torch.log10(torch.abs(H1)**2+torch.abs(H2)**2)\n",
        "\n",
        "    return channel_gain\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class envCB_1:\n",
        "\n",
        "    def __init__(self, num_ant, num_bits, idx, options, model_signal, exp_idx):\n",
        "\n",
        "        self.idx = idx\n",
        "        self.exp_idx = exp_idx\n",
        "        self.num_ant = num_ant\n",
        "        self.num_bits = num_bits\n",
        "        self.cb_size = 2 ** self.num_bits\n",
        "        self.codebook = self.codebook_gen()\n",
        "\n",
        "\n",
        "        self.state = torch.zeros((1, self.num_ant)).float().cuda()\n",
        "        self.bf_vec = self.init_bf_vec()\n",
        "        self.previous_gain = 0\n",
        "        self.previous_gain_pred = 0\n",
        "        self.th_step = 0.01\n",
        "        self.threshold = torch.tensor([-100]).float().cuda()\n",
        "        self.count = 1\n",
        "        self.record_freq = 10\n",
        "        self.record_decay_th = 1000\n",
        "        self.achievement = torch.tensor([0]).float().cuda()\n",
        "        self.gain_record = [np.array(-100.)]\n",
        "        self.N_count = 1\n",
        "        self.best_bf_vec = self.init_best()\n",
        "        # self.opt_bf_gain()\n",
        "        self.options = options\n",
        "        self.prefix = torch.tensor(options['ch_t3'][:5]).float().cuda()\n",
        "        self.model_signal = model_signal\n",
        "\n",
        "        azim_min = -148\n",
        "        azim_max = -25\n",
        "        elev_min = -58\n",
        "        elev_max = -17\n",
        "        dist_min = 10\n",
        "        dist_max = 29\n",
        "        x_min = 35\n",
        "        x_max = 65\n",
        "        y_min = -25\n",
        "        y_max = -5\n",
        "\n",
        "        self.prefix[0] = (self.prefix[0] - x_min)/(x_max- x_min)\n",
        "        self.prefix[1] = (self.prefix[1] - y_min)/(y_max- y_min)\n",
        "        self.prefix[2] = (self.prefix[2] - azim_min)/(azim_max- azim_min)\n",
        "        self.prefix[3] = (self.prefix[3] - elev_min)/(elev_max- elev_min)\n",
        "        self.prefix[4] = (self.prefix[4] - dist_min)/(dist_max- dist_min)\n",
        "\n",
        "    def step(self, input_action):  # input_action: (1, num_ant), rep: phase vector\n",
        "        self.state = input_action\n",
        "        reward, bf_gain = self.reward_fn()\n",
        "        terminal = 0\n",
        "        return self.state.clone(), reward, bf_gain, terminal\n",
        "\n",
        "    def reward_fn(self):\n",
        "        y_min = -70\n",
        "        y_max = 37\n",
        "        state_temp = self.state\n",
        "        state_temp = state_temp.reshape(2,-1)\n",
        "\n",
        "        bf_gain = self.model_signal(torch.concat((self.prefix.reshape(1,-1),state_temp.reshape(1,-1)),1))\n",
        "        #print(\"bf_gain1\", bf_gain1, bf_gain1.shape)\n",
        "\n",
        "        if bf_gain > self.previous_gain:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif(self.state, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        else:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif(self.state, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        self.previous_gain = self.previous_gain_pred\n",
        "        return reward, bf_gain\n",
        "\n",
        "    def get_reward(self, input_action):\n",
        "        y_min = -70\n",
        "        y_max = 37\n",
        "\n",
        "        inner_state = input_action\n",
        "\n",
        "        self.options['ph_table_rep-1'] = self.options['ph_table'].repeat(self.num_ant, 1)\n",
        "        mat_dist = torch.abs(inner_state.reshape(-1, 1) - self.options['ph_table_rep-1'])\n",
        "        # Quantization Processing\n",
        "        # self.options['ph_table_rep'].cuda()\n",
        "        action_quant = self.options['ph_table_rep-1'][range(self.num_ant), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
        "        #print(\"action quant in reward\", action_quant.shape)\n",
        "        print(self.prefix.shape, action_quant.shape)\n",
        "        bf_gain = self.model_signal(torch.cat((self.prefix.reshape(1,-1),action_quant.reshape(1,-1)),1))\n",
        "        #print(\"bf_gain1\", bf_gain1, bf_gain1.shape)\n",
        "\n",
        "\n",
        "        if bf_gain > self.previous_gain_pred:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif_get_reward(action_quant, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        else:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif_get_reward(action_quant, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        # if self.count % self.record_freq == 0:\n",
        "        #     self.gain_vs_iter()\n",
        "        #     if self.count == self.record_decay_th:\n",
        "        #         self.record_freq = 1000\n",
        "        self.previous_gain_pred = bf_gain\n",
        "        self.count += 1\n",
        "        return reward, bf_gain, (action_quant.clone()).reshape(1,-1),(action_quant.clone()).reshape(1,-1)\n",
        "\n",
        "    def threshold_modif(self, ph_vec, bf_gain):\n",
        "        self.achievement = bf_gain\n",
        "        self.gain_recording(ph_vec, self.idx)\n",
        "        # self.threshold += self.th_step\n",
        "        self.threshold = bf_gain\n",
        "        return self.threshold\n",
        "\n",
        "    def threshold_modif_get_reward(self, inner_bf, bf_gain):\n",
        "        self.achievement = bf_gain\n",
        "        self.gain_recording(inner_bf, self.idx)\n",
        "        # self.threshold += self.th_step\n",
        "        self.threshold = bf_gain\n",
        "        return self.threshold\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def gain_recording(self, bf_vec, idx):\n",
        "        new_gain = torch.Tensor.cpu(self.achievement).detach().numpy().reshape((1, 1))\n",
        "        bf_print = torch.Tensor.cpu(bf_vec).detach().numpy().reshape(1, -1)\n",
        "        if new_gain > max(self.gain_record):\n",
        "            self.gain_record.append(new_gain)\n",
        "            self.best_bf_vec = torch.Tensor.cpu(bf_vec).detach().numpy().reshape(1, -1)\n",
        "            if os.path.exists('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt'):\n",
        "                with open('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', 'ab') as bm:\n",
        "                    np.savetxt(bm, new_gain, fmt='%.2f', delimiter='\\n')\n",
        "                with open('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', 'ab') as bm:\n",
        "                    np.savetxt(bm, bf_print, fmt='%.5f', delimiter=',')\n",
        "            else:\n",
        "                np.savetxt('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', new_gain, fmt='%.2f', delimiter='\\n')\n",
        "                # with open('beams/beams_' + str(idx) + '_max.txt', 'ab') as bm:\n",
        "                #     np.savetxt(bm, new_gain, fmt='%.2f', delimiter='\\n')\n",
        "                with open('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', 'ab') as bm:\n",
        "                    np.savetxt(bm, bf_print, fmt='%.5f', delimiter=',')\n",
        "\n",
        "    def codebook_gen(self):\n",
        "        angles = np.linspace(0, 2 * np.pi, self.cb_size, endpoint=False)\n",
        "        cb = np.exp(1j * angles)\n",
        "        codebook = torch.zeros((self.cb_size, 2))  # shape of the codebook\n",
        "        for ii in range(cb.shape[0]):\n",
        "            codebook[ii, 0] = torch.tensor(np.real(cb[ii]))\n",
        "            codebook[ii, 1] = torch.tensor(np.imag(cb[ii]))\n",
        "        return codebook\n",
        "\n",
        "    def init_bf_vec(self):\n",
        "        bf_vec = torch.empty((1, 2 * self.num_ant))\n",
        "        bf_vec[0, ::2] = torch.tensor([1])\n",
        "        bf_vec[0, 1::2] = torch.tensor([0])\n",
        "        bf_vec = bf_vec.float().cuda()\n",
        "        return bf_vec\n",
        "\n",
        "    def init_best(self):\n",
        "        ph_book = np.linspace(-np.pi, np.pi, 2 ** self.num_bits, endpoint=False)\n",
        "        ph_vec = np.array([[ph_book[np.random.randint(0, len(ph_book))] for ii in range(self.num_ant)]])\n",
        "        bf_complex = np.exp(1j * ph_vec)\n",
        "        bf_vec = np.empty((1, 2 * self.num_ant))\n",
        "        for kk in range(self.num_ant):\n",
        "            bf_vec[0, 2 * kk] = np.real(bf_complex[0, kk])\n",
        "            bf_vec[0, 2 * kk + 1] = np.imag(bf_complex[0, kk])\n",
        "        return bf_vec\n",
        "\n",
        "\n",
        "class GainPred_Dense(nn.Module):\n",
        "\n",
        "    def __init__(self, M):\n",
        "        super(GainPred_Dense, self).__init__()\n",
        "        self.M = 36\n",
        "        self.input = 77\n",
        "        self.J = 8*self.input\n",
        "        self.K = 6*self.input\n",
        "        self.L = 4*self.input\n",
        "        self.N = 2*self.input\n",
        "\n",
        "        self.fc1 = nn.Linear( self.input, self.J) # 77 616\n",
        "        nn.init.xavier_normal_(self.fc1.weight)\n",
        "        self.bn1 = nn.BatchNorm1d(self.J)\n",
        "        self.dropout = nn.Dropout(0.001)\n",
        "        self.fc2 = nn.Linear(self.J, self.K) # 616 462\n",
        "        nn.init.xavier_normal_(self.fc2.weight)\n",
        "        self.bn2 = nn.BatchNorm1d(self.K)\n",
        "        self.fc3 = nn.Linear(self.K, self.K) # 462 462\n",
        "        self.bn3 = nn.BatchNorm1d(self.K)\n",
        "        self.fc4 = nn.Linear(self.K, self.L) # 462 308\n",
        "        self.bn4 = nn.BatchNorm1d(self.L)\n",
        "        self.fc5 = nn.Linear(self.L, self.L) # 308 308\n",
        "        self.bn5 = nn.BatchNorm1d(self.L)\n",
        "        self.fc6 = nn.Linear(self.L, self.N) # 308 144\n",
        "        self.bn6 = nn.BatchNorm1d(self.N)\n",
        "        self.fc7 = nn.Linear(self.N, 1) #144 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, ph):\n",
        "\n",
        "        device = ph.device\n",
        "        bf_vec = self.phase2bf(ph[:,5:]).cuda()\n",
        "        bf_vec_r, bf_vec_i = bf_vec[:, :self.M], bf_vec[:, self.M:]\n",
        "        bf_vec_cat = torch.cat((bf_vec_r, bf_vec_i), dim=1)\n",
        "        bf_vec = torch.cat((ph[:,:5].real,bf_vec_cat ),1)\n",
        "        x = torch.relu(self.bn1(self.fc1(bf_vec)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn4(self.fc4(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn5(self.fc5(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn6(self.fc6(x)))\n",
        "        x= self.dropout(x)\n",
        "        y_min = -90\n",
        "        y_max = 35\n",
        "        out = torch.sigmoid(self.fc7(x))*(y_max-y_min)+y_min\n",
        "        return out\n",
        "\n",
        "    def phase2bf(self, ph_mat):\n",
        "\n",
        "        bf_mat = torch.exp(1j * ph_mat)\n",
        "        bf_mat_r = torch.real(bf_mat)\n",
        "        bf_mat_i = torch.imag(bf_mat)\n",
        "\n",
        "\n",
        "        bf_mat_ = torch.cat((bf_mat_r, bf_mat_i), dim=1)\n",
        "\n",
        "        return bf_mat_\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "#import argparse\n",
        "import scipy.io as scio\n",
        "#from train_ddpg import train\n",
        "import random\n",
        "\n",
        "training_size = [5, 10]\n",
        "for tr_size in training_size:\n",
        "  for i in range(10,25):\n",
        "\n",
        "    print(\"main\")\n",
        "    #parser = argparse.ArgumentParser()\n",
        "\n",
        "    #parser.add_argument(dest='exp_id')\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    options = {\n",
        "        'gpu_idx': 0,\n",
        "        'num_ant': 36,\n",
        "        'num_mat': 36,\n",
        "        'num_bits': 2,\n",
        "        'pf_print': 100,\n",
        "\n",
        "        'path_target': 'drive/MyDrive/Colab_Notebooks/Learning_in_DT_environment/RL_training.mat' ,       #user2\n",
        "\n",
        "        'best_value': torch.tensor(-30).cuda(),\n",
        "\n",
        "        'best_state': None,\n",
        "        'noise' : -90,\n",
        "        'save_freq': 10000\n",
        "\n",
        "    }\n",
        "\n",
        "    train_opt = {\n",
        "        'state': 0,\n",
        "        'best_state': 0,\n",
        "        'num_iter': 10000,\n",
        "        'tau': 1e-1,\n",
        "        'overall_iter': 1,\n",
        "        'replay_memory': [],\n",
        "        'replay_memory_size': 100000,\n",
        "        'minibatch_size': 512,\n",
        "        'gamma': 0.95\n",
        "    }\n",
        "\n",
        "    if not os.path.exists('beams/'):\n",
        "        os.mkdir('beams/')\n",
        "\n",
        "    if not os.path.exists('pfs/'):\n",
        "        os.mkdir('pfs/')\n",
        "\n",
        "    ch_t3 = scio.loadmat(options['path_target'])['HR']\n",
        "    ch_BS = scio.loadmat(options['path_target'])['Gs']\n",
        "    ch_BI = scio.loadmat(options['path_target'])['G1']\n",
        "    ch_BI2 = scio.loadmat(options['path_target'])['G2']\n",
        "\n",
        "\n",
        "    options['ch_bs_irs'] = ch_BS\n",
        "    options['ch_bs_irs_i'] = ch_BI\n",
        "    options['ch_bs_irs_i2'] = ch_BI2\n",
        "\n",
        "    options['ch_t3'] = ch_t3\n",
        "\n",
        "    # Quantization settings\n",
        "    options['num_ph'] = 2 ** options['num_bits']\n",
        "    options['multi_step'] = torch.from_numpy(\n",
        "        np.linspace(int(-(options['num_ph'] - 2) / 2),\n",
        "                    int(options['num_ph'] / 2),\n",
        "                    num=options['num_ph'],\n",
        "                    endpoint=True)).type(dtype=torch.float32).reshape(1, -1)\n",
        "\n",
        "    options['pi'] = torch.tensor(np.pi)\n",
        "    options['ph_table'] = (2 * options['pi']) / options['num_ph'] * options['multi_step']\n",
        "    options['ph_table_rep'] = options['ph_table'].repeat(options['num_ant'], 1)\n",
        "    options['ph_table_rep-1'] = options['ph_table'].repeat(2*options['num_ant'], 1)\n",
        "    options['best_bf_pred_comb']  = float('-inf');\n",
        "    options['best_bf_comb']  = float('-inf');\n",
        "\n",
        "    options['previous_gain_pred']  = float('-inf');\n",
        "    options['previous_gain']  = float('-inf');\n",
        "    options['previous_diff_pred']  = float('inf');\n",
        "    options['previous_diff']  = float('inf');\n",
        "    print(\"options\", len(options))\n",
        "\n",
        "    train(options, train_opt, int(i), int(i), tr_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5xLR5aFgLiD",
        "outputId": "52864ba4-37c2-467a-90d4-c4d6f52bc7e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "main\n",
            "options 26\n",
            "Beam 0 training begins. GPU being used: 0\n",
            "Initial State Activated.\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 2\n",
            "Beam: 0, Iteration: 2, Q value: 0.0103, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -23.18, Critic Loss: 1.01, Policy Loss: -0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 4\n",
            "Beam: 0, Iteration: 3, Q value: 0.0082, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -2.65, Critic Loss: 1.00, Policy Loss: -0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 6\n",
            "Beam: 0, Iteration: 4, Q value: 0.0065, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -9.69, Critic Loss: 2.31, Policy Loss: -0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 8\n",
            "Beam: 0, Iteration: 5, Q value: 0.0047, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -9.52, Critic Loss: 2.94, Policy Loss: -0.00\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 10\n",
            "Beam: 0, Iteration: 6, Q value: 0.0027, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -10.11, Critic Loss: 3.30, Policy Loss: -0.00\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 12\n",
            "Beam: 0, Iteration: 7, Q value: 0.0005, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -6.34, Critic Loss: 2.87, Policy Loss: 0.00\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 14\n",
            "Beam: 0, Iteration: 8, Q value: -0.0018, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -2.15, Critic Loss: 2.56, Policy Loss: 0.00\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 16\n",
            "Beam: 0, Iteration: 9, Q value: -0.0040, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -4.29, Critic Loss: 2.33, Policy Loss: 0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 18\n",
            "Beam: 0, Iteration: 10, Q value: -0.0062, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -8.10, Critic Loss: 2.59, Policy Loss: 0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 20\n",
            "Beam: 0, Iteration: 11, Q value: -0.0085, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -13.80, Critic Loss: 2.78, Policy Loss: 0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 22\n",
            "Beam: 0, Iteration: 12, Q value: -0.0109, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -13.12, Critic Loss: 2.94, Policy Loss: 0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 24\n",
            "Beam: 0, Iteration: 13, Q value: -0.0133, Reward: -1.0000, BF Gain pred: -6.77, BF Gain: -12.92, Critic Loss: 3.06, Policy Loss: 0.01\n",
            "Best gain pred -inf Best gain -inf\n",
            "Best gain pred and | length 0\n",
            "Length of the buffer: 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-7316d7f62fa0>:322: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  real_time_perf[iteration] = torch.Tensor.cpu(bf_gain_pred.detach()).numpy()\n",
            "<ipython-input-2-7316d7f62fa0>:323: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  real_time_perf_n[iteration] = torch.Tensor.cpu(bf_gain.detach()).numpy()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Length of the buffer: 21616\n",
            "Beam: 0, Iteration: 9255, Q value: 0.0320, Reward: 1.0000, BF Gain pred: 8.66, BF Gain: 12.06, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21622\n",
            "Beam: 0, Iteration: 9256, Q value: -0.0521, Reward: -1.0000, BF Gain pred: 5.48, BF Gain: 8.90, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21628\n",
            "Beam: 0, Iteration: 9257, Q value: 0.9087, Reward: -1.0000, BF Gain pred: 3.19, BF Gain: 3.75, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21634\n",
            "Beam: 0, Iteration: 9258, Q value: 0.9797, Reward: 1.0000, BF Gain pred: 3.99, BF Gain: 6.38, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21640\n",
            "Beam: 0, Iteration: 9259, Q value: 1.1361, Reward: 1.0000, BF Gain pred: 6.32, BF Gain: 5.39, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21646\n",
            "Beam: 0, Iteration: 9260, Q value: 0.5943, Reward: -1.0000, BF Gain pred: 4.30, BF Gain: 3.70, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21652\n",
            "Beam: 0, Iteration: 9261, Q value: 1.0906, Reward: 1.0000, BF Gain pred: 5.50, BF Gain: 2.79, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21658\n",
            "Beam: 0, Iteration: 9262, Q value: 0.3925, Reward: 1.0000, BF Gain pred: 9.58, BF Gain: 12.23, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21664\n",
            "Beam: 0, Iteration: 9263, Q value: -0.2702, Reward: -1.0000, BF Gain pred: 4.51, BF Gain: 7.54, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21670\n",
            "Beam: 0, Iteration: 9264, Q value: 0.8716, Reward: -1.0000, BF Gain pred: 2.85, BF Gain: 2.72, Critic Loss: 0.10, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21676\n",
            "Beam: 0, Iteration: 9265, Q value: 0.0392, Reward: -1.0000, BF Gain pred: 1.62, BF Gain: 2.63, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21682\n",
            "Beam: 0, Iteration: 9266, Q value: -0.0059, Reward: 1.0000, BF Gain pred: 2.15, BF Gain: 0.21, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21684\n",
            "Beam: 0, Iteration: 9267, Q value: 1.0274, Reward: -1.0000, BF Gain pred: 1.26, BF Gain: 5.38, Critic Loss: 0.11, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21690\n",
            "Beam: 0, Iteration: 9268, Q value: 1.0766, Reward: 1.0000, BF Gain pred: 9.12, BF Gain: 7.66, Critic Loss: 0.18, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21696\n",
            "Beam: 0, Iteration: 9269, Q value: -0.5918, Reward: -1.0000, BF Gain pred: 4.82, BF Gain: 4.64, Critic Loss: 0.11, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21702\n",
            "Beam: 0, Iteration: 9270, Q value: 0.8379, Reward: 1.0000, BF Gain pred: 11.10, BF Gain: 5.69, Critic Loss: 0.11, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21708\n",
            "Beam: 0, Iteration: 9271, Q value: 0.8190, Reward: -1.0000, BF Gain pred: 7.08, BF Gain: 4.75, Critic Loss: 0.15, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21714\n",
            "Beam: 0, Iteration: 9272, Q value: -0.0410, Reward: -1.0000, BF Gain pred: 5.71, BF Gain: 4.66, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21720\n",
            "Beam: 0, Iteration: 9273, Q value: 0.3582, Reward: 1.0000, BF Gain pred: 13.56, BF Gain: 7.07, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21726\n",
            "Beam: 0, Iteration: 9274, Q value: 0.6550, Reward: -1.0000, BF Gain pred: 5.87, BF Gain: 3.49, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21732\n",
            "Beam: 0, Iteration: 9275, Q value: 0.4990, Reward: 1.0000, BF Gain pred: 8.63, BF Gain: 10.71, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21738\n",
            "Beam: 0, Iteration: 9276, Q value: 0.6661, Reward: -1.0000, BF Gain pred: 6.61, BF Gain: 2.90, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21744\n",
            "Beam: 0, Iteration: 9277, Q value: 0.9356, Reward: -1.0000, BF Gain pred: 6.08, BF Gain: 2.69, Critic Loss: 0.10, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21750\n",
            "Beam: 0, Iteration: 9278, Q value: 0.6671, Reward: -1.0000, BF Gain pred: -1.28, BF Gain: -3.58, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21756\n",
            "Beam: 0, Iteration: 9279, Q value: 0.8164, Reward: 1.0000, BF Gain pred: 2.70, BF Gain: -2.06, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21762\n",
            "Beam: 0, Iteration: 9280, Q value: 0.8608, Reward: -1.0000, BF Gain pred: 1.95, BF Gain: 2.36, Critic Loss: 0.15, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21764\n",
            "Beam: 0, Iteration: 9281, Q value: 0.0013, Reward: 1.0000, BF Gain pred: 3.97, BF Gain: 4.98, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21770\n",
            "Beam: 0, Iteration: 9282, Q value: 0.2122, Reward: 1.0000, BF Gain pred: 11.35, BF Gain: 0.38, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21776\n",
            "Beam: 0, Iteration: 9283, Q value: 1.1001, Reward: -1.0000, BF Gain pred: 3.38, BF Gain: 3.01, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21782\n",
            "Beam: 0, Iteration: 9284, Q value: -0.0420, Reward: -1.0000, BF Gain pred: -1.75, BF Gain: 1.65, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21788\n",
            "Beam: 0, Iteration: 9285, Q value: 0.9412, Reward: 1.0000, BF Gain pred: 5.85, BF Gain: 6.66, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21794\n",
            "Beam: 0, Iteration: 9286, Q value: 1.2854, Reward: 1.0000, BF Gain pred: 5.93, BF Gain: 5.39, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21800\n",
            "Beam: 0, Iteration: 9287, Q value: 0.9084, Reward: 1.0000, BF Gain pred: 12.97, BF Gain: 4.10, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21806\n",
            "Beam: 0, Iteration: 9288, Q value: 0.8339, Reward: -1.0000, BF Gain pred: 6.53, BF Gain: 5.86, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21812\n",
            "Beam: 0, Iteration: 9289, Q value: 0.7027, Reward: 1.0000, BF Gain pred: 10.51, BF Gain: 6.76, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21818\n",
            "Beam: 0, Iteration: 9290, Q value: 0.0779, Reward: -1.0000, BF Gain pred: -0.38, BF Gain: -1.86, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21824\n",
            "Beam: 0, Iteration: 9291, Q value: -0.0500, Reward: 1.0000, BF Gain pred: 8.47, BF Gain: 11.97, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21830\n",
            "Beam: 0, Iteration: 9292, Q value: 0.2529, Reward: 1.0000, BF Gain pred: 9.87, BF Gain: 4.06, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21836\n",
            "Beam: 0, Iteration: 9293, Q value: 0.4968, Reward: -1.0000, BF Gain pred: -2.00, BF Gain: -2.00, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21842\n",
            "Beam: 0, Iteration: 9294, Q value: 1.0667, Reward: 1.0000, BF Gain pred: 4.81, BF Gain: 11.92, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21848\n",
            "Beam: 0, Iteration: 9295, Q value: 0.2162, Reward: 1.0000, BF Gain pred: 8.89, BF Gain: 5.27, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21854\n",
            "Beam: 0, Iteration: 9296, Q value: 0.7663, Reward: -1.0000, BF Gain pred: 6.50, BF Gain: 5.75, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21860\n",
            "Beam: 0, Iteration: 9297, Q value: 0.1004, Reward: 1.0000, BF Gain pred: 9.28, BF Gain: 4.83, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21866\n",
            "Beam: 0, Iteration: 9298, Q value: 0.5342, Reward: -1.0000, BF Gain pred: 4.37, BF Gain: 4.06, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21868\n",
            "Beam: 0, Iteration: 9299, Q value: 0.0774, Reward: 1.0000, BF Gain pred: 5.68, BF Gain: 8.56, Critic Loss: 0.13, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21874\n",
            "Beam: 0, Iteration: 9300, Q value: -1.4984, Reward: -1.0000, BF Gain pred: 3.55, BF Gain: 1.58, Critic Loss: 0.11, Policy Loss: -1.10\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21880\n",
            "Beam: 0, Iteration: 9301, Q value: -0.4031, Reward: 1.0000, BF Gain pred: 5.88, BF Gain: 5.07, Critic Loss: 0.13, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21886\n",
            "Beam: 0, Iteration: 9302, Q value: -0.2257, Reward: -1.0000, BF Gain pred: 3.98, BF Gain: 5.39, Critic Loss: 0.14, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21892\n",
            "Beam: 0, Iteration: 9303, Q value: 0.1228, Reward: 1.0000, BF Gain pred: 8.84, BF Gain: 4.90, Critic Loss: 0.10, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21898\n",
            "Beam: 0, Iteration: 9304, Q value: -0.1385, Reward: -1.0000, BF Gain pred: 0.48, BF Gain: 1.59, Critic Loss: 0.11, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21904\n",
            "Beam: 0, Iteration: 9305, Q value: 0.7827, Reward: 1.0000, BF Gain pred: 2.45, BF Gain: -0.92, Critic Loss: 0.12, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21910\n",
            "Beam: 0, Iteration: 9306, Q value: 0.5409, Reward: 1.0000, BF Gain pred: 19.08, BF Gain: 9.65, Critic Loss: 0.14, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21916\n",
            "Beam: 0, Iteration: 9307, Q value: -0.4329, Reward: -1.0000, BF Gain pred: 4.87, BF Gain: 0.29, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21922\n",
            "Beam: 0, Iteration: 9308, Q value: -0.1100, Reward: 1.0000, BF Gain pred: 11.16, BF Gain: 6.00, Critic Loss: 0.16, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21924\n",
            "Beam: 0, Iteration: 9309, Q value: -1.8425, Reward: -1.0000, BF Gain pred: -6.26, BF Gain: -1.69, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21930\n",
            "Beam: 0, Iteration: 9310, Q value: -0.3485, Reward: 1.0000, BF Gain pred: 3.03, BF Gain: 5.31, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21936\n",
            "Beam: 0, Iteration: 9311, Q value: -0.0217, Reward: 1.0000, BF Gain pred: 13.52, BF Gain: 2.34, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21942\n",
            "Beam: 0, Iteration: 9312, Q value: 0.0169, Reward: -1.0000, BF Gain pred: 3.25, BF Gain: 2.15, Critic Loss: 0.14, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21948\n",
            "Beam: 0, Iteration: 9313, Q value: 0.5274, Reward: -1.0000, BF Gain pred: 0.57, BF Gain: 0.20, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21954\n",
            "Beam: 0, Iteration: 9314, Q value: 0.6180, Reward: 1.0000, BF Gain pred: 12.72, BF Gain: 5.42, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21960\n",
            "Beam: 0, Iteration: 9315, Q value: -0.4236, Reward: -1.0000, BF Gain pred: 9.71, BF Gain: 9.76, Critic Loss: 0.11, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21966\n",
            "Beam: 0, Iteration: 9316, Q value: 0.0475, Reward: 1.0000, BF Gain pred: 15.17, BF Gain: 12.48, Critic Loss: 0.14, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21972\n",
            "Beam: 0, Iteration: 9317, Q value: -0.8534, Reward: -1.0000, BF Gain pred: -1.17, BF Gain: -1.17, Critic Loss: 0.16, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21974\n",
            "Beam: 0, Iteration: 9318, Q value: 0.9705, Reward: 1.0000, BF Gain pred: 14.86, BF Gain: 13.59, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21980\n",
            "Beam: 0, Iteration: 9319, Q value: 0.2950, Reward: -1.0000, BF Gain pred: 2.13, BF Gain: 2.27, Critic Loss: 0.15, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21986\n",
            "Beam: 0, Iteration: 9320, Q value: -0.1095, Reward: 1.0000, BF Gain pred: 7.64, BF Gain: 6.10, Critic Loss: 0.13, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21992\n",
            "Beam: 0, Iteration: 9321, Q value: 0.8136, Reward: -1.0000, BF Gain pred: 3.93, BF Gain: 11.22, Critic Loss: 0.11, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 21998\n",
            "Beam: 0, Iteration: 9322, Q value: 0.3364, Reward: -1.0000, BF Gain pred: 1.16, BF Gain: -4.37, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22004\n",
            "Beam: 0, Iteration: 9323, Q value: -0.2419, Reward: 1.0000, BF Gain pred: 9.82, BF Gain: 6.05, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22010\n",
            "Beam: 0, Iteration: 9324, Q value: 0.8337, Reward: -1.0000, BF Gain pred: 3.31, BF Gain: -0.08, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22016\n",
            "Beam: 0, Iteration: 9325, Q value: -0.4376, Reward: -1.0000, BF Gain pred: 2.11, BF Gain: 1.69, Critic Loss: 0.15, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22018\n",
            "Beam: 0, Iteration: 9326, Q value: 1.2242, Reward: 1.0000, BF Gain pred: 9.13, BF Gain: 6.13, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22024\n",
            "Beam: 0, Iteration: 9327, Q value: 0.6960, Reward: -1.0000, BF Gain pred: 3.07, BF Gain: 2.80, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22030\n",
            "Beam: 0, Iteration: 9328, Q value: 0.9692, Reward: 1.0000, BF Gain pred: 7.55, BF Gain: 6.64, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22036\n",
            "Beam: 0, Iteration: 9329, Q value: 0.8384, Reward: -1.0000, BF Gain pred: 6.54, BF Gain: -0.76, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22042\n",
            "Beam: 0, Iteration: 9330, Q value: 0.8381, Reward: -1.0000, BF Gain pred: 1.35, BF Gain: 6.69, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22048\n",
            "Beam: 0, Iteration: 9331, Q value: 1.2723, Reward: 1.0000, BF Gain pred: 4.02, BF Gain: 1.94, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22054\n",
            "Beam: 0, Iteration: 9332, Q value: 0.7485, Reward: -1.0000, BF Gain pred: 4.01, BF Gain: 0.92, Critic Loss: 0.15, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22060\n",
            "Beam: 0, Iteration: 9333, Q value: 0.2244, Reward: 1.0000, BF Gain pred: 11.14, BF Gain: 8.11, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22066\n",
            "Beam: 0, Iteration: 9334, Q value: -0.1843, Reward: -1.0000, BF Gain pred: 4.75, BF Gain: 9.23, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22072\n",
            "Beam: 0, Iteration: 9335, Q value: 0.3438, Reward: -1.0000, BF Gain pred: 3.63, BF Gain: 1.74, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22078\n",
            "Beam: 0, Iteration: 9336, Q value: 0.4991, Reward: -1.0000, BF Gain pred: 2.03, BF Gain: -2.64, Critic Loss: 0.11, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22084\n",
            "Beam: 0, Iteration: 9337, Q value: 1.1041, Reward: -1.0000, BF Gain pred: -1.80, BF Gain: -1.41, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22090\n",
            "Beam: 0, Iteration: 9338, Q value: 0.7899, Reward: 1.0000, BF Gain pred: 0.56, BF Gain: 0.75, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22096\n",
            "Beam: 0, Iteration: 9339, Q value: 0.3915, Reward: 1.0000, BF Gain pred: 14.08, BF Gain: 7.31, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22102\n",
            "Beam: 0, Iteration: 9340, Q value: -0.0712, Reward: -1.0000, BF Gain pred: 1.69, BF Gain: 4.43, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22108\n",
            "Beam: 0, Iteration: 9341, Q value: 1.2610, Reward: 1.0000, BF Gain pred: 8.47, BF Gain: 5.89, Critic Loss: 0.11, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22114\n",
            "Beam: 0, Iteration: 9342, Q value: 0.8524, Reward: -1.0000, BF Gain pred: 7.30, BF Gain: 5.73, Critic Loss: 0.15, Policy Loss: -1.07\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22120\n",
            "Beam: 0, Iteration: 9343, Q value: -0.2958, Reward: 1.0000, BF Gain pred: 7.76, BF Gain: 8.29, Critic Loss: 0.12, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22126\n",
            "Beam: 0, Iteration: 9344, Q value: 0.2944, Reward: -1.0000, BF Gain pred: -2.90, BF Gain: -1.89, Critic Loss: 0.13, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22132\n",
            "Beam: 0, Iteration: 9345, Q value: 0.3930, Reward: 1.0000, BF Gain pred: 8.22, BF Gain: 20.02, Critic Loss: 0.14, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22138\n",
            "Beam: 0, Iteration: 9346, Q value: 0.9870, Reward: -1.0000, BF Gain pred: 6.35, BF Gain: 9.89, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22144\n",
            "Beam: 0, Iteration: 9347, Q value: -0.0749, Reward: 1.0000, BF Gain pred: 10.64, BF Gain: 16.48, Critic Loss: 0.15, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22150\n",
            "Beam: 0, Iteration: 9348, Q value: -0.4252, Reward: -1.0000, BF Gain pred: 10.03, BF Gain: 6.00, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22156\n",
            "Beam: 0, Iteration: 9349, Q value: 1.2856, Reward: -1.0000, BF Gain pred: 3.33, BF Gain: 1.46, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22162\n",
            "Beam: 0, Iteration: 9350, Q value: 1.0418, Reward: -1.0000, BF Gain pred: 2.89, BF Gain: 3.63, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22168\n",
            "Beam: 0, Iteration: 9351, Q value: 0.2205, Reward: 1.0000, BF Gain pred: 7.32, BF Gain: 8.84, Critic Loss: 0.11, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22174\n",
            "Beam: 0, Iteration: 9352, Q value: 0.5245, Reward: -1.0000, BF Gain pred: 1.22, BF Gain: 2.20, Critic Loss: 0.12, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22180\n",
            "Beam: 0, Iteration: 9353, Q value: 0.7979, Reward: 1.0000, BF Gain pred: 1.59, BF Gain: 5.06, Critic Loss: 0.11, Policy Loss: -1.10\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22186\n",
            "Beam: 0, Iteration: 9354, Q value: 0.2088, Reward: 1.0000, BF Gain pred: 2.15, BF Gain: 1.21, Critic Loss: 0.12, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22192\n",
            "Beam: 0, Iteration: 9355, Q value: 1.1361, Reward: -1.0000, BF Gain pred: -2.93, BF Gain: -1.37, Critic Loss: 0.15, Policy Loss: -1.11\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22194\n",
            "Beam: 0, Iteration: 9356, Q value: 0.5815, Reward: 1.0000, BF Gain pred: 0.78, BF Gain: 0.00, Critic Loss: 0.14, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22200\n",
            "Beam: 0, Iteration: 9357, Q value: 1.1638, Reward: 1.0000, BF Gain pred: 9.37, BF Gain: 6.09, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22206\n",
            "Beam: 0, Iteration: 9358, Q value: 0.1568, Reward: -1.0000, BF Gain pred: -1.04, BF Gain: 0.91, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22208\n",
            "Beam: 0, Iteration: 9359, Q value: 0.7661, Reward: 1.0000, BF Gain pred: 18.66, BF Gain: 9.72, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22214\n",
            "Beam: 0, Iteration: 9360, Q value: 0.5225, Reward: -1.0000, BF Gain pred: 4.03, BF Gain: 4.00, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22220\n",
            "Beam: 0, Iteration: 9361, Q value: 0.6202, Reward: 1.0000, BF Gain pred: 9.35, BF Gain: 8.42, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22226\n",
            "Beam: 0, Iteration: 9362, Q value: 0.9841, Reward: -1.0000, BF Gain pred: 6.27, BF Gain: 4.53, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22232\n",
            "Beam: 0, Iteration: 9363, Q value: 0.4567, Reward: 1.0000, BF Gain pred: 15.72, BF Gain: 14.98, Critic Loss: 0.13, Policy Loss: -1.12\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22234\n",
            "Beam: 0, Iteration: 9364, Q value: 0.1402, Reward: -1.0000, BF Gain pred: 8.42, BF Gain: 9.59, Critic Loss: 0.13, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22240\n",
            "Beam: 0, Iteration: 9365, Q value: -0.2957, Reward: -1.0000, BF Gain pred: 5.05, BF Gain: 6.92, Critic Loss: 0.11, Policy Loss: -1.08\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22246\n",
            "Beam: 0, Iteration: 9366, Q value: 0.2850, Reward: -1.0000, BF Gain pred: 4.10, BF Gain: 3.58, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22252\n",
            "Beam: 0, Iteration: 9367, Q value: -0.1056, Reward: 1.0000, BF Gain pred: 5.15, BF Gain: 4.88, Critic Loss: 0.15, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22258\n",
            "Beam: 0, Iteration: 9368, Q value: 0.9999, Reward: 1.0000, BF Gain pred: 9.68, BF Gain: 8.34, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22264\n",
            "Beam: 0, Iteration: 9369, Q value: 0.0101, Reward: 1.0000, BF Gain pred: 11.01, BF Gain: 11.88, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22270\n",
            "Beam: 0, Iteration: 9370, Q value: 0.2065, Reward: -1.0000, BF Gain pred: 3.90, BF Gain: 3.15, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22276\n",
            "Beam: 0, Iteration: 9371, Q value: 0.0496, Reward: -1.0000, BF Gain pred: 0.64, BF Gain: -0.41, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22282\n",
            "Beam: 0, Iteration: 9372, Q value: 0.9554, Reward: 1.0000, BF Gain pred: 10.78, BF Gain: 12.52, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22288\n",
            "Beam: 0, Iteration: 9373, Q value: -0.2403, Reward: -1.0000, BF Gain pred: 1.22, BF Gain: -2.85, Critic Loss: 0.09, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22294\n",
            "Beam: 0, Iteration: 9374, Q value: 0.9719, Reward: 1.0000, BF Gain pred: 7.85, BF Gain: 7.61, Critic Loss: 0.09, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22300\n",
            "Beam: 0, Iteration: 9375, Q value: 0.5231, Reward: 1.0000, BF Gain pred: 14.19, BF Gain: 11.35, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22306\n",
            "Beam: 0, Iteration: 9376, Q value: -0.1069, Reward: -1.0000, BF Gain pred: 12.33, BF Gain: 3.14, Critic Loss: 0.08, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22308\n",
            "Beam: 0, Iteration: 9377, Q value: 0.3094, Reward: -1.0000, BF Gain pred: 4.04, BF Gain: 3.18, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22314\n",
            "Beam: 0, Iteration: 9378, Q value: 0.0929, Reward: 1.0000, BF Gain pred: 7.37, BF Gain: 8.29, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22320\n",
            "Beam: 0, Iteration: 9379, Q value: 0.7434, Reward: 1.0000, BF Gain pred: 7.56, BF Gain: 2.47, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22326\n",
            "Beam: 0, Iteration: 9380, Q value: 0.4919, Reward: 1.0000, BF Gain pred: 7.81, BF Gain: 6.42, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22328\n",
            "Beam: 0, Iteration: 9381, Q value: 0.6528, Reward: 1.0000, BF Gain pred: 14.24, BF Gain: 4.25, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22334\n",
            "Beam: 0, Iteration: 9382, Q value: -0.3066, Reward: -1.0000, BF Gain pred: 7.77, BF Gain: 11.40, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22340\n",
            "Beam: 0, Iteration: 9383, Q value: 0.5259, Reward: -1.0000, BF Gain pred: 1.99, BF Gain: 6.15, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22346\n",
            "Beam: 0, Iteration: 9384, Q value: -0.5899, Reward: 1.0000, BF Gain pred: 4.90, BF Gain: 4.56, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22352\n",
            "Beam: 0, Iteration: 9385, Q value: 0.7838, Reward: 1.0000, BF Gain pred: 8.94, BF Gain: 5.19, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22358\n",
            "Beam: 0, Iteration: 9386, Q value: 0.0663, Reward: -1.0000, BF Gain pred: 5.97, BF Gain: 5.28, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22364\n",
            "Beam: 0, Iteration: 9387, Q value: -0.3402, Reward: -1.0000, BF Gain pred: 4.56, BF Gain: 4.02, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22370\n",
            "Beam: 0, Iteration: 9388, Q value: -0.7988, Reward: -1.0000, BF Gain pred: 3.61, BF Gain: 7.01, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22376\n",
            "Beam: 0, Iteration: 9389, Q value: 0.8805, Reward: 1.0000, BF Gain pred: 3.69, BF Gain: 7.07, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22378\n",
            "Beam: 0, Iteration: 9390, Q value: 0.5904, Reward: 1.0000, BF Gain pred: 4.58, BF Gain: 2.46, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22384\n",
            "Beam: 0, Iteration: 9391, Q value: -0.0731, Reward: -1.0000, BF Gain pred: 2.15, BF Gain: 1.57, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22390\n",
            "Beam: 0, Iteration: 9392, Q value: 0.5692, Reward: -1.0000, BF Gain pred: 0.98, BF Gain: 0.70, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22396\n",
            "Beam: 0, Iteration: 9393, Q value: 0.1655, Reward: 1.0000, BF Gain pred: 7.63, BF Gain: 5.75, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22402\n",
            "Beam: 0, Iteration: 9394, Q value: 0.6956, Reward: 1.0000, BF Gain pred: 9.64, BF Gain: 5.57, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22408\n",
            "Beam: 0, Iteration: 9395, Q value: 0.3570, Reward: -1.0000, BF Gain pred: 5.52, BF Gain: 2.52, Critic Loss: 0.15, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22414\n",
            "Beam: 0, Iteration: 9396, Q value: 0.6952, Reward: 1.0000, BF Gain pred: 11.18, BF Gain: 5.38, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22420\n",
            "Beam: 0, Iteration: 9397, Q value: -0.6749, Reward: 1.0000, BF Gain pred: 14.86, BF Gain: 14.86, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22426\n",
            "Beam: 0, Iteration: 9398, Q value: 0.0254, Reward: -1.0000, BF Gain pred: 11.66, BF Gain: 14.44, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22432\n",
            "Beam: 0, Iteration: 9399, Q value: -0.1492, Reward: -1.0000, BF Gain pred: 6.23, BF Gain: 6.23, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22438\n",
            "Beam: 0, Iteration: 9400, Q value: 0.6741, Reward: -1.0000, BF Gain pred: 4.34, BF Gain: 3.86, Critic Loss: 0.10, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22444\n",
            "Beam: 0, Iteration: 9401, Q value: -0.3608, Reward: -1.0000, BF Gain pred: 3.80, BF Gain: 8.81, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22450\n",
            "Beam: 0, Iteration: 9402, Q value: -0.5874, Reward: -1.0000, BF Gain pred: 2.75, BF Gain: 3.55, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22456\n",
            "Beam: 0, Iteration: 9403, Q value: -0.3556, Reward: 1.0000, BF Gain pred: 7.01, BF Gain: 7.05, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22462\n",
            "Beam: 0, Iteration: 9404, Q value: 0.4647, Reward: 1.0000, BF Gain pred: 8.41, BF Gain: 11.71, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22468\n",
            "Beam: 0, Iteration: 9405, Q value: -0.1269, Reward: -1.0000, BF Gain pred: 4.24, BF Gain: 5.05, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22474\n",
            "Beam: 0, Iteration: 9406, Q value: 1.2894, Reward: -1.0000, BF Gain pred: 2.27, BF Gain: -0.29, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22480\n",
            "Beam: 0, Iteration: 9407, Q value: 0.8480, Reward: 1.0000, BF Gain pred: 6.18, BF Gain: 4.95, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22486\n",
            "Beam: 0, Iteration: 9408, Q value: 0.5797, Reward: -1.0000, BF Gain pred: 1.06, BF Gain: 4.03, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22492\n",
            "Beam: 0, Iteration: 9409, Q value: 0.9890, Reward: 1.0000, BF Gain pred: 7.20, BF Gain: 7.51, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22498\n",
            "Beam: 0, Iteration: 9410, Q value: 0.4883, Reward: -1.0000, BF Gain pred: 2.89, BF Gain: 1.98, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22504\n",
            "Beam: 0, Iteration: 9411, Q value: 0.2774, Reward: 1.0000, BF Gain pred: 11.46, BF Gain: 3.92, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22510\n",
            "Beam: 0, Iteration: 9412, Q value: 0.4551, Reward: -1.0000, BF Gain pred: 7.18, BF Gain: 3.95, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22516\n",
            "Beam: 0, Iteration: 9413, Q value: 0.7838, Reward: -1.0000, BF Gain pred: 3.70, BF Gain: 2.29, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22522\n",
            "Beam: 0, Iteration: 9414, Q value: 0.3285, Reward: -1.0000, BF Gain pred: 0.94, BF Gain: 0.68, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22528\n",
            "Beam: 0, Iteration: 9415, Q value: -0.3704, Reward: 1.0000, BF Gain pred: 2.64, BF Gain: 2.42, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22534\n",
            "Beam: 0, Iteration: 9416, Q value: -0.3932, Reward: 1.0000, BF Gain pred: 5.33, BF Gain: 8.44, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22540\n",
            "Beam: 0, Iteration: 9417, Q value: 0.0434, Reward: -1.0000, BF Gain pred: -2.35, BF Gain: -0.55, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22546\n",
            "Beam: 0, Iteration: 9418, Q value: -0.1061, Reward: 1.0000, BF Gain pred: 1.48, BF Gain: -0.63, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22552\n",
            "Beam: 0, Iteration: 9419, Q value: 0.5087, Reward: -1.0000, BF Gain pred: 0.09, BF Gain: 3.03, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22558\n",
            "Beam: 0, Iteration: 9420, Q value: 0.8789, Reward: 1.0000, BF Gain pred: 3.82, BF Gain: 6.49, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22564\n",
            "Beam: 0, Iteration: 9421, Q value: 1.2892, Reward: -1.0000, BF Gain pred: 2.54, BF Gain: 2.54, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22570\n",
            "Beam: 0, Iteration: 9422, Q value: 0.7621, Reward: -1.0000, BF Gain pred: 1.24, BF Gain: 2.13, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22576\n",
            "Beam: 0, Iteration: 9423, Q value: 0.0772, Reward: 1.0000, BF Gain pred: 2.20, BF Gain: 2.20, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22582\n",
            "Beam: 0, Iteration: 9424, Q value: 0.0362, Reward: 1.0000, BF Gain pred: 11.09, BF Gain: 6.25, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22588\n",
            "Beam: 0, Iteration: 9425, Q value: -0.0924, Reward: -1.0000, BF Gain pred: 1.66, BF Gain: -1.32, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22594\n",
            "Beam: 0, Iteration: 9426, Q value: 0.7327, Reward: 1.0000, BF Gain pred: 8.85, BF Gain: 7.44, Critic Loss: 0.09, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22600\n",
            "Beam: 0, Iteration: 9427, Q value: 0.9066, Reward: 1.0000, BF Gain pred: 10.46, BF Gain: 10.47, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22606\n",
            "Beam: 0, Iteration: 9428, Q value: 0.3922, Reward: 1.0000, BF Gain pred: 12.39, BF Gain: 16.30, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22612\n",
            "Beam: 0, Iteration: 9429, Q value: -0.5889, Reward: -1.0000, BF Gain pred: 5.89, BF Gain: 0.25, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22618\n",
            "Beam: 0, Iteration: 9430, Q value: 0.1373, Reward: -1.0000, BF Gain pred: 0.61, BF Gain: 3.23, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22624\n",
            "Beam: 0, Iteration: 9431, Q value: 1.1972, Reward: 1.0000, BF Gain pred: 1.91, BF Gain: 2.94, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22630\n",
            "Beam: 0, Iteration: 9432, Q value: 0.7931, Reward: 1.0000, BF Gain pred: 6.30, BF Gain: 5.19, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22632\n",
            "Beam: 0, Iteration: 9433, Q value: 0.5000, Reward: -1.0000, BF Gain pred: 3.32, BF Gain: 0.55, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22638\n",
            "Beam: 0, Iteration: 9434, Q value: 0.3689, Reward: -1.0000, BF Gain pred: -1.74, BF Gain: 5.46, Critic Loss: 0.10, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22644\n",
            "Beam: 0, Iteration: 9435, Q value: 0.7498, Reward: 1.0000, BF Gain pred: 6.21, BF Gain: 1.95, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22650\n",
            "Beam: 0, Iteration: 9436, Q value: 0.9235, Reward: 1.0000, BF Gain pred: 15.99, BF Gain: 15.16, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22656\n",
            "Beam: 0, Iteration: 9437, Q value: 0.1203, Reward: -1.0000, BF Gain pred: 3.86, BF Gain: 3.55, Critic Loss: 0.10, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22662\n",
            "Beam: 0, Iteration: 9438, Q value: -0.5170, Reward: 1.0000, BF Gain pred: 9.30, BF Gain: 12.60, Critic Loss: 0.10, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22668\n",
            "Beam: 0, Iteration: 9439, Q value: 0.2765, Reward: -1.0000, BF Gain pred: 3.64, BF Gain: 4.67, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22674\n",
            "Beam: 0, Iteration: 9440, Q value: 0.8766, Reward: 1.0000, BF Gain pred: 8.06, BF Gain: 4.14, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22680\n",
            "Beam: 0, Iteration: 9441, Q value: 0.3941, Reward: -1.0000, BF Gain pred: 2.51, BF Gain: 4.07, Critic Loss: 0.14, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22686\n",
            "Beam: 0, Iteration: 9442, Q value: 0.6951, Reward: 1.0000, BF Gain pred: 11.71, BF Gain: 11.71, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22692\n",
            "Beam: 0, Iteration: 9443, Q value: 0.1885, Reward: -1.0000, BF Gain pred: 7.57, BF Gain: 4.37, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22698\n",
            "Beam: 0, Iteration: 9444, Q value: 0.3258, Reward: -1.0000, BF Gain pred: 5.18, BF Gain: 3.66, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22704\n",
            "Beam: 0, Iteration: 9445, Q value: -0.3102, Reward: -1.0000, BF Gain pred: -0.33, BF Gain: 1.49, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22710\n",
            "Beam: 0, Iteration: 9446, Q value: 0.8813, Reward: 1.0000, BF Gain pred: 6.67, BF Gain: 4.12, Critic Loss: 0.14, Policy Loss: -1.14\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22712\n",
            "Beam: 0, Iteration: 9447, Q value: 0.9639, Reward: -1.0000, BF Gain pred: 2.72, BF Gain: 0.78, Critic Loss: 0.15, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22718\n",
            "Beam: 0, Iteration: 9448, Q value: 0.5365, Reward: 1.0000, BF Gain pred: 4.76, BF Gain: -2.70, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22724\n",
            "Beam: 0, Iteration: 9449, Q value: 0.3940, Reward: -1.0000, BF Gain pred: 1.92, BF Gain: 2.24, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22730\n",
            "Beam: 0, Iteration: 9450, Q value: 0.7214, Reward: 1.0000, BF Gain pred: 3.86, BF Gain: 4.25, Critic Loss: 0.16, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22736\n",
            "Beam: 0, Iteration: 9451, Q value: 1.0257, Reward: -1.0000, BF Gain pred: -0.14, BF Gain: 1.21, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22742\n",
            "Beam: 0, Iteration: 9452, Q value: 1.1636, Reward: 1.0000, BF Gain pred: 9.95, BF Gain: 8.45, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22748\n",
            "Beam: 0, Iteration: 9453, Q value: -0.7254, Reward: -1.0000, BF Gain pred: 1.84, BF Gain: 1.49, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22754\n",
            "Beam: 0, Iteration: 9454, Q value: 0.2456, Reward: 1.0000, BF Gain pred: 7.15, BF Gain: 5.43, Critic Loss: 0.14, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22760\n",
            "Beam: 0, Iteration: 9455, Q value: -0.8782, Reward: -1.0000, BF Gain pred: 6.86, BF Gain: 3.88, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22762\n",
            "Beam: 0, Iteration: 9456, Q value: 1.1669, Reward: -1.0000, BF Gain pred: 4.27, BF Gain: 5.50, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22768\n",
            "Beam: 0, Iteration: 9457, Q value: 1.0916, Reward: -1.0000, BF Gain pred: 3.61, BF Gain: 9.18, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22774\n",
            "Beam: 0, Iteration: 9458, Q value: -0.7829, Reward: -1.0000, BF Gain pred: 1.47, BF Gain: 4.49, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22780\n",
            "Beam: 0, Iteration: 9459, Q value: 1.1154, Reward: 1.0000, BF Gain pred: 8.32, BF Gain: 10.80, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22786\n",
            "Beam: 0, Iteration: 9460, Q value: 0.0871, Reward: -1.0000, BF Gain pred: 3.19, BF Gain: 1.63, Critic Loss: 0.14, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22792\n",
            "Beam: 0, Iteration: 9461, Q value: 0.0696, Reward: 1.0000, BF Gain pred: 4.36, BF Gain: 4.72, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22798\n",
            "Beam: 0, Iteration: 9462, Q value: -0.4153, Reward: 1.0000, BF Gain pred: 4.45, BF Gain: 8.46, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22804\n",
            "Beam: 0, Iteration: 9463, Q value: 0.6918, Reward: -1.0000, BF Gain pred: 2.39, BF Gain: 7.31, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22810\n",
            "Beam: 0, Iteration: 9464, Q value: 0.4468, Reward: -1.0000, BF Gain pred: 0.32, BF Gain: -3.51, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22812\n",
            "Beam: 0, Iteration: 9465, Q value: 0.3299, Reward: 1.0000, BF Gain pred: 9.38, BF Gain: 6.95, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22818\n",
            "Beam: 0, Iteration: 9466, Q value: -0.2644, Reward: -1.0000, BF Gain pred: -8.74, BF Gain: -6.45, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22824\n",
            "Beam: 0, Iteration: 9467, Q value: 0.9191, Reward: 1.0000, BF Gain pred: 6.52, BF Gain: 6.28, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22830\n",
            "Beam: 0, Iteration: 9468, Q value: 0.8602, Reward: 1.0000, BF Gain pred: 8.46, BF Gain: 6.09, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22836\n",
            "Beam: 0, Iteration: 9469, Q value: 0.2377, Reward: -1.0000, BF Gain pred: 5.73, BF Gain: 5.33, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22842\n",
            "Beam: 0, Iteration: 9470, Q value: 1.2916, Reward: -1.0000, BF Gain pred: 1.86, BF Gain: 1.92, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22848\n",
            "Beam: 0, Iteration: 9471, Q value: 0.7810, Reward: 1.0000, BF Gain pred: 7.56, BF Gain: 5.33, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22854\n",
            "Beam: 0, Iteration: 9472, Q value: 0.8584, Reward: -1.0000, BF Gain pred: 0.28, BF Gain: 1.14, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22860\n",
            "Beam: 0, Iteration: 9473, Q value: 0.1860, Reward: 1.0000, BF Gain pred: 4.42, BF Gain: 4.13, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22866\n",
            "Beam: 0, Iteration: 9474, Q value: -0.1632, Reward: -1.0000, BF Gain pred: -2.12, BF Gain: -2.12, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22872\n",
            "Beam: 0, Iteration: 9475, Q value: 0.2247, Reward: 1.0000, BF Gain pred: 6.63, BF Gain: 6.63, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22878\n",
            "Beam: 0, Iteration: 9476, Q value: 0.6682, Reward: 1.0000, BF Gain pred: 8.74, BF Gain: 4.85, Critic Loss: 0.15, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22884\n",
            "Beam: 0, Iteration: 9477, Q value: 0.5178, Reward: 1.0000, BF Gain pred: 9.36, BF Gain: 8.74, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22890\n",
            "Beam: 0, Iteration: 9478, Q value: 0.9962, Reward: -1.0000, BF Gain pred: 2.79, BF Gain: 4.24, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22896\n",
            "Beam: 0, Iteration: 9479, Q value: -0.5967, Reward: 1.0000, BF Gain pred: 7.63, BF Gain: 6.98, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22902\n",
            "Beam: 0, Iteration: 9480, Q value: 1.1452, Reward: -1.0000, BF Gain pred: 5.75, BF Gain: 5.11, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22904\n",
            "Beam: 0, Iteration: 9481, Q value: 0.1864, Reward: 1.0000, BF Gain pred: 7.69, BF Gain: 6.05, Critic Loss: 0.14, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22910\n",
            "Beam: 0, Iteration: 9482, Q value: -0.4216, Reward: -1.0000, BF Gain pred: 5.81, BF Gain: 5.75, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22916\n",
            "Beam: 0, Iteration: 9483, Q value: 0.2995, Reward: -1.0000, BF Gain pred: 0.60, BF Gain: -0.41, Critic Loss: 0.15, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22922\n",
            "Beam: 0, Iteration: 9484, Q value: 1.2911, Reward: 1.0000, BF Gain pred: 6.55, BF Gain: 5.63, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22924\n",
            "Beam: 0, Iteration: 9485, Q value: 0.0481, Reward: -1.0000, BF Gain pred: 2.43, BF Gain: 1.21, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22930\n",
            "Beam: 0, Iteration: 9486, Q value: 0.3709, Reward: 1.0000, BF Gain pred: 6.42, BF Gain: 12.28, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22936\n",
            "Beam: 0, Iteration: 9487, Q value: 0.9401, Reward: 1.0000, BF Gain pred: 9.87, BF Gain: 9.97, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22942\n",
            "Beam: 0, Iteration: 9488, Q value: 0.5915, Reward: -1.0000, BF Gain pred: 0.11, BF Gain: 1.30, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22948\n",
            "Beam: 0, Iteration: 9489, Q value: 0.6773, Reward: 1.0000, BF Gain pred: 3.34, BF Gain: 4.15, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22954\n",
            "Beam: 0, Iteration: 9490, Q value: 0.4239, Reward: 1.0000, BF Gain pred: 7.68, BF Gain: 8.57, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22960\n",
            "Beam: 0, Iteration: 9491, Q value: 0.1784, Reward: -1.0000, BF Gain pred: 1.55, BF Gain: 0.56, Critic Loss: 0.09, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22966\n",
            "Beam: 0, Iteration: 9492, Q value: 0.6844, Reward: 1.0000, BF Gain pred: 4.03, BF Gain: 6.10, Critic Loss: 0.13, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22972\n",
            "Beam: 0, Iteration: 9493, Q value: 0.7536, Reward: 1.0000, BF Gain pred: 7.26, BF Gain: 7.73, Critic Loss: 0.15, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22978\n",
            "Beam: 0, Iteration: 9494, Q value: 0.3148, Reward: 1.0000, BF Gain pred: 8.76, BF Gain: 8.90, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22984\n",
            "Beam: 0, Iteration: 9495, Q value: 0.8246, Reward: -1.0000, BF Gain pred: 4.01, BF Gain: 2.07, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22990\n",
            "Beam: 0, Iteration: 9496, Q value: -0.1816, Reward: -1.0000, BF Gain pred: 3.72, BF Gain: 4.94, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22996\n",
            "Beam: 0, Iteration: 9497, Q value: 0.5147, Reward: 1.0000, BF Gain pred: 8.59, BF Gain: 8.59, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 22998\n",
            "Beam: 0, Iteration: 9498, Q value: -0.0544, Reward: 1.0000, BF Gain pred: 9.31, BF Gain: 9.31, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23004\n",
            "Beam: 0, Iteration: 9499, Q value: 0.0238, Reward: 1.0000, BF Gain pred: 13.41, BF Gain: 9.63, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23010\n",
            "Beam: 0, Iteration: 9500, Q value: -0.4148, Reward: -1.0000, BF Gain pred: 5.39, BF Gain: 7.09, Critic Loss: 0.11, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23016\n",
            "Beam: 0, Iteration: 9501, Q value: 0.3724, Reward: 1.0000, BF Gain pred: 6.22, BF Gain: 6.80, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23022\n",
            "Beam: 0, Iteration: 9502, Q value: 0.5848, Reward: 1.0000, BF Gain pred: 9.66, BF Gain: 6.09, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23024\n",
            "Beam: 0, Iteration: 9503, Q value: 0.1564, Reward: -1.0000, BF Gain pred: 1.79, BF Gain: 4.48, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23026\n",
            "Beam: 0, Iteration: 9504, Q value: 1.0172, Reward: 1.0000, BF Gain pred: 15.61, BF Gain: 14.39, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23032\n",
            "Beam: 0, Iteration: 9505, Q value: 0.1651, Reward: -1.0000, BF Gain pred: 15.12, BF Gain: 11.65, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23038\n",
            "Beam: 0, Iteration: 9506, Q value: -0.7629, Reward: -1.0000, BF Gain pred: 7.62, BF Gain: 14.78, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23044\n",
            "Beam: 0, Iteration: 9507, Q value: 0.5967, Reward: -1.0000, BF Gain pred: 1.12, BF Gain: 3.84, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23050\n",
            "Beam: 0, Iteration: 9508, Q value: 0.7886, Reward: 1.0000, BF Gain pred: 10.60, BF Gain: 6.29, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23056\n",
            "Beam: 0, Iteration: 9509, Q value: 0.3421, Reward: -1.0000, BF Gain pred: 8.79, BF Gain: 7.79, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23062\n",
            "Beam: 0, Iteration: 9510, Q value: -0.7855, Reward: -1.0000, BF Gain pred: 1.44, BF Gain: -1.36, Critic Loss: 0.14, Policy Loss: -1.18\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23064\n",
            "Beam: 0, Iteration: 9511, Q value: 0.4822, Reward: 1.0000, BF Gain pred: 2.12, BF Gain: 1.27, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23070\n",
            "Beam: 0, Iteration: 9512, Q value: -0.1148, Reward: 1.0000, BF Gain pred: 4.84, BF Gain: 5.00, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23076\n",
            "Beam: 0, Iteration: 9513, Q value: 0.7650, Reward: 1.0000, BF Gain pred: 11.52, BF Gain: 10.29, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23082\n",
            "Beam: 0, Iteration: 9514, Q value: 0.8095, Reward: -1.0000, BF Gain pred: 9.59, BF Gain: 8.53, Critic Loss: 0.13, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23088\n",
            "Beam: 0, Iteration: 9515, Q value: 0.4477, Reward: -1.0000, BF Gain pred: 3.33, BF Gain: 3.33, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23094\n",
            "Beam: 0, Iteration: 9516, Q value: 0.1406, Reward: 1.0000, BF Gain pred: 5.93, BF Gain: 4.67, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23100\n",
            "Beam: 0, Iteration: 9517, Q value: -0.2530, Reward: -1.0000, BF Gain pred: 0.55, BF Gain: -0.57, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23106\n",
            "Beam: 0, Iteration: 9518, Q value: -0.7570, Reward: 1.0000, BF Gain pred: 3.27, BF Gain: 7.19, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23112\n",
            "Beam: 0, Iteration: 9519, Q value: 0.5947, Reward: 1.0000, BF Gain pred: 4.73, BF Gain: 6.35, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23118\n",
            "Beam: 0, Iteration: 9520, Q value: 1.2941, Reward: 1.0000, BF Gain pred: 8.31, BF Gain: 8.60, Critic Loss: 0.15, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23124\n",
            "Beam: 0, Iteration: 9521, Q value: 1.0170, Reward: -1.0000, BF Gain pred: 2.30, BF Gain: 3.06, Critic Loss: 0.13, Policy Loss: -1.24\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23130\n",
            "Beam: 0, Iteration: 9522, Q value: 0.5662, Reward: 1.0000, BF Gain pred: 11.64, BF Gain: 7.93, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23136\n",
            "Beam: 0, Iteration: 9523, Q value: 0.4015, Reward: -1.0000, BF Gain pred: 7.45, BF Gain: 13.84, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23142\n",
            "Beam: 0, Iteration: 9524, Q value: 1.1067, Reward: -1.0000, BF Gain pred: 7.24, BF Gain: 7.26, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23148\n",
            "Beam: 0, Iteration: 9525, Q value: -0.5645, Reward: -1.0000, BF Gain pred: 4.94, BF Gain: 6.88, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23154\n",
            "Beam: 0, Iteration: 9526, Q value: 0.7671, Reward: 1.0000, BF Gain pred: 10.76, BF Gain: 14.43, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23160\n",
            "Beam: 0, Iteration: 9527, Q value: 0.7523, Reward: 1.0000, BF Gain pred: 21.20, BF Gain: 8.13, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23166\n",
            "Beam: 0, Iteration: 9528, Q value: -0.2282, Reward: -1.0000, BF Gain pred: 7.71, BF Gain: 8.29, Critic Loss: 0.11, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23172\n",
            "Beam: 0, Iteration: 9529, Q value: 0.0363, Reward: 1.0000, BF Gain pred: 10.50, BF Gain: 13.58, Critic Loss: 0.11, Policy Loss: -1.08\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23174\n",
            "Beam: 0, Iteration: 9530, Q value: -1.4068, Reward: -1.0000, BF Gain pred: 5.18, BF Gain: 3.66, Critic Loss: 0.12, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23180\n",
            "Beam: 0, Iteration: 9531, Q value: 0.5036, Reward: -1.0000, BF Gain pred: 3.53, BF Gain: 4.32, Critic Loss: 0.11, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23186\n",
            "Beam: 0, Iteration: 9532, Q value: -0.0057, Reward: 1.0000, BF Gain pred: 4.64, BF Gain: 9.03, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23192\n",
            "Beam: 0, Iteration: 9533, Q value: 0.4947, Reward: 1.0000, BF Gain pred: 9.15, BF Gain: 9.15, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23194\n",
            "Beam: 0, Iteration: 9534, Q value: -0.4402, Reward: -1.0000, BF Gain pred: 6.26, BF Gain: 3.20, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23200\n",
            "Beam: 0, Iteration: 9535, Q value: 0.4265, Reward: 1.0000, BF Gain pred: 10.36, BF Gain: 8.83, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23206\n",
            "Beam: 0, Iteration: 9536, Q value: -0.0110, Reward: -1.0000, BF Gain pred: 7.97, BF Gain: 0.79, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23208\n",
            "Beam: 0, Iteration: 9537, Q value: -0.2166, Reward: -1.0000, BF Gain pred: 3.42, BF Gain: 1.11, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23214\n",
            "Beam: 0, Iteration: 9538, Q value: 0.2968, Reward: 1.0000, BF Gain pred: 9.56, BF Gain: 12.87, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23220\n",
            "Beam: 0, Iteration: 9539, Q value: -0.3501, Reward: -1.0000, BF Gain pred: 7.11, BF Gain: 0.87, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23226\n",
            "Beam: 0, Iteration: 9540, Q value: 0.7354, Reward: -1.0000, BF Gain pred: 0.34, BF Gain: -1.38, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23232\n",
            "Beam: 0, Iteration: 9541, Q value: 0.6329, Reward: 1.0000, BF Gain pred: 3.37, BF Gain: 5.64, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23238\n",
            "Beam: 0, Iteration: 9542, Q value: 0.7940, Reward: 1.0000, BF Gain pred: 13.67, BF Gain: 10.23, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23244\n",
            "Beam: 0, Iteration: 9543, Q value: 0.5234, Reward: -1.0000, BF Gain pred: 7.19, BF Gain: 5.46, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23250\n",
            "Beam: 0, Iteration: 9544, Q value: 0.3670, Reward: -1.0000, BF Gain pred: 0.08, BF Gain: 3.72, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23252\n",
            "Beam: 0, Iteration: 9545, Q value: 0.5076, Reward: 1.0000, BF Gain pred: 14.74, BF Gain: 10.35, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23258\n",
            "Beam: 0, Iteration: 9546, Q value: -0.7388, Reward: -1.0000, BF Gain pred: 1.08, BF Gain: 2.04, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23264\n",
            "Beam: 0, Iteration: 9547, Q value: 1.0999, Reward: 1.0000, BF Gain pred: 10.14, BF Gain: 15.39, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23270\n",
            "Beam: 0, Iteration: 9548, Q value: -0.0647, Reward: -1.0000, BF Gain pred: 8.67, BF Gain: 11.98, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23276\n",
            "Beam: 0, Iteration: 9549, Q value: 0.3800, Reward: -1.0000, BF Gain pred: 3.31, BF Gain: 2.12, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23282\n",
            "Beam: 0, Iteration: 9550, Q value: 0.1520, Reward: 1.0000, BF Gain pred: 22.01, BF Gain: 15.70, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23288\n",
            "Beam: 0, Iteration: 9551, Q value: 0.0949, Reward: -1.0000, BF Gain pred: 0.46, BF Gain: 6.32, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23290\n",
            "Beam: 0, Iteration: 9552, Q value: -0.3976, Reward: 1.0000, BF Gain pred: 2.58, BF Gain: 10.49, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23296\n",
            "Beam: 0, Iteration: 9553, Q value: 1.2920, Reward: 1.0000, BF Gain pred: 4.42, BF Gain: 6.10, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23302\n",
            "Beam: 0, Iteration: 9554, Q value: 0.8068, Reward: -1.0000, BF Gain pred: 3.82, BF Gain: 1.46, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23308\n",
            "Beam: 0, Iteration: 9555, Q value: -0.0294, Reward: 1.0000, BF Gain pred: 5.00, BF Gain: 5.54, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23314\n",
            "Beam: 0, Iteration: 9556, Q value: 0.3375, Reward: 1.0000, BF Gain pred: 5.50, BF Gain: 5.42, Critic Loss: 0.13, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23320\n",
            "Beam: 0, Iteration: 9557, Q value: 0.1613, Reward: -1.0000, BF Gain pred: 2.24, BF Gain: 5.33, Critic Loss: 0.12, Policy Loss: -1.10\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23326\n",
            "Beam: 0, Iteration: 9558, Q value: -0.1002, Reward: -1.0000, BF Gain pred: 0.63, BF Gain: 1.11, Critic Loss: 0.13, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23332\n",
            "Beam: 0, Iteration: 9559, Q value: 0.1568, Reward: 1.0000, BF Gain pred: 6.61, BF Gain: 8.44, Critic Loss: 0.14, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23338\n",
            "Beam: 0, Iteration: 9560, Q value: 0.5451, Reward: -1.0000, BF Gain pred: 3.49, BF Gain: 9.30, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23344\n",
            "Beam: 0, Iteration: 9561, Q value: -0.2598, Reward: 1.0000, BF Gain pred: 4.14, BF Gain: 6.52, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23350\n",
            "Beam: 0, Iteration: 9562, Q value: 0.0922, Reward: -1.0000, BF Gain pred: 3.26, BF Gain: 5.13, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23356\n",
            "Beam: 0, Iteration: 9563, Q value: 0.9844, Reward: 1.0000, BF Gain pred: 8.84, BF Gain: 9.55, Critic Loss: 0.16, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23362\n",
            "Beam: 0, Iteration: 9564, Q value: -0.7230, Reward: -1.0000, BF Gain pred: 6.01, BF Gain: -0.07, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23368\n",
            "Beam: 0, Iteration: 9565, Q value: 0.6616, Reward: -1.0000, BF Gain pred: 5.93, BF Gain: 7.84, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23374\n",
            "Beam: 0, Iteration: 9566, Q value: 0.0011, Reward: 1.0000, BF Gain pred: 14.05, BF Gain: 10.57, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23376\n",
            "Beam: 0, Iteration: 9567, Q value: -0.0761, Reward: -1.0000, BF Gain pred: 12.26, BF Gain: 6.21, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23382\n",
            "Beam: 0, Iteration: 9568, Q value: 0.2819, Reward: -1.0000, BF Gain pred: 8.17, BF Gain: 4.16, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23388\n",
            "Beam: 0, Iteration: 9569, Q value: 0.7096, Reward: 1.0000, BF Gain pred: 8.39, BF Gain: 6.10, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23394\n",
            "Beam: 0, Iteration: 9570, Q value: 0.5670, Reward: -1.0000, BF Gain pred: 0.75, BF Gain: 0.90, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23400\n",
            "Beam: 0, Iteration: 9571, Q value: 0.7551, Reward: 1.0000, BF Gain pred: 2.78, BF Gain: 0.88, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23406\n",
            "Beam: 0, Iteration: 9572, Q value: 0.3579, Reward: 1.0000, BF Gain pred: 7.41, BF Gain: 8.13, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23412\n",
            "Beam: 0, Iteration: 9573, Q value: 0.2684, Reward: -1.0000, BF Gain pred: 2.01, BF Gain: 1.88, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23418\n",
            "Beam: 0, Iteration: 9574, Q value: 0.0936, Reward: 1.0000, BF Gain pred: 5.70, BF Gain: 6.72, Critic Loss: 0.13, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23424\n",
            "Beam: 0, Iteration: 9575, Q value: -0.5337, Reward: 1.0000, BF Gain pred: 8.32, BF Gain: 6.28, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23426\n",
            "Beam: 0, Iteration: 9576, Q value: 0.1963, Reward: -1.0000, BF Gain pred: 0.67, BF Gain: -6.09, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23432\n",
            "Beam: 0, Iteration: 9577, Q value: -0.6974, Reward: 1.0000, BF Gain pred: 7.26, BF Gain: 6.44, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23438\n",
            "Beam: 0, Iteration: 9578, Q value: 0.2962, Reward: -1.0000, BF Gain pred: 6.43, BF Gain: 4.97, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23444\n",
            "Beam: 0, Iteration: 9579, Q value: 0.7111, Reward: -1.0000, BF Gain pred: 4.54, BF Gain: 9.28, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23450\n",
            "Beam: 0, Iteration: 9580, Q value: 0.8752, Reward: 1.0000, BF Gain pred: 6.97, BF Gain: 5.57, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23456\n",
            "Beam: 0, Iteration: 9581, Q value: -0.1021, Reward: -1.0000, BF Gain pred: 3.10, BF Gain: 5.18, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23462\n",
            "Beam: 0, Iteration: 9582, Q value: -0.3503, Reward: 1.0000, BF Gain pred: 4.18, BF Gain: 4.26, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23468\n",
            "Beam: 0, Iteration: 9583, Q value: -0.0967, Reward: 1.0000, BF Gain pred: 4.30, BF Gain: 3.84, Critic Loss: 0.14, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23474\n",
            "Beam: 0, Iteration: 9584, Q value: -0.0075, Reward: 1.0000, BF Gain pred: 12.55, BF Gain: 6.15, Critic Loss: 0.13, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23480\n",
            "Beam: 0, Iteration: 9585, Q value: 0.9757, Reward: -1.0000, BF Gain pred: 11.82, BF Gain: 11.82, Critic Loss: 0.13, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23486\n",
            "Beam: 0, Iteration: 9586, Q value: 1.0968, Reward: -1.0000, BF Gain pred: 8.87, BF Gain: 7.55, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23488\n",
            "Beam: 0, Iteration: 9587, Q value: 0.7475, Reward: -1.0000, BF Gain pred: 3.21, BF Gain: 4.86, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23494\n",
            "Beam: 0, Iteration: 9588, Q value: -0.2797, Reward: 1.0000, BF Gain pred: 3.94, BF Gain: 2.87, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23500\n",
            "Beam: 0, Iteration: 9589, Q value: 0.5348, Reward: 1.0000, BF Gain pred: 9.90, BF Gain: 5.58, Critic Loss: 0.11, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23506\n",
            "Beam: 0, Iteration: 9590, Q value: 1.2207, Reward: -1.0000, BF Gain pred: -2.93, BF Gain: 2.17, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23512\n",
            "Beam: 0, Iteration: 9591, Q value: 1.1822, Reward: 1.0000, BF Gain pred: 7.20, BF Gain: -0.90, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23518\n",
            "Beam: 0, Iteration: 9592, Q value: 1.0072, Reward: 1.0000, BF Gain pred: 14.28, BF Gain: 7.79, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23524\n",
            "Beam: 0, Iteration: 9593, Q value: 0.5714, Reward: -1.0000, BF Gain pred: 7.25, BF Gain: 4.46, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23530\n",
            "Beam: 0, Iteration: 9594, Q value: 0.2683, Reward: 1.0000, BF Gain pred: 11.48, BF Gain: 10.71, Critic Loss: 0.09, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23536\n",
            "Beam: 0, Iteration: 9595, Q value: -0.0097, Reward: -1.0000, BF Gain pred: 10.51, BF Gain: 4.85, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23542\n",
            "Beam: 0, Iteration: 9596, Q value: 0.1736, Reward: -1.0000, BF Gain pred: 7.74, BF Gain: 9.58, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23548\n",
            "Beam: 0, Iteration: 9597, Q value: -0.4905, Reward: -1.0000, BF Gain pred: 4.58, BF Gain: 6.20, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23554\n",
            "Beam: 0, Iteration: 9598, Q value: 1.0763, Reward: -1.0000, BF Gain pred: -4.31, BF Gain: -5.59, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23560\n",
            "Beam: 0, Iteration: 9599, Q value: 0.7284, Reward: 1.0000, BF Gain pred: -0.34, BF Gain: 3.01, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23562\n",
            "Beam: 0, Iteration: 9600, Q value: 1.1649, Reward: 1.0000, BF Gain pred: 3.60, BF Gain: 1.16, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23568\n",
            "Beam: 0, Iteration: 9601, Q value: 1.1764, Reward: 1.0000, BF Gain pred: 6.38, BF Gain: 5.89, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23574\n",
            "Beam: 0, Iteration: 9602, Q value: 0.7378, Reward: -1.0000, BF Gain pred: 3.10, BF Gain: 3.10, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23580\n",
            "Beam: 0, Iteration: 9603, Q value: 0.8273, Reward: 1.0000, BF Gain pred: 10.29, BF Gain: 4.06, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23582\n",
            "Beam: 0, Iteration: 9604, Q value: 0.1676, Reward: -1.0000, BF Gain pred: 0.82, BF Gain: 0.82, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23588\n",
            "Beam: 0, Iteration: 9605, Q value: 0.4094, Reward: 1.0000, BF Gain pred: 2.34, BF Gain: 0.33, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23594\n",
            "Beam: 0, Iteration: 9606, Q value: -0.1466, Reward: 1.0000, BF Gain pred: 4.92, BF Gain: 5.20, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23600\n",
            "Beam: 0, Iteration: 9607, Q value: 1.0654, Reward: 1.0000, BF Gain pred: 10.95, BF Gain: 10.34, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23606\n",
            "Beam: 0, Iteration: 9608, Q value: 0.0355, Reward: 1.0000, BF Gain pred: 13.09, BF Gain: 11.36, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23612\n",
            "Beam: 0, Iteration: 9609, Q value: 0.4201, Reward: -1.0000, BF Gain pred: 8.70, BF Gain: 7.06, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23618\n",
            "Beam: 0, Iteration: 9610, Q value: 0.3343, Reward: -1.0000, BF Gain pred: 5.18, BF Gain: 6.07, Critic Loss: 0.15, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23624\n",
            "Beam: 0, Iteration: 9611, Q value: -0.5431, Reward: 1.0000, BF Gain pred: 13.20, BF Gain: 13.20, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23630\n",
            "Beam: 0, Iteration: 9612, Q value: 0.3610, Reward: -1.0000, BF Gain pred: -0.84, BF Gain: 4.28, Critic Loss: 0.12, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23636\n",
            "Beam: 0, Iteration: 9613, Q value: 1.0837, Reward: 1.0000, BF Gain pred: 10.24, BF Gain: 12.83, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23638\n",
            "Beam: 0, Iteration: 9614, Q value: 1.1618, Reward: -1.0000, BF Gain pred: 8.15, BF Gain: 13.45, Critic Loss: 0.10, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23644\n",
            "Beam: 0, Iteration: 9615, Q value: -0.3330, Reward: -1.0000, BF Gain pred: 5.89, BF Gain: 12.74, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23646\n",
            "Beam: 0, Iteration: 9616, Q value: 0.5563, Reward: 1.0000, BF Gain pred: 11.52, BF Gain: 8.54, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23652\n",
            "Beam: 0, Iteration: 9617, Q value: 1.0318, Reward: -1.0000, BF Gain pred: 6.55, BF Gain: 7.46, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23654\n",
            "Beam: 0, Iteration: 9618, Q value: 0.7728, Reward: 1.0000, BF Gain pred: 7.68, BF Gain: 8.78, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23660\n",
            "Beam: 0, Iteration: 9619, Q value: -0.3637, Reward: -1.0000, BF Gain pred: 3.28, BF Gain: 4.53, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23666\n",
            "Beam: 0, Iteration: 9620, Q value: 0.7898, Reward: 1.0000, BF Gain pred: 7.94, BF Gain: 19.18, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23672\n",
            "Beam: 0, Iteration: 9621, Q value: 0.7117, Reward: 1.0000, BF Gain pred: 13.78, BF Gain: 11.88, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23678\n",
            "Beam: 0, Iteration: 9622, Q value: -0.2305, Reward: -1.0000, BF Gain pred: 3.44, BF Gain: 3.11, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23684\n",
            "Beam: 0, Iteration: 9623, Q value: 0.0135, Reward: 1.0000, BF Gain pred: 6.82, BF Gain: 7.62, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23690\n",
            "Beam: 0, Iteration: 9624, Q value: -0.0762, Reward: 1.0000, BF Gain pred: 8.77, BF Gain: 4.63, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23696\n",
            "Beam: 0, Iteration: 9625, Q value: 0.2149, Reward: -1.0000, BF Gain pred: 4.29, BF Gain: 2.40, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23702\n",
            "Beam: 0, Iteration: 9626, Q value: 0.6799, Reward: 1.0000, BF Gain pred: 6.92, BF Gain: 8.19, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23708\n",
            "Beam: 0, Iteration: 9627, Q value: -0.3199, Reward: -1.0000, BF Gain pred: -0.12, BF Gain: -0.99, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23714\n",
            "Beam: 0, Iteration: 9628, Q value: 0.5593, Reward: 1.0000, BF Gain pred: 9.62, BF Gain: 8.57, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23720\n",
            "Beam: 0, Iteration: 9629, Q value: 0.2493, Reward: -1.0000, BF Gain pred: 3.71, BF Gain: 7.56, Critic Loss: 0.12, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23726\n",
            "Beam: 0, Iteration: 9630, Q value: 0.5625, Reward: 1.0000, BF Gain pred: 8.12, BF Gain: 12.59, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23732\n",
            "Beam: 0, Iteration: 9631, Q value: -0.0944, Reward: -1.0000, BF Gain pred: 7.17, BF Gain: 8.19, Critic Loss: 0.14, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23738\n",
            "Beam: 0, Iteration: 9632, Q value: 1.0459, Reward: -1.0000, BF Gain pred: -0.03, BF Gain: -1.02, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23744\n",
            "Beam: 0, Iteration: 9633, Q value: 0.8214, Reward: 1.0000, BF Gain pred: 3.11, BF Gain: 2.53, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23750\n",
            "Beam: 0, Iteration: 9634, Q value: -0.4291, Reward: -1.0000, BF Gain pred: 0.19, BF Gain: 1.95, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23756\n",
            "Beam: 0, Iteration: 9635, Q value: 0.0229, Reward: 1.0000, BF Gain pred: 4.37, BF Gain: 1.92, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23762\n",
            "Beam: 0, Iteration: 9636, Q value: 0.6667, Reward: 1.0000, BF Gain pred: 5.78, BF Gain: 5.78, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23768\n",
            "Beam: 0, Iteration: 9637, Q value: 0.6499, Reward: -1.0000, BF Gain pred: 5.54, BF Gain: 6.22, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23774\n",
            "Beam: 0, Iteration: 9638, Q value: 0.3251, Reward: -1.0000, BF Gain pred: 4.59, BF Gain: 9.96, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23780\n",
            "Beam: 0, Iteration: 9639, Q value: -0.6020, Reward: -1.0000, BF Gain pred: 2.27, BF Gain: 2.16, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23786\n",
            "Beam: 0, Iteration: 9640, Q value: 0.0191, Reward: -1.0000, BF Gain pred: 0.47, BF Gain: 2.15, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23792\n",
            "Beam: 0, Iteration: 9641, Q value: 1.2475, Reward: 1.0000, BF Gain pred: 7.59, BF Gain: 6.46, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23798\n",
            "Beam: 0, Iteration: 9642, Q value: 0.3867, Reward: 1.0000, BF Gain pred: 8.92, BF Gain: 4.96, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23804\n",
            "Beam: 0, Iteration: 9643, Q value: 0.2654, Reward: -1.0000, BF Gain pred: -0.47, BF Gain: 2.90, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23810\n",
            "Beam: 0, Iteration: 9644, Q value: 0.8013, Reward: 1.0000, BF Gain pred: 9.63, BF Gain: 6.03, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23816\n",
            "Beam: 0, Iteration: 9645, Q value: -0.4364, Reward: -1.0000, BF Gain pred: -2.15, BF Gain: 2.35, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23822\n",
            "Beam: 0, Iteration: 9646, Q value: 0.5514, Reward: 1.0000, BF Gain pred: 2.00, BF Gain: 7.55, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23828\n",
            "Beam: 0, Iteration: 9647, Q value: -0.0281, Reward: 1.0000, BF Gain pred: 10.17, BF Gain: 3.19, Critic Loss: 0.10, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23834\n",
            "Beam: 0, Iteration: 9648, Q value: 0.1783, Reward: 1.0000, BF Gain pred: 10.75, BF Gain: 8.59, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23840\n",
            "Beam: 0, Iteration: 9649, Q value: 0.7599, Reward: -1.0000, BF Gain pred: 5.53, BF Gain: 9.60, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23846\n",
            "Beam: 0, Iteration: 9650, Q value: -0.2951, Reward: 1.0000, BF Gain pred: 7.04, BF Gain: 6.43, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23852\n",
            "Beam: 0, Iteration: 9651, Q value: 0.7531, Reward: -1.0000, BF Gain pred: 3.19, BF Gain: 9.77, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23858\n",
            "Beam: 0, Iteration: 9652, Q value: 1.2498, Reward: 1.0000, BF Gain pred: 12.63, BF Gain: 10.43, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23864\n",
            "Beam: 0, Iteration: 9653, Q value: -0.6089, Reward: -1.0000, BF Gain pred: 4.85, BF Gain: 7.85, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23870\n",
            "Beam: 0, Iteration: 9654, Q value: -0.3989, Reward: -1.0000, BF Gain pred: 0.75, BF Gain: 1.73, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23876\n",
            "Beam: 0, Iteration: 9655, Q value: 0.1525, Reward: 1.0000, BF Gain pred: 9.92, BF Gain: 6.96, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23882\n",
            "Beam: 0, Iteration: 9656, Q value: -0.1595, Reward: -1.0000, BF Gain pred: 6.82, BF Gain: 5.44, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23888\n",
            "Beam: 0, Iteration: 9657, Q value: -0.4719, Reward: -1.0000, BF Gain pred: 4.71, BF Gain: 7.69, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23894\n",
            "Beam: 0, Iteration: 9658, Q value: 0.4356, Reward: -1.0000, BF Gain pred: -1.19, BF Gain: -0.62, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23900\n",
            "Beam: 0, Iteration: 9659, Q value: 0.8328, Reward: 1.0000, BF Gain pred: 4.30, BF Gain: 0.26, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23902\n",
            "Beam: 0, Iteration: 9660, Q value: 0.4466, Reward: 1.0000, BF Gain pred: 8.80, BF Gain: 9.94, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23908\n",
            "Beam: 0, Iteration: 9661, Q value: -0.1930, Reward: -1.0000, BF Gain pred: 2.02, BF Gain: 3.52, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23914\n",
            "Beam: 0, Iteration: 9662, Q value: 0.1635, Reward: -1.0000, BF Gain pred: -0.35, BF Gain: -1.80, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23920\n",
            "Beam: 0, Iteration: 9663, Q value: -0.1944, Reward: 1.0000, BF Gain pred: 6.48, BF Gain: 6.30, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23926\n",
            "Beam: 0, Iteration: 9664, Q value: -0.0248, Reward: 1.0000, BF Gain pred: 8.83, BF Gain: 11.99, Critic Loss: 0.14, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23932\n",
            "Beam: 0, Iteration: 9665, Q value: 0.8639, Reward: -1.0000, BF Gain pred: 7.93, BF Gain: 6.71, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23938\n",
            "Beam: 0, Iteration: 9666, Q value: -0.0963, Reward: -1.0000, BF Gain pred: 5.27, BF Gain: 6.81, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23944\n",
            "Beam: 0, Iteration: 9667, Q value: 0.1062, Reward: -1.0000, BF Gain pred: 0.04, BF Gain: 0.04, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23950\n",
            "Beam: 0, Iteration: 9668, Q value: 1.1240, Reward: 1.0000, BF Gain pred: 12.28, BF Gain: 7.00, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23956\n",
            "Beam: 0, Iteration: 9669, Q value: -0.2601, Reward: -1.0000, BF Gain pred: 5.08, BF Gain: 10.13, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23958\n",
            "Beam: 0, Iteration: 9670, Q value: 0.7332, Reward: 1.0000, BF Gain pred: 13.25, BF Gain: 15.06, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23964\n",
            "Beam: 0, Iteration: 9671, Q value: -0.0495, Reward: -1.0000, BF Gain pred: 4.27, BF Gain: 9.42, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23966\n",
            "Beam: 0, Iteration: 9672, Q value: 0.7615, Reward: 1.0000, BF Gain pred: 5.78, BF Gain: 7.90, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23972\n",
            "Beam: 0, Iteration: 9673, Q value: -0.9575, Reward: -1.0000, BF Gain pred: -3.66, BF Gain: -4.86, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23978\n",
            "Beam: 0, Iteration: 9674, Q value: 1.2902, Reward: 1.0000, BF Gain pred: 5.18, BF Gain: 2.40, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23984\n",
            "Beam: 0, Iteration: 9675, Q value: 0.8856, Reward: -1.0000, BF Gain pred: 4.60, BF Gain: 5.42, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23990\n",
            "Beam: 0, Iteration: 9676, Q value: -0.1175, Reward: 1.0000, BF Gain pred: 6.63, BF Gain: 4.72, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 23996\n",
            "Beam: 0, Iteration: 9677, Q value: -0.6494, Reward: -1.0000, BF Gain pred: -5.40, BF Gain: -5.15, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24002\n",
            "Beam: 0, Iteration: 9678, Q value: 0.8322, Reward: 1.0000, BF Gain pred: 6.61, BF Gain: 9.50, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24008\n",
            "Beam: 0, Iteration: 9679, Q value: 0.5515, Reward: 1.0000, BF Gain pred: 9.65, BF Gain: 9.65, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24014\n",
            "Beam: 0, Iteration: 9680, Q value: 0.3449, Reward: -1.0000, BF Gain pred: 8.59, BF Gain: 6.19, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24020\n",
            "Beam: 0, Iteration: 9681, Q value: -0.0231, Reward: -1.0000, BF Gain pred: 1.37, BF Gain: 1.19, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24026\n",
            "Beam: 0, Iteration: 9682, Q value: 0.7517, Reward: 1.0000, BF Gain pred: 7.87, BF Gain: 7.87, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24032\n",
            "Beam: 0, Iteration: 9683, Q value: -0.0363, Reward: 1.0000, BF Gain pred: 10.20, BF Gain: 12.74, Critic Loss: 0.12, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24038\n",
            "Beam: 0, Iteration: 9684, Q value: -0.0908, Reward: 1.0000, BF Gain pred: 11.50, BF Gain: 11.88, Critic Loss: 0.09, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24044\n",
            "Beam: 0, Iteration: 9685, Q value: -0.2571, Reward: -1.0000, BF Gain pred: 4.71, BF Gain: 7.28, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24046\n",
            "Beam: 0, Iteration: 9686, Q value: 1.0135, Reward: 1.0000, BF Gain pred: 9.24, BF Gain: 14.86, Critic Loss: 0.10, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24052\n",
            "Beam: 0, Iteration: 9687, Q value: -0.1665, Reward: -1.0000, BF Gain pred: 9.22, BF Gain: 9.62, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24058\n",
            "Beam: 0, Iteration: 9688, Q value: 1.1244, Reward: -1.0000, BF Gain pred: 8.74, BF Gain: 6.36, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24064\n",
            "Beam: 0, Iteration: 9689, Q value: 0.8415, Reward: 1.0000, BF Gain pred: 12.60, BF Gain: 2.37, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24066\n",
            "Beam: 0, Iteration: 9690, Q value: 0.8679, Reward: -1.0000, BF Gain pred: 5.88, BF Gain: 6.58, Critic Loss: 0.10, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24072\n",
            "Beam: 0, Iteration: 9691, Q value: 1.1512, Reward: -1.0000, BF Gain pred: 4.99, BF Gain: 3.88, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24074\n",
            "Beam: 0, Iteration: 9692, Q value: 0.0946, Reward: -1.0000, BF Gain pred: 0.95, BF Gain: -1.04, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24080\n",
            "Beam: 0, Iteration: 9693, Q value: -0.2605, Reward: 1.0000, BF Gain pred: 2.44, BF Gain: 0.56, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24086\n",
            "Beam: 0, Iteration: 9694, Q value: 1.1778, Reward: 1.0000, BF Gain pred: 11.10, BF Gain: 10.24, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24092\n",
            "Beam: 0, Iteration: 9695, Q value: 0.6373, Reward: -1.0000, BF Gain pred: 3.46, BF Gain: 4.37, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24098\n",
            "Beam: 0, Iteration: 9696, Q value: 0.7184, Reward: -1.0000, BF Gain pred: -1.68, BF Gain: -0.45, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24104\n",
            "Beam: 0, Iteration: 9697, Q value: 1.2947, Reward: 1.0000, BF Gain pred: 7.28, BF Gain: 8.78, Critic Loss: 0.11, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24110\n",
            "Beam: 0, Iteration: 9698, Q value: 0.5323, Reward: 1.0000, BF Gain pred: 7.65, BF Gain: 1.56, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24116\n",
            "Beam: 0, Iteration: 9699, Q value: 0.6626, Reward: 1.0000, BF Gain pred: 8.54, BF Gain: 4.61, Critic Loss: 0.11, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24122\n",
            "Beam: 0, Iteration: 9700, Q value: 0.1502, Reward: 1.0000, BF Gain pred: 10.73, BF Gain: 5.51, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24128\n",
            "Beam: 0, Iteration: 9701, Q value: -0.0753, Reward: -1.0000, BF Gain pred: 3.30, BF Gain: 1.62, Critic Loss: 0.15, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24134\n",
            "Beam: 0, Iteration: 9702, Q value: 0.6009, Reward: 1.0000, BF Gain pred: 3.60, BF Gain: 3.95, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24140\n",
            "Beam: 0, Iteration: 9703, Q value: 0.9964, Reward: 1.0000, BF Gain pred: 7.27, BF Gain: 5.88, Critic Loss: 0.15, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24146\n",
            "Beam: 0, Iteration: 9704, Q value: 0.3151, Reward: 1.0000, BF Gain pred: 8.78, BF Gain: 5.56, Critic Loss: 0.11, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24152\n",
            "Beam: 0, Iteration: 9705, Q value: -0.6649, Reward: -1.0000, BF Gain pred: 8.68, BF Gain: 9.49, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24158\n",
            "Beam: 0, Iteration: 9706, Q value: -0.4723, Reward: -1.0000, BF Gain pred: 8.20, BF Gain: 3.93, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24164\n",
            "Beam: 0, Iteration: 9707, Q value: 0.3783, Reward: -1.0000, BF Gain pred: 7.74, BF Gain: 7.64, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24170\n",
            "Beam: 0, Iteration: 9708, Q value: 0.6148, Reward: -1.0000, BF Gain pred: 6.77, BF Gain: 4.39, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24176\n",
            "Beam: 0, Iteration: 9709, Q value: 0.4045, Reward: -1.0000, BF Gain pred: 0.71, BF Gain: 2.60, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24182\n",
            "Beam: 0, Iteration: 9710, Q value: 0.8239, Reward: 1.0000, BF Gain pred: 15.25, BF Gain: 13.47, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24188\n",
            "Beam: 0, Iteration: 9711, Q value: -0.1222, Reward: -1.0000, BF Gain pred: 3.26, BF Gain: 3.26, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24194\n",
            "Beam: 0, Iteration: 9712, Q value: -0.4489, Reward: 1.0000, BF Gain pred: 9.15, BF Gain: 5.37, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24196\n",
            "Beam: 0, Iteration: 9713, Q value: 0.2874, Reward: 1.0000, BF Gain pred: 11.50, BF Gain: 12.16, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24202\n",
            "Beam: 0, Iteration: 9714, Q value: -1.4374, Reward: -1.0000, BF Gain pred: 4.87, BF Gain: 5.32, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24208\n",
            "Beam: 0, Iteration: 9715, Q value: -0.0397, Reward: 1.0000, BF Gain pred: 9.42, BF Gain: 3.58, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24214\n",
            "Beam: 0, Iteration: 9716, Q value: 0.2837, Reward: -1.0000, BF Gain pred: 6.11, BF Gain: 5.55, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24220\n",
            "Beam: 0, Iteration: 9717, Q value: 0.1350, Reward: 1.0000, BF Gain pred: 9.27, BF Gain: 7.65, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24226\n",
            "Beam: 0, Iteration: 9718, Q value: -0.0822, Reward: -1.0000, BF Gain pred: 1.02, BF Gain: -2.12, Critic Loss: 0.09, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24228\n",
            "Beam: 0, Iteration: 9719, Q value: 1.2942, Reward: -1.0000, BF Gain pred: -0.72, BF Gain: 3.69, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24234\n",
            "Beam: 0, Iteration: 9720, Q value: 0.5888, Reward: 1.0000, BF Gain pred: 10.28, BF Gain: 9.75, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24240\n",
            "Beam: 0, Iteration: 9721, Q value: 0.8696, Reward: -1.0000, BF Gain pred: 6.26, BF Gain: 16.53, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24246\n",
            "Beam: 0, Iteration: 9722, Q value: -0.7365, Reward: -1.0000, BF Gain pred: 5.54, BF Gain: 1.98, Critic Loss: 0.10, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24252\n",
            "Beam: 0, Iteration: 9723, Q value: 0.2303, Reward: -1.0000, BF Gain pred: 2.74, BF Gain: 5.72, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24258\n",
            "Beam: 0, Iteration: 9724, Q value: -0.2575, Reward: 1.0000, BF Gain pred: 6.79, BF Gain: 4.03, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24260\n",
            "Beam: 0, Iteration: 9725, Q value: 0.4315, Reward: -1.0000, BF Gain pred: 1.08, BF Gain: 2.15, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24262\n",
            "Beam: 0, Iteration: 9726, Q value: 0.3577, Reward: 1.0000, BF Gain pred: 8.13, BF Gain: 7.01, Critic Loss: 0.09, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24268\n",
            "Beam: 0, Iteration: 9727, Q value: 1.0638, Reward: -1.0000, BF Gain pred: 5.88, BF Gain: 11.18, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24274\n",
            "Beam: 0, Iteration: 9728, Q value: -0.0499, Reward: 1.0000, BF Gain pred: 8.15, BF Gain: 8.53, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24280\n",
            "Beam: 0, Iteration: 9729, Q value: 0.4435, Reward: -1.0000, BF Gain pred: 7.97, BF Gain: 9.92, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24286\n",
            "Beam: 0, Iteration: 9730, Q value: 0.2088, Reward: -1.0000, BF Gain pred: 1.83, BF Gain: 3.68, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24288\n",
            "Beam: 0, Iteration: 9731, Q value: 0.4116, Reward: 1.0000, BF Gain pred: 5.53, BF Gain: 7.19, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24294\n",
            "Beam: 0, Iteration: 9732, Q value: 0.4806, Reward: 1.0000, BF Gain pred: 7.37, BF Gain: 9.22, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24300\n",
            "Beam: 0, Iteration: 9733, Q value: 0.6830, Reward: 1.0000, BF Gain pred: 9.36, BF Gain: 15.65, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24306\n",
            "Beam: 0, Iteration: 9734, Q value: 0.1484, Reward: -1.0000, BF Gain pred: 5.85, BF Gain: 4.48, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24312\n",
            "Beam: 0, Iteration: 9735, Q value: 0.7768, Reward: 1.0000, BF Gain pred: 8.67, BF Gain: 9.85, Critic Loss: 0.15, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24318\n",
            "Beam: 0, Iteration: 9736, Q value: -0.1415, Reward: 1.0000, BF Gain pred: 8.98, BF Gain: 8.98, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24324\n",
            "Beam: 0, Iteration: 9737, Q value: -0.1191, Reward: -1.0000, BF Gain pred: 3.47, BF Gain: 6.19, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24330\n",
            "Beam: 0, Iteration: 9738, Q value: -0.4265, Reward: 1.0000, BF Gain pred: 7.56, BF Gain: 7.16, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24336\n",
            "Beam: 0, Iteration: 9739, Q value: 0.3063, Reward: 1.0000, BF Gain pred: 13.34, BF Gain: 13.24, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24342\n",
            "Beam: 0, Iteration: 9740, Q value: -0.4716, Reward: 1.0000, BF Gain pred: 18.01, BF Gain: 13.14, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24344\n",
            "Beam: 0, Iteration: 9741, Q value: -0.6302, Reward: -1.0000, BF Gain pred: 7.20, BF Gain: 7.95, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24350\n",
            "Beam: 0, Iteration: 9742, Q value: 0.5781, Reward: -1.0000, BF Gain pred: 4.53, BF Gain: -0.13, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24356\n",
            "Beam: 0, Iteration: 9743, Q value: 1.2967, Reward: 1.0000, BF Gain pred: 11.49, BF Gain: 6.37, Critic Loss: 0.10, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24358\n",
            "Beam: 0, Iteration: 9744, Q value: 0.3188, Reward: -1.0000, BF Gain pred: 6.44, BF Gain: 5.46, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24364\n",
            "Beam: 0, Iteration: 9745, Q value: 0.3827, Reward: -1.0000, BF Gain pred: 5.73, BF Gain: 10.28, Critic Loss: 0.14, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24370\n",
            "Beam: 0, Iteration: 9746, Q value: 0.0411, Reward: -1.0000, BF Gain pred: 2.70, BF Gain: 4.78, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24372\n",
            "Beam: 0, Iteration: 9747, Q value: 1.2093, Reward: 1.0000, BF Gain pred: 6.39, BF Gain: -0.20, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24378\n",
            "Beam: 0, Iteration: 9748, Q value: -0.0226, Reward: -1.0000, BF Gain pred: 3.52, BF Gain: 3.52, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24384\n",
            "Beam: 0, Iteration: 9749, Q value: 0.1464, Reward: 1.0000, BF Gain pred: 12.25, BF Gain: 11.52, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24390\n",
            "Beam: 0, Iteration: 9750, Q value: -0.0520, Reward: -1.0000, BF Gain pred: 8.04, BF Gain: 7.30, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24396\n",
            "Beam: 0, Iteration: 9751, Q value: 0.2284, Reward: -1.0000, BF Gain pred: 6.64, BF Gain: 9.25, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24402\n",
            "Beam: 0, Iteration: 9752, Q value: -0.0167, Reward: 1.0000, BF Gain pred: 6.80, BF Gain: 6.80, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24408\n",
            "Beam: 0, Iteration: 9753, Q value: 0.8526, Reward: -1.0000, BF Gain pred: 2.52, BF Gain: 1.43, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24414\n",
            "Beam: 0, Iteration: 9754, Q value: 0.3547, Reward: 1.0000, BF Gain pred: 7.87, BF Gain: 7.42, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24420\n",
            "Beam: 0, Iteration: 9755, Q value: 0.3902, Reward: -1.0000, BF Gain pred: 1.71, BF Gain: 3.54, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24426\n",
            "Beam: 0, Iteration: 9756, Q value: 0.1810, Reward: -1.0000, BF Gain pred: 1.49, BF Gain: 1.68, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24432\n",
            "Beam: 0, Iteration: 9757, Q value: 0.5945, Reward: 1.0000, BF Gain pred: 5.70, BF Gain: 3.95, Critic Loss: 0.11, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24438\n",
            "Beam: 0, Iteration: 9758, Q value: 0.2680, Reward: 1.0000, BF Gain pred: 6.45, BF Gain: 5.46, Critic Loss: 0.13, Policy Loss: -1.13\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24440\n",
            "Beam: 0, Iteration: 9759, Q value: -0.0044, Reward: -1.0000, BF Gain pred: 5.06, BF Gain: 2.00, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24446\n",
            "Beam: 0, Iteration: 9760, Q value: 0.2403, Reward: -1.0000, BF Gain pred: 2.93, BF Gain: 2.00, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24448\n",
            "Beam: 0, Iteration: 9761, Q value: 1.2960, Reward: -1.0000, BF Gain pred: 0.75, BF Gain: -0.24, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24454\n",
            "Beam: 0, Iteration: 9762, Q value: 0.6282, Reward: 1.0000, BF Gain pred: 8.38, BF Gain: 8.38, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24460\n",
            "Beam: 0, Iteration: 9763, Q value: 1.0847, Reward: -1.0000, BF Gain pred: 4.41, BF Gain: 1.98, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24466\n",
            "Beam: 0, Iteration: 9764, Q value: 1.0677, Reward: -1.0000, BF Gain pred: 0.53, BF Gain: 0.83, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24472\n",
            "Beam: 0, Iteration: 9765, Q value: 0.9592, Reward: 1.0000, BF Gain pred: 7.52, BF Gain: 8.79, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24478\n",
            "Beam: 0, Iteration: 9766, Q value: 0.5993, Reward: -1.0000, BF Gain pred: 1.87, BF Gain: -0.68, Critic Loss: 0.13, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24484\n",
            "Beam: 0, Iteration: 9767, Q value: 0.4950, Reward: -1.0000, BF Gain pred: -1.43, BF Gain: 0.12, Critic Loss: 0.14, Policy Loss: -1.10\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24490\n",
            "Beam: 0, Iteration: 9768, Q value: 0.6377, Reward: 1.0000, BF Gain pred: 6.43, BF Gain: -0.18, Critic Loss: 0.11, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24496\n",
            "Beam: 0, Iteration: 9769, Q value: -0.6486, Reward: -1.0000, BF Gain pred: -1.46, BF Gain: -5.24, Critic Loss: 0.13, Policy Loss: -1.10\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24502\n",
            "Beam: 0, Iteration: 9770, Q value: 0.8750, Reward: 1.0000, BF Gain pred: 6.47, BF Gain: 3.45, Critic Loss: 0.14, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24508\n",
            "Beam: 0, Iteration: 9771, Q value: 0.2477, Reward: -1.0000, BF Gain pred: 6.14, BF Gain: 6.58, Critic Loss: 0.11, Policy Loss: -1.09\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24514\n",
            "Beam: 0, Iteration: 9772, Q value: 1.0737, Reward: -1.0000, BF Gain pred: 5.02, BF Gain: 2.53, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24520\n",
            "Beam: 0, Iteration: 9773, Q value: 0.8102, Reward: -1.0000, BF Gain pred: -3.73, BF Gain: -5.57, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24526\n",
            "Beam: 0, Iteration: 9774, Q value: 0.6547, Reward: 1.0000, BF Gain pred: 7.93, BF Gain: 6.64, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24532\n",
            "Beam: 0, Iteration: 9775, Q value: -0.2476, Reward: -1.0000, BF Gain pred: 2.40, BF Gain: 0.05, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24538\n",
            "Beam: 0, Iteration: 9776, Q value: 0.4210, Reward: 1.0000, BF Gain pred: 4.60, BF Gain: 4.30, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24544\n",
            "Beam: 0, Iteration: 9777, Q value: 0.3280, Reward: 1.0000, BF Gain pred: 6.68, BF Gain: 7.68, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24550\n",
            "Beam: 0, Iteration: 9778, Q value: 0.4779, Reward: -1.0000, BF Gain pred: -1.88, BF Gain: 3.34, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24556\n",
            "Beam: 0, Iteration: 9779, Q value: 0.2440, Reward: 1.0000, BF Gain pred: 5.83, BF Gain: 4.95, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24562\n",
            "Beam: 0, Iteration: 9780, Q value: -0.1699, Reward: 1.0000, BF Gain pred: 13.29, BF Gain: 9.28, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24568\n",
            "Beam: 0, Iteration: 9781, Q value: 0.1334, Reward: -1.0000, BF Gain pred: 9.58, BF Gain: 8.39, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24574\n",
            "Beam: 0, Iteration: 9782, Q value: -0.2959, Reward: -1.0000, BF Gain pred: 5.05, BF Gain: 7.76, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24576\n",
            "Beam: 0, Iteration: 9783, Q value: 0.6077, Reward: -1.0000, BF Gain pred: 1.96, BF Gain: 1.96, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24578\n",
            "Beam: 0, Iteration: 9784, Q value: -0.4782, Reward: -1.0000, BF Gain pred: 1.56, BF Gain: 3.01, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24584\n",
            "Beam: 0, Iteration: 9785, Q value: 1.2147, Reward: 1.0000, BF Gain pred: 11.25, BF Gain: 7.52, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24590\n",
            "Beam: 0, Iteration: 9786, Q value: 1.1173, Reward: -1.0000, BF Gain pred: 4.08, BF Gain: 1.54, Critic Loss: 0.14, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24596\n",
            "Beam: 0, Iteration: 9787, Q value: 1.0220, Reward: 1.0000, BF Gain pred: 7.69, BF Gain: 8.15, Critic Loss: 0.10, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24602\n",
            "Beam: 0, Iteration: 9788, Q value: 1.0767, Reward: -1.0000, BF Gain pred: 4.24, BF Gain: 2.70, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24608\n",
            "Beam: 0, Iteration: 9789, Q value: -0.4452, Reward: 1.0000, BF Gain pred: 10.02, BF Gain: 12.07, Critic Loss: 0.15, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24614\n",
            "Beam: 0, Iteration: 9790, Q value: -0.4349, Reward: -1.0000, BF Gain pred: 4.77, BF Gain: 1.74, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24620\n",
            "Beam: 0, Iteration: 9791, Q value: 0.1485, Reward: 1.0000, BF Gain pred: 11.01, BF Gain: 8.49, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24626\n",
            "Beam: 0, Iteration: 9792, Q value: 0.1688, Reward: -1.0000, BF Gain pred: 6.23, BF Gain: 6.61, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24632\n",
            "Beam: 0, Iteration: 9793, Q value: -0.3745, Reward: -1.0000, BF Gain pred: 3.47, BF Gain: 3.47, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24638\n",
            "Beam: 0, Iteration: 9794, Q value: 0.4274, Reward: -1.0000, BF Gain pred: 1.63, BF Gain: 1.41, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24644\n",
            "Beam: 0, Iteration: 9795, Q value: 0.1284, Reward: -1.0000, BF Gain pred: 1.06, BF Gain: 1.06, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24650\n",
            "Beam: 0, Iteration: 9796, Q value: 1.2950, Reward: 1.0000, BF Gain pred: 3.78, BF Gain: 3.78, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24656\n",
            "Beam: 0, Iteration: 9797, Q value: 0.1050, Reward: 1.0000, BF Gain pred: 9.12, BF Gain: 4.78, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24662\n",
            "Beam: 0, Iteration: 9798, Q value: 0.5582, Reward: -1.0000, BF Gain pred: 5.46, BF Gain: 2.46, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24668\n",
            "Beam: 0, Iteration: 9799, Q value: 0.1599, Reward: -1.0000, BF Gain pred: 3.57, BF Gain: 0.85, Critic Loss: 0.13, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24674\n",
            "Beam: 0, Iteration: 9800, Q value: 0.8812, Reward: 1.0000, BF Gain pred: 10.71, BF Gain: 10.71, Critic Loss: 0.15, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24676\n",
            "Beam: 0, Iteration: 9801, Q value: 0.7126, Reward: -1.0000, BF Gain pred: 6.78, BF Gain: 8.20, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24682\n",
            "Beam: 0, Iteration: 9802, Q value: 0.2806, Reward: -1.0000, BF Gain pred: 3.23, BF Gain: 3.10, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24688\n",
            "Beam: 0, Iteration: 9803, Q value: 0.4250, Reward: -1.0000, BF Gain pred: -1.08, BF Gain: 0.10, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24694\n",
            "Beam: 0, Iteration: 9804, Q value: 0.5999, Reward: 1.0000, BF Gain pred: 9.31, BF Gain: 6.82, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24700\n",
            "Beam: 0, Iteration: 9805, Q value: 0.4212, Reward: -1.0000, BF Gain pred: 5.28, BF Gain: 5.00, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24706\n",
            "Beam: 0, Iteration: 9806, Q value: -0.0823, Reward: 1.0000, BF Gain pred: 6.80, BF Gain: 6.00, Critic Loss: 0.15, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24712\n",
            "Beam: 0, Iteration: 9807, Q value: -0.1796, Reward: -1.0000, BF Gain pred: 1.85, BF Gain: 4.82, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24718\n",
            "Beam: 0, Iteration: 9808, Q value: 0.8644, Reward: -1.0000, BF Gain pred: 1.82, BF Gain: 2.10, Critic Loss: 0.16, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24724\n",
            "Beam: 0, Iteration: 9809, Q value: 0.1071, Reward: 1.0000, BF Gain pred: 3.65, BF Gain: 3.10, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24730\n",
            "Beam: 0, Iteration: 9810, Q value: -0.4083, Reward: 1.0000, BF Gain pred: 6.28, BF Gain: 7.22, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24736\n",
            "Beam: 0, Iteration: 9811, Q value: 0.0486, Reward: 1.0000, BF Gain pred: 10.80, BF Gain: 12.57, Critic Loss: 0.11, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24742\n",
            "Beam: 0, Iteration: 9812, Q value: -0.7195, Reward: -1.0000, BF Gain pred: 5.75, BF Gain: 5.75, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24748\n",
            "Beam: 0, Iteration: 9813, Q value: -0.1999, Reward: -1.0000, BF Gain pred: 4.52, BF Gain: 7.43, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24754\n",
            "Beam: 0, Iteration: 9814, Q value: -0.9026, Reward: -1.0000, BF Gain pred: 1.23, BF Gain: 1.15, Critic Loss: 0.15, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24760\n",
            "Beam: 0, Iteration: 9815, Q value: 0.4649, Reward: 1.0000, BF Gain pred: 2.37, BF Gain: 2.75, Critic Loss: 0.14, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24766\n",
            "Beam: 0, Iteration: 9816, Q value: 0.5552, Reward: -1.0000, BF Gain pred: -0.17, BF Gain: 0.38, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24768\n",
            "Beam: 0, Iteration: 9817, Q value: 1.2948, Reward: 1.0000, BF Gain pred: 3.36, BF Gain: 5.94, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24774\n",
            "Beam: 0, Iteration: 9818, Q value: 0.5646, Reward: 1.0000, BF Gain pred: 4.56, BF Gain: 3.14, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24780\n",
            "Beam: 0, Iteration: 9819, Q value: -1.5512, Reward: -1.0000, BF Gain pred: 0.74, BF Gain: 2.75, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24782\n",
            "Beam: 0, Iteration: 9820, Q value: 0.8561, Reward: 1.0000, BF Gain pred: 9.80, BF Gain: 7.89, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24784\n",
            "Beam: 0, Iteration: 9821, Q value: -0.5824, Reward: 1.0000, BF Gain pred: 10.96, BF Gain: 14.13, Critic Loss: 0.15, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24790\n",
            "Beam: 0, Iteration: 9822, Q value: -0.0333, Reward: -1.0000, BF Gain pred: 9.39, BF Gain: 9.62, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24796\n",
            "Beam: 0, Iteration: 9823, Q value: 0.5079, Reward: 1.0000, BF Gain pred: 9.61, BF Gain: 8.17, Critic Loss: 0.14, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24802\n",
            "Beam: 0, Iteration: 9824, Q value: 0.6693, Reward: -1.0000, BF Gain pred: 4.81, BF Gain: 9.66, Critic Loss: 0.09, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24804\n",
            "Beam: 0, Iteration: 9825, Q value: 1.2347, Reward: -1.0000, BF Gain pred: 2.14, BF Gain: 5.11, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24806\n",
            "Beam: 0, Iteration: 9826, Q value: -0.2264, Reward: -1.0000, BF Gain pred: 0.15, BF Gain: 1.40, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24812\n",
            "Beam: 0, Iteration: 9827, Q value: 0.8900, Reward: 1.0000, BF Gain pred: 5.04, BF Gain: 4.75, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24818\n",
            "Beam: 0, Iteration: 9828, Q value: 0.7756, Reward: -1.0000, BF Gain pred: 3.79, BF Gain: 2.11, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24824\n",
            "Beam: 0, Iteration: 9829, Q value: -0.0857, Reward: 1.0000, BF Gain pred: 14.07, BF Gain: 11.74, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24830\n",
            "Beam: 0, Iteration: 9830, Q value: 0.0604, Reward: -1.0000, BF Gain pred: 3.91, BF Gain: 2.03, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24836\n",
            "Beam: 0, Iteration: 9831, Q value: 0.5477, Reward: 1.0000, BF Gain pred: 9.52, BF Gain: 8.28, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24842\n",
            "Beam: 0, Iteration: 9832, Q value: -0.3616, Reward: -1.0000, BF Gain pred: 1.40, BF Gain: 0.01, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24848\n",
            "Beam: 0, Iteration: 9833, Q value: 1.1378, Reward: 1.0000, BF Gain pred: 1.69, BF Gain: 2.69, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24854\n",
            "Beam: 0, Iteration: 9834, Q value: -0.1115, Reward: 1.0000, BF Gain pred: 11.05, BF Gain: 6.99, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24860\n",
            "Beam: 0, Iteration: 9835, Q value: 0.3609, Reward: -1.0000, BF Gain pred: 5.96, BF Gain: 7.24, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24866\n",
            "Beam: 0, Iteration: 9836, Q value: 0.9733, Reward: 1.0000, BF Gain pred: 7.04, BF Gain: 6.40, Critic Loss: 0.13, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24872\n",
            "Beam: 0, Iteration: 9837, Q value: -1.4399, Reward: -1.0000, BF Gain pred: 3.01, BF Gain: 3.01, Critic Loss: 0.14, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24878\n",
            "Beam: 0, Iteration: 9838, Q value: 0.6475, Reward: 1.0000, BF Gain pred: 8.76, BF Gain: 7.11, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24884\n",
            "Beam: 0, Iteration: 9839, Q value: 0.5929, Reward: -1.0000, BF Gain pred: 4.89, BF Gain: 5.71, Critic Loss: 0.14, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24890\n",
            "Beam: 0, Iteration: 9840, Q value: 0.8449, Reward: -1.0000, BF Gain pred: 4.64, BF Gain: 4.67, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24896\n",
            "Beam: 0, Iteration: 9841, Q value: -0.0737, Reward: 1.0000, BF Gain pred: 5.34, BF Gain: 1.26, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24902\n",
            "Beam: 0, Iteration: 9842, Q value: 0.5725, Reward: 1.0000, BF Gain pred: 12.14, BF Gain: 16.79, Critic Loss: 0.15, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24908\n",
            "Beam: 0, Iteration: 9843, Q value: 0.9065, Reward: -1.0000, BF Gain pred: 4.63, BF Gain: 4.41, Critic Loss: 0.09, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24910\n",
            "Beam: 0, Iteration: 9844, Q value: -0.5791, Reward: -1.0000, BF Gain pred: 3.10, BF Gain: 7.62, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24916\n",
            "Beam: 0, Iteration: 9845, Q value: 0.8342, Reward: 1.0000, BF Gain pred: 7.91, BF Gain: 2.28, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24922\n",
            "Beam: 0, Iteration: 9846, Q value: 0.1287, Reward: 1.0000, BF Gain pred: 8.99, BF Gain: 8.99, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24928\n",
            "Beam: 0, Iteration: 9847, Q value: 0.5079, Reward: -1.0000, BF Gain pred: 6.41, BF Gain: 12.53, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24934\n",
            "Beam: 0, Iteration: 9848, Q value: 0.6171, Reward: -1.0000, BF Gain pred: -3.34, BF Gain: -2.14, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24940\n",
            "Beam: 0, Iteration: 9849, Q value: 1.2942, Reward: 1.0000, BF Gain pred: 8.31, BF Gain: 10.89, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24946\n",
            "Beam: 0, Iteration: 9850, Q value: 0.9991, Reward: -1.0000, BF Gain pred: 3.96, BF Gain: 2.46, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24952\n",
            "Beam: 0, Iteration: 9851, Q value: 1.0727, Reward: 1.0000, BF Gain pred: 14.25, BF Gain: 9.84, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24958\n",
            "Beam: 0, Iteration: 9852, Q value: 0.1201, Reward: -1.0000, BF Gain pred: 9.41, BF Gain: 9.86, Critic Loss: 0.13, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24964\n",
            "Beam: 0, Iteration: 9853, Q value: -0.4979, Reward: -1.0000, BF Gain pred: -0.88, BF Gain: 1.77, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24970\n",
            "Beam: 0, Iteration: 9854, Q value: 1.1795, Reward: 1.0000, BF Gain pred: 10.55, BF Gain: 6.75, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24976\n",
            "Beam: 0, Iteration: 9855, Q value: 0.2276, Reward: -1.0000, BF Gain pred: 0.95, BF Gain: 5.72, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24982\n",
            "Beam: 0, Iteration: 9856, Q value: -0.2408, Reward: 1.0000, BF Gain pred: 3.82, BF Gain: 3.97, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24988\n",
            "Beam: 0, Iteration: 9857, Q value: 0.5848, Reward: -1.0000, BF Gain pred: 2.50, BF Gain: 2.77, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 24994\n",
            "Beam: 0, Iteration: 9858, Q value: -0.2112, Reward: 1.0000, BF Gain pred: 5.89, BF Gain: 7.65, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25000\n",
            "Beam: 0, Iteration: 9859, Q value: 0.4964, Reward: -1.0000, BF Gain pred: 5.47, BF Gain: 5.85, Critic Loss: 0.10, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25006\n",
            "Beam: 0, Iteration: 9860, Q value: -0.0458, Reward: 1.0000, BF Gain pred: 10.29, BF Gain: 10.29, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25012\n",
            "Beam: 0, Iteration: 9861, Q value: 0.7339, Reward: -1.0000, BF Gain pred: 0.02, BF Gain: 0.65, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25018\n",
            "Beam: 0, Iteration: 9862, Q value: -0.6969, Reward: 1.0000, BF Gain pred: 10.52, BF Gain: 6.90, Critic Loss: 0.13, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25024\n",
            "Beam: 0, Iteration: 9863, Q value: 0.7726, Reward: -1.0000, BF Gain pred: 5.88, BF Gain: 5.78, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25026\n",
            "Beam: 0, Iteration: 9864, Q value: -0.1701, Reward: -1.0000, BF Gain pred: 5.83, BF Gain: 7.53, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25032\n",
            "Beam: 0, Iteration: 9865, Q value: 1.2955, Reward: -1.0000, BF Gain pred: 1.89, BF Gain: 2.80, Critic Loss: 0.09, Policy Loss: -1.23\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25038\n",
            "Beam: 0, Iteration: 9866, Q value: 0.2182, Reward: 1.0000, BF Gain pred: 12.90, BF Gain: 9.15, Critic Loss: 0.14, Policy Loss: -1.24\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25044\n",
            "Beam: 0, Iteration: 9867, Q value: 0.7115, Reward: -1.0000, BF Gain pred: 12.71, BF Gain: 17.60, Critic Loss: 0.16, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25050\n",
            "Beam: 0, Iteration: 9868, Q value: 0.0644, Reward: -1.0000, BF Gain pred: 1.84, BF Gain: -1.79, Critic Loss: 0.10, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25056\n",
            "Beam: 0, Iteration: 9869, Q value: 0.1220, Reward: -1.0000, BF Gain pred: -2.18, BF Gain: 0.49, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25062\n",
            "Beam: 0, Iteration: 9870, Q value: 0.5922, Reward: 1.0000, BF Gain pred: 0.91, BF Gain: 3.18, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25068\n",
            "Beam: 0, Iteration: 9871, Q value: -0.0403, Reward: 1.0000, BF Gain pred: 6.15, BF Gain: 12.04, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25074\n",
            "Beam: 0, Iteration: 9872, Q value: 0.2837, Reward: -1.0000, BF Gain pred: 2.97, BF Gain: 3.47, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25080\n",
            "Beam: 0, Iteration: 9873, Q value: 0.6574, Reward: 1.0000, BF Gain pred: 11.15, BF Gain: 11.96, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25086\n",
            "Beam: 0, Iteration: 9874, Q value: -0.8670, Reward: -1.0000, BF Gain pred: 4.71, BF Gain: 3.43, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25092\n",
            "Beam: 0, Iteration: 9875, Q value: -0.4714, Reward: -1.0000, BF Gain pred: 2.07, BF Gain: -3.61, Critic Loss: 0.15, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25098\n",
            "Beam: 0, Iteration: 9876, Q value: 0.3917, Reward: 1.0000, BF Gain pred: 5.27, BF Gain: 7.44, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25100\n",
            "Beam: 0, Iteration: 9877, Q value: -0.3725, Reward: 1.0000, BF Gain pred: 5.82, BF Gain: 2.15, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25106\n",
            "Beam: 0, Iteration: 9878, Q value: -1.2418, Reward: -1.0000, BF Gain pred: 3.19, BF Gain: 3.40, Critic Loss: 0.13, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25112\n",
            "Beam: 0, Iteration: 9879, Q value: 0.2558, Reward: 1.0000, BF Gain pred: 8.48, BF Gain: 12.03, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25118\n",
            "Beam: 0, Iteration: 9880, Q value: 0.2392, Reward: 1.0000, BF Gain pred: 12.35, BF Gain: 10.22, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25120\n",
            "Beam: 0, Iteration: 9881, Q value: 0.0829, Reward: -1.0000, BF Gain pred: 3.79, BF Gain: 4.70, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25126\n",
            "Beam: 0, Iteration: 9882, Q value: -0.5288, Reward: 1.0000, BF Gain pred: 7.49, BF Gain: 12.89, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25132\n",
            "Beam: 0, Iteration: 9883, Q value: 0.9233, Reward: -1.0000, BF Gain pred: 6.55, BF Gain: 10.86, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25134\n",
            "Beam: 0, Iteration: 9884, Q value: -0.2372, Reward: 1.0000, BF Gain pred: 7.99, BF Gain: 2.67, Critic Loss: 0.12, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25140\n",
            "Beam: 0, Iteration: 9885, Q value: 0.0720, Reward: -1.0000, BF Gain pred: 2.75, BF Gain: 4.52, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25146\n",
            "Beam: 0, Iteration: 9886, Q value: -0.0016, Reward: 1.0000, BF Gain pred: 4.14, BF Gain: 5.31, Critic Loss: 0.10, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25152\n",
            "Beam: 0, Iteration: 9887, Q value: -0.2391, Reward: 1.0000, BF Gain pred: 6.13, BF Gain: 4.65, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25158\n",
            "Beam: 0, Iteration: 9888, Q value: 0.6314, Reward: -1.0000, BF Gain pred: 4.61, BF Gain: 5.69, Critic Loss: 0.12, Policy Loss: -1.09\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25160\n",
            "Beam: 0, Iteration: 9889, Q value: 0.1498, Reward: 1.0000, BF Gain pred: 6.94, BF Gain: 4.72, Critic Loss: 0.13, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25166\n",
            "Beam: 0, Iteration: 9890, Q value: -0.5492, Reward: -1.0000, BF Gain pred: -3.34, BF Gain: -3.46, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25172\n",
            "Beam: 0, Iteration: 9891, Q value: 0.5824, Reward: 1.0000, BF Gain pred: 13.77, BF Gain: 13.77, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25178\n",
            "Beam: 0, Iteration: 9892, Q value: 0.0669, Reward: -1.0000, BF Gain pred: 6.00, BF Gain: 4.02, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25184\n",
            "Beam: 0, Iteration: 9893, Q value: 1.2201, Reward: 1.0000, BF Gain pred: 13.27, BF Gain: 8.73, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25190\n",
            "Beam: 0, Iteration: 9894, Q value: -0.4637, Reward: -1.0000, BF Gain pred: 1.75, BF Gain: 1.75, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25196\n",
            "Beam: 0, Iteration: 9895, Q value: 0.9038, Reward: -1.0000, BF Gain pred: -1.66, BF Gain: 1.01, Critic Loss: 0.13, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25202\n",
            "Beam: 0, Iteration: 9896, Q value: 0.8335, Reward: 1.0000, BF Gain pred: 6.18, BF Gain: 10.40, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25208\n",
            "Beam: 0, Iteration: 9897, Q value: -0.2339, Reward: -1.0000, BF Gain pred: 5.83, BF Gain: 3.21, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25214\n",
            "Beam: 0, Iteration: 9898, Q value: 0.2291, Reward: -1.0000, BF Gain pred: 4.83, BF Gain: 5.92, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25220\n",
            "Beam: 0, Iteration: 9899, Q value: -0.0563, Reward: 1.0000, BF Gain pred: 7.66, BF Gain: 0.72, Critic Loss: 0.11, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25226\n",
            "Beam: 0, Iteration: 9900, Q value: -0.4674, Reward: -1.0000, BF Gain pred: -1.30, BF Gain: 0.81, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25232\n",
            "Beam: 0, Iteration: 9901, Q value: 0.7999, Reward: 1.0000, BF Gain pred: -0.03, BF Gain: 0.76, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25238\n",
            "Beam: 0, Iteration: 9902, Q value: 0.9821, Reward: 1.0000, BF Gain pred: 6.49, BF Gain: 8.05, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25244\n",
            "Beam: 0, Iteration: 9903, Q value: -1.2875, Reward: 1.0000, BF Gain pred: 7.87, BF Gain: 5.52, Critic Loss: 0.10, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25250\n",
            "Beam: 0, Iteration: 9904, Q value: 0.6502, Reward: -1.0000, BF Gain pred: 0.92, BF Gain: -2.38, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25256\n",
            "Beam: 0, Iteration: 9905, Q value: 1.2938, Reward: 1.0000, BF Gain pred: 3.65, BF Gain: 4.22, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25262\n",
            "Beam: 0, Iteration: 9906, Q value: -0.2108, Reward: 1.0000, BF Gain pred: 9.02, BF Gain: 9.02, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25268\n",
            "Beam: 0, Iteration: 9907, Q value: -0.2757, Reward: -1.0000, BF Gain pred: 6.08, BF Gain: 4.25, Critic Loss: 0.14, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25274\n",
            "Beam: 0, Iteration: 9908, Q value: 0.2986, Reward: 1.0000, BF Gain pred: 10.58, BF Gain: 10.09, Critic Loss: 0.14, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25280\n",
            "Beam: 0, Iteration: 9909, Q value: -0.0255, Reward: -1.0000, BF Gain pred: -4.82, BF Gain: -4.28, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25286\n",
            "Beam: 0, Iteration: 9910, Q value: 1.2014, Reward: 1.0000, BF Gain pred: 1.69, BF Gain: 2.13, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25292\n",
            "Beam: 0, Iteration: 9911, Q value: 0.7566, Reward: 1.0000, BF Gain pred: 8.31, BF Gain: 6.02, Critic Loss: 0.14, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25298\n",
            "Beam: 0, Iteration: 9912, Q value: 0.6674, Reward: -1.0000, BF Gain pred: 2.57, BF Gain: 2.91, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25304\n",
            "Beam: 0, Iteration: 9913, Q value: -0.8337, Reward: 1.0000, BF Gain pred: 3.21, BF Gain: 3.56, Critic Loss: 0.09, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25310\n",
            "Beam: 0, Iteration: 9914, Q value: 0.8175, Reward: -1.0000, BF Gain pred: 2.35, BF Gain: 2.35, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25316\n",
            "Beam: 0, Iteration: 9915, Q value: 0.5105, Reward: -1.0000, BF Gain pred: 2.02, BF Gain: 1.91, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25322\n",
            "Beam: 0, Iteration: 9916, Q value: 1.2843, Reward: 1.0000, BF Gain pred: 7.88, BF Gain: 8.40, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25328\n",
            "Beam: 0, Iteration: 9917, Q value: 0.4700, Reward: -1.0000, BF Gain pred: 7.04, BF Gain: 7.64, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25330\n",
            "Beam: 0, Iteration: 9918, Q value: 0.4782, Reward: -1.0000, BF Gain pred: 2.17, BF Gain: 2.17, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25336\n",
            "Beam: 0, Iteration: 9919, Q value: -0.1656, Reward: 1.0000, BF Gain pred: 4.67, BF Gain: 2.82, Critic Loss: 0.11, Policy Loss: -1.12\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25342\n",
            "Beam: 0, Iteration: 9920, Q value: 0.4909, Reward: -1.0000, BF Gain pred: -1.56, BF Gain: -3.03, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25348\n",
            "Beam: 0, Iteration: 9921, Q value: 0.4687, Reward: 1.0000, BF Gain pred: -0.38, BF Gain: 0.46, Critic Loss: 0.12, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25354\n",
            "Beam: 0, Iteration: 9922, Q value: 0.5885, Reward: 1.0000, BF Gain pred: 1.80, BF Gain: 2.16, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25360\n",
            "Beam: 0, Iteration: 9923, Q value: 0.4237, Reward: 1.0000, BF Gain pred: 7.24, BF Gain: 9.17, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25366\n",
            "Beam: 0, Iteration: 9924, Q value: 0.6755, Reward: -1.0000, BF Gain pred: 5.98, BF Gain: 5.34, Critic Loss: 0.10, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25372\n",
            "Beam: 0, Iteration: 9925, Q value: -0.2268, Reward: -1.0000, BF Gain pred: 1.24, BF Gain: 1.02, Critic Loss: 0.10, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25378\n",
            "Beam: 0, Iteration: 9926, Q value: 0.1816, Reward: 1.0000, BF Gain pred: 3.08, BF Gain: 3.16, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25384\n",
            "Beam: 0, Iteration: 9927, Q value: -0.0858, Reward: -1.0000, BF Gain pred: 2.53, BF Gain: 2.53, Critic Loss: 0.13, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25390\n",
            "Beam: 0, Iteration: 9928, Q value: 0.4418, Reward: 1.0000, BF Gain pred: 8.09, BF Gain: 5.62, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25396\n",
            "Beam: 0, Iteration: 9929, Q value: 0.3007, Reward: -1.0000, BF Gain pred: 4.79, BF Gain: 4.49, Critic Loss: 0.09, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25398\n",
            "Beam: 0, Iteration: 9930, Q value: -0.6876, Reward: -1.0000, BF Gain pred: 2.25, BF Gain: 2.25, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25404\n",
            "Beam: 0, Iteration: 9931, Q value: 0.4563, Reward: -1.0000, BF Gain pred: 1.31, BF Gain: 2.76, Critic Loss: 0.09, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25410\n",
            "Beam: 0, Iteration: 9932, Q value: 0.8298, Reward: -1.0000, BF Gain pred: 1.21, BF Gain: 1.21, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25416\n",
            "Beam: 0, Iteration: 9933, Q value: 0.1411, Reward: 1.0000, BF Gain pred: 2.78, BF Gain: 4.05, Critic Loss: 0.09, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25422\n",
            "Beam: 0, Iteration: 9934, Q value: 0.9129, Reward: 1.0000, BF Gain pred: 8.70, BF Gain: 7.65, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25428\n",
            "Beam: 0, Iteration: 9935, Q value: 1.0766, Reward: -1.0000, BF Gain pred: 2.30, BF Gain: -1.60, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25434\n",
            "Beam: 0, Iteration: 9936, Q value: 0.8753, Reward: 1.0000, BF Gain pred: 11.03, BF Gain: 8.38, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25436\n",
            "Beam: 0, Iteration: 9937, Q value: 1.0016, Reward: 1.0000, BF Gain pred: 16.91, BF Gain: 15.11, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25442\n",
            "Beam: 0, Iteration: 9938, Q value: 0.2650, Reward: -1.0000, BF Gain pred: 7.71, BF Gain: 8.03, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25448\n",
            "Beam: 0, Iteration: 9939, Q value: -0.4443, Reward: 1.0000, BF Gain pred: 7.74, BF Gain: 10.54, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25454\n",
            "Beam: 0, Iteration: 9940, Q value: -0.1432, Reward: -1.0000, BF Gain pred: 2.50, BF Gain: 3.11, Critic Loss: 0.08, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25460\n",
            "Beam: 0, Iteration: 9941, Q value: 0.8627, Reward: 1.0000, BF Gain pred: 4.16, BF Gain: 5.89, Critic Loss: 0.13, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25466\n",
            "Beam: 0, Iteration: 9942, Q value: 1.1245, Reward: -1.0000, BF Gain pred: 2.80, BF Gain: 0.22, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25472\n",
            "Beam: 0, Iteration: 9943, Q value: 1.2897, Reward: -1.0000, BF Gain pred: 1.28, BF Gain: 4.07, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25478\n",
            "Beam: 0, Iteration: 9944, Q value: 0.0666, Reward: -1.0000, BF Gain pred: -0.51, BF Gain: -2.38, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25484\n",
            "Beam: 0, Iteration: 9945, Q value: 0.9004, Reward: 1.0000, BF Gain pred: 5.30, BF Gain: 1.67, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25490\n",
            "Beam: 0, Iteration: 9946, Q value: -0.1471, Reward: -1.0000, BF Gain pred: 4.43, BF Gain: 4.94, Critic Loss: 0.10, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25496\n",
            "Beam: 0, Iteration: 9947, Q value: 1.0362, Reward: -1.0000, BF Gain pred: -3.07, BF Gain: 0.32, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25502\n",
            "Beam: 0, Iteration: 9948, Q value: 0.2758, Reward: 1.0000, BF Gain pred: 2.65, BF Gain: 2.65, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25508\n",
            "Beam: 0, Iteration: 9949, Q value: -0.4877, Reward: -1.0000, BF Gain pred: 1.51, BF Gain: 2.66, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25514\n",
            "Beam: 0, Iteration: 9950, Q value: 0.0945, Reward: 1.0000, BF Gain pred: 8.14, BF Gain: 6.64, Critic Loss: 0.09, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25520\n",
            "Beam: 0, Iteration: 9951, Q value: 0.6087, Reward: -1.0000, BF Gain pred: 5.55, BF Gain: 3.53, Critic Loss: 0.11, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25526\n",
            "Beam: 0, Iteration: 9952, Q value: 0.6073, Reward: -1.0000, BF Gain pred: 5.07, BF Gain: 5.07, Critic Loss: 0.09, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25532\n",
            "Beam: 0, Iteration: 9953, Q value: 0.8671, Reward: -1.0000, BF Gain pred: 1.53, BF Gain: 5.25, Critic Loss: 0.09, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25538\n",
            "Beam: 0, Iteration: 9954, Q value: 0.9988, Reward: 1.0000, BF Gain pred: 6.80, BF Gain: 9.91, Critic Loss: 0.09, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25544\n",
            "Beam: 0, Iteration: 9955, Q value: -1.7950, Reward: 1.0000, BF Gain pred: 7.10, BF Gain: 3.57, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25546\n",
            "Beam: 0, Iteration: 9956, Q value: -0.7846, Reward: -1.0000, BF Gain pred: 5.68, BF Gain: 5.68, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25552\n",
            "Beam: 0, Iteration: 9957, Q value: 0.2266, Reward: -1.0000, BF Gain pred: 5.04, BF Gain: 6.28, Critic Loss: 0.09, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25558\n",
            "Beam: 0, Iteration: 9958, Q value: 1.2025, Reward: -1.0000, BF Gain pred: 3.20, BF Gain: 2.20, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25564\n",
            "Beam: 0, Iteration: 9959, Q value: -0.1053, Reward: -1.0000, BF Gain pred: -4.93, BF Gain: -4.29, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25570\n",
            "Beam: 0, Iteration: 9960, Q value: 1.0602, Reward: 1.0000, BF Gain pred: 13.29, BF Gain: 9.95, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25576\n",
            "Beam: 0, Iteration: 9961, Q value: 0.5460, Reward: -1.0000, BF Gain pred: 7.74, BF Gain: 4.50, Critic Loss: 0.11, Policy Loss: -1.22\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25578\n",
            "Beam: 0, Iteration: 9962, Q value: 0.9601, Reward: 1.0000, BF Gain pred: 9.59, BF Gain: 8.58, Critic Loss: 0.11, Policy Loss: -1.24\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25584\n",
            "Beam: 0, Iteration: 9963, Q value: 0.4231, Reward: 1.0000, BF Gain pred: 11.83, BF Gain: 4.64, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25590\n",
            "Beam: 0, Iteration: 9964, Q value: 0.7624, Reward: -1.0000, BF Gain pred: 1.06, BF Gain: 1.06, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25596\n",
            "Beam: 0, Iteration: 9965, Q value: 0.7756, Reward: 1.0000, BF Gain pred: 9.86, BF Gain: 12.97, Critic Loss: 0.08, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25602\n",
            "Beam: 0, Iteration: 9966, Q value: 0.1390, Reward: -1.0000, BF Gain pred: -0.79, BF Gain: -2.17, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25608\n",
            "Beam: 0, Iteration: 9967, Q value: 0.7247, Reward: 1.0000, BF Gain pred: 6.35, BF Gain: 5.04, Critic Loss: 0.10, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25614\n",
            "Beam: 0, Iteration: 9968, Q value: 1.0324, Reward: 1.0000, BF Gain pred: 9.93, BF Gain: 9.93, Critic Loss: 0.10, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25620\n",
            "Beam: 0, Iteration: 9969, Q value: 0.9261, Reward: -1.0000, BF Gain pred: 2.42, BF Gain: 0.52, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25626\n",
            "Beam: 0, Iteration: 9970, Q value: 0.3816, Reward: 1.0000, BF Gain pred: 7.74, BF Gain: 7.97, Critic Loss: 0.11, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25632\n",
            "Beam: 0, Iteration: 9971, Q value: -0.7016, Reward: 1.0000, BF Gain pred: 9.20, BF Gain: 7.55, Critic Loss: 0.12, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25638\n",
            "Beam: 0, Iteration: 9972, Q value: -0.6305, Reward: -1.0000, BF Gain pred: 8.87, BF Gain: 8.87, Critic Loss: 0.11, Policy Loss: -1.16\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25644\n",
            "Beam: 0, Iteration: 9973, Q value: 0.0737, Reward: -1.0000, BF Gain pred: 2.93, BF Gain: 3.63, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25650\n",
            "Beam: 0, Iteration: 9974, Q value: 1.0987, Reward: 1.0000, BF Gain pred: 7.59, BF Gain: 6.46, Critic Loss: 0.12, Policy Loss: -1.14\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25652\n",
            "Beam: 0, Iteration: 9975, Q value: 0.2188, Reward: -1.0000, BF Gain pred: 2.64, BF Gain: 4.33, Critic Loss: 0.15, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25658\n",
            "Beam: 0, Iteration: 9976, Q value: 0.7616, Reward: 1.0000, BF Gain pred: 4.35, BF Gain: 8.82, Critic Loss: 0.09, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25664\n",
            "Beam: 0, Iteration: 9977, Q value: 0.8721, Reward: -1.0000, BF Gain pred: 3.88, BF Gain: 7.27, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25670\n",
            "Beam: 0, Iteration: 9978, Q value: 1.0241, Reward: 1.0000, BF Gain pred: 7.67, BF Gain: 17.03, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25676\n",
            "Beam: 0, Iteration: 9979, Q value: 0.6496, Reward: 1.0000, BF Gain pred: 11.80, BF Gain: 10.78, Critic Loss: 0.14, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25682\n",
            "Beam: 0, Iteration: 9980, Q value: -0.2014, Reward: -1.0000, BF Gain pred: 10.79, BF Gain: 14.17, Critic Loss: 0.12, Policy Loss: -1.11\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25688\n",
            "Beam: 0, Iteration: 9981, Q value: -1.2570, Reward: -1.0000, BF Gain pred: 3.59, BF Gain: 2.21, Critic Loss: 0.14, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25694\n",
            "Beam: 0, Iteration: 9982, Q value: 0.8456, Reward: 1.0000, BF Gain pred: 5.40, BF Gain: 9.36, Critic Loss: 0.12, Policy Loss: -1.13\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25700\n",
            "Beam: 0, Iteration: 9983, Q value: 0.4242, Reward: 1.0000, BF Gain pred: 7.19, BF Gain: 4.66, Critic Loss: 0.08, Policy Loss: -1.14\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25706\n",
            "Beam: 0, Iteration: 9984, Q value: -0.7863, Reward: 1.0000, BF Gain pred: 9.97, BF Gain: 4.84, Critic Loss: 0.10, Policy Loss: -1.15\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25712\n",
            "Beam: 0, Iteration: 9985, Q value: 0.1427, Reward: -1.0000, BF Gain pred: 1.74, BF Gain: 3.23, Critic Loss: 0.14, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25718\n",
            "Beam: 0, Iteration: 9986, Q value: 0.5093, Reward: -1.0000, BF Gain pred: 1.68, BF Gain: 1.68, Critic Loss: 0.12, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25724\n",
            "Beam: 0, Iteration: 9987, Q value: 0.6807, Reward: 1.0000, BF Gain pred: 6.37, BF Gain: 8.43, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25730\n",
            "Beam: 0, Iteration: 9988, Q value: 0.3004, Reward: 1.0000, BF Gain pred: 11.13, BF Gain: 15.04, Critic Loss: 0.08, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25736\n",
            "Beam: 0, Iteration: 9989, Q value: 0.4293, Reward: 1.0000, BF Gain pred: 11.99, BF Gain: 10.00, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25742\n",
            "Beam: 0, Iteration: 9990, Q value: 0.2597, Reward: -1.0000, BF Gain pred: 3.69, BF Gain: 3.44, Critic Loss: 0.10, Policy Loss: -1.18\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25748\n",
            "Beam: 0, Iteration: 9991, Q value: -0.5794, Reward: -1.0000, BF Gain pred: -0.40, BF Gain: 2.91, Critic Loss: 0.13, Policy Loss: -1.20\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25754\n",
            "Beam: 0, Iteration: 9992, Q value: 0.0071, Reward: 1.0000, BF Gain pred: 6.21, BF Gain: 3.60, Critic Loss: 0.12, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25760\n",
            "Beam: 0, Iteration: 9993, Q value: 0.4225, Reward: 1.0000, BF Gain pred: 6.38, BF Gain: 4.79, Critic Loss: 0.11, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25766\n",
            "Beam: 0, Iteration: 9994, Q value: 0.6406, Reward: 1.0000, BF Gain pred: 7.39, BF Gain: 10.28, Critic Loss: 0.12, Policy Loss: -1.16\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25768\n",
            "Beam: 0, Iteration: 9995, Q value: 0.7906, Reward: -1.0000, BF Gain pred: 5.00, BF Gain: 2.31, Critic Loss: 0.09, Policy Loss: -1.17\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25774\n",
            "Beam: 0, Iteration: 9996, Q value: 0.6221, Reward: 1.0000, BF Gain pred: 8.33, BF Gain: 9.20, Critic Loss: 0.13, Policy Loss: -1.19\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25780\n",
            "Beam: 0, Iteration: 9997, Q value: 0.6725, Reward: 1.0000, BF Gain pred: 8.45, BF Gain: 9.58, Critic Loss: 0.12, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25786\n",
            "Beam: 0, Iteration: 9998, Q value: -1.3544, Reward: -1.0000, BF Gain pred: 3.71, BF Gain: 3.58, Critic Loss: 0.12, Policy Loss: -1.22\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25792\n",
            "Beam: 0, Iteration: 9999, Q value: 0.9384, Reward: 1.0000, BF Gain pred: 10.26, BF Gain: 10.26, Critic Loss: 0.11, Policy Loss: -1.21\n",
            "\n",
            " Added best experiences \n",
            "\n",
            "Best gain pred tensor([[24.4133]], device='cuda:0', dtype=torch.float64) Best gain tensor([[29.9210]], device='cuda:0', dtype=torch.float64)\n",
            "Best gain pred and | length 4\n",
            "Length of the buffer: 25798\n"
          ]
        },
        {
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: 'pretrained_model/'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-7316d7f62fa0>\u001b[0m in \u001b[0;36m<cell line: 764>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-7316d7f62fa0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(options, train_options, beam_id, exp_id)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrain_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall_iter'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_freq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained_model_actual/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pretrained_model/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pretrained_model/beam'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeam_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_iter'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'overall_iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcritic_net1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'pretrained_model/'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Critic_(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Critic_, self).__init__()\n",
        "\n",
        "        self.scaling_factor = 16\n",
        "        self.fc1 = nn.Linear(input_size, self.scaling_factor * input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(self.scaling_factor * input_size)\n",
        "        self.fc2 = nn.Linear(self.scaling_factor * input_size, self.scaling_factor * output_size)\n",
        "        self.bn2 = nn.BatchNorm1d(self.scaling_factor * output_size)\n",
        "        self.fc3 = nn.Linear(self.scaling_factor * output_size, output_size)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        #print(\"state, action\", state, action)\n",
        "        x = torch.cat((state, action), 1)\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, ch, p_factor):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.M = int(input_size / 2)\n",
        "        self.output_size = output_size\n",
        "        self.penalty_factor = torch.tensor(p_factor).float().cuda()\n",
        "\n",
        "        # just for debugging\n",
        "        self.ch_r = torch.from_numpy(ch[:, :self.M].transpose()).float().cuda()\n",
        "        self.ch_i = torch.from_numpy(ch[:, self.M:].transpose()).float().cuda()\n",
        "\n",
        "        # self.H_r = nn.Parameter(torch.randn(self.M, 16))\n",
        "        self.H_r = torch.from_numpy(ch[:, :self.M].transpose()).float().cuda()\n",
        "        # self.H_i = nn.Parameter(torch.randn(self.M, 16))\n",
        "        self.H_i = torch.from_numpy(ch[:, self.M:].transpose()).float().cuda()\n",
        "\n",
        "        self.Thetas = nn.Parameter(torch.randn(self.M, 32))\n",
        "        self.R = nn.Parameter(torch.rand(self.M, 32))\n",
        "\n",
        "        self.fc1 = nn.Linear(522, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        # x = torch.cat((state, action), 1)\n",
        "\n",
        "        # self.H_r = (1 / torch.sqrt(torch.tensor(self.M))) * torch.cos(self.Thetas) * self.R\n",
        "        # self.H_i = (1 / torch.sqrt(torch.tensor(self.M))) * torch.sin(self.Thetas) * self.R\n",
        "\n",
        "        x_state = phase2bf(state)\n",
        "        x_state_r, x_state_i = x_state[:, :self.M], x_state[:, self.M:]\n",
        "\n",
        "        z_r = x_state_r @ self.H_r + x_state_i @ self.H_i\n",
        "        z_i = x_state_r @ self.H_i - x_state_i @ self.H_r\n",
        "\n",
        "        z = z_r ** 2 + z_i ** 2\n",
        "        z = z ** self.penalty_factor\n",
        "        # z_min = torch.min(z, dim=1).values.reshape(-1, 1)\n",
        "\n",
        "        x_action = phase2bf(action)\n",
        "        x_action_r, x_action_i = x_action[:, :self.M], x_action[:, self.M:]\n",
        "\n",
        "        u_r = x_action_r @ self.H_r + x_action_i @ self.H_i\n",
        "        u_i = x_action_r @ self.H_i - x_action_i @ self.H_r\n",
        "\n",
        "        u = u_r ** 2 + u_i ** 2\n",
        "        u = u ** self.penalty_factor\n",
        "        # u_min = torch.min(u, dim=1).values.reshape(-1, 1)\n",
        "\n",
        "        # ----------- up to this point ----------- #\n",
        "        # \"z\" is of size (batch, S)\n",
        "        # \"u\" is of size (batch, S)\n",
        "        # ---------------------------------------- #\n",
        "\n",
        "        feature = torch.cat((z, u), dim=1)\n",
        "        out = torch.relu(self.fc1(feature))\n",
        "        out = torch.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        # out = 10 * torch.log10(torch.mean(u, dim=1).reshape(-1, 1)) - 10 * torch.log10(torch.mean(z, dim=1).reshape(-1, 1))\n",
        "\n",
        "        # ----------- this one works ----------- #\n",
        "        # out = torch.mean(10 * torch.log10(u), dim=1).reshape(-1, 1) - torch.mean(10 * torch.log10(z), dim=1).reshape(-1, 1)\n",
        "        # -------------------------------------- #\n",
        "\n",
        "        # out = -torch.mean(torch.divide(torch.tensor(1.), u), dim=1).reshape(-1, 1) + \\\n",
        "        #       torch.mean(torch.divide(torch.tensor(1.), z), dim=1).reshape(-1, 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.pi = torch.tensor(np.pi).float().cuda()\n",
        "        self.scaling_factor = 16\n",
        "        self.fc1 = nn.Linear(input_size, self.scaling_factor * input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(self.scaling_factor * input_size)\n",
        "        self.fc2 = nn.Linear(self.scaling_factor * input_size, self.scaling_factor * output_size)\n",
        "        self.bn2 = nn.BatchNorm1d(self.scaling_factor * output_size)\n",
        "        self.fc3 = nn.Linear(self.scaling_factor * output_size, output_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.bn1(self.fc1(state)))\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = torch.tanh(self.fc3(x)) * self.pi\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Actor_(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Actor_, self).__init__()\n",
        "        self.pi = torch.tensor(np.pi).float().cuda()\n",
        "        self.scaling_factor = 1\n",
        "        self.fc1 = nn.Linear(input_size, self.scaling_factor * input_size)\n",
        "        # self.bn1 = nn.BatchNorm1d(self.scaling_factor * input_size)\n",
        "        # self.fc2 = nn.Linear(self.scaling_factor * input_size, self.scaling_factor * output_size)\n",
        "        # self.bn2 = nn.BatchNorm1d(self.scaling_factor * output_size)\n",
        "        # self.fc3 = nn.Linear(self.scaling_factor * output_size, output_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # x = F.relu(self.bn1(self.fc1(state)))\n",
        "        # x = F.relu(self.bn2(self.fc2(x)))\n",
        "        # x = torch.tanh(self.fc3(x)) * self.pi\n",
        "        x = self.fc1(state)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.uniform_(m.weight, -0.01, 0.01)\n",
        "        m.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "class OUNoise(object):\n",
        "    def __init__(self, action_shape, mu=0.0, theta=0.15, max_sigma=1, min_sigma=0.08, decay_period=10000):\n",
        "        self.mu = mu\n",
        "        self.theta = theta\n",
        "        self.sigma = max_sigma\n",
        "        self.max_sigma = max_sigma\n",
        "        self.min_sigma = min_sigma\n",
        "        self.decay_period = decay_period\n",
        "        self.action_dim = action_shape\n",
        "        self.low = -np.pi\n",
        "        self.high = np.pi\n",
        "        self.state = self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        state = torch.ones(self.action_dim) * self.mu\n",
        "        return state.float().cuda()\n",
        "\n",
        "    def evolve_state(self):\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * torch.normal(0, 1, size=self.action_dim).cuda()\n",
        "        self.state = x + dx\n",
        "        return self.state\n",
        "\n",
        "    def get_action(self, action, t=0):\n",
        "        ou_state = self.evolve_state()\n",
        "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
        "        # return torch.clamp(action + ou_state, self.low, self.high)\n",
        "        return action + ou_state\n",
        "\n",
        "\n",
        "def phase2bf(ph_mat):\n",
        "    # ph_mat: (i) a tensor, (ii) B x M\n",
        "    # bf_mat: (i) a tensor, (ii) B x 2M\n",
        "    # B stands for batch size and M is the number of antenna\n",
        "\n",
        "    M = torch.tensor(ph_mat.shape[1]).to(ph_mat.device)\n",
        "    bf_mat = torch.exp(1j * ph_mat)\n",
        "    bf_mat_r = torch.real(bf_mat)\n",
        "    bf_mat_i = torch.imag(bf_mat)\n",
        "\n",
        "    bf_mat_ = (1 / torch.sqrt(M)) * torch.cat((bf_mat_r, bf_mat_i), dim=1)\n",
        "\n",
        "    return bf_mat_\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as scio\n",
        "#from DDPG_classes import Actor, Critic_, OUNoise, init_weights\n",
        "#from env_ddpg import envCB\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "def train(options, train_options, beam_id, exp_id):\n",
        "\n",
        "    with torch.cuda.device(options['gpu_idx']):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        print('Beam', beam_id, 'training begins. GPU being used:', torch.cuda.current_device())\n",
        "        options['best_value'] = torch.tensor(-30).cuda()\n",
        "        options['best_state'] = torch.rand(1,25).cuda()\n",
        "        options['ph_table_rep'] = options['ph_table_rep'].cuda()\n",
        "        options['multi_step'] = options['multi_step'].cuda()\n",
        "        options['ph_table'] = options['ph_table'].cuda()\n",
        "\n",
        "\n",
        "        options['ph_table_rep'] = options['ph_table_rep'].cuda()\n",
        "        options['multi_step'] = options['multi_step'].cuda()\n",
        "        options['ph_table'] = options['ph_table'].cuda()\n",
        "\n",
        "\n",
        "        real_time_perf_true = np.zeros((train_options['num_iter'],))\n",
        "        real_time_perf_n_true = np.zeros((train_options['num_iter'],))\n",
        "\n",
        "        real_time_perf = np.zeros((train_options['num_iter'],))\n",
        "        real_time_perf_n = np.zeros((train_options['num_iter'],))\n",
        "\n",
        "        actor_net1 = Actor(options['num_ant'], options['num_ant'])\n",
        "        critic_net1 = Critic_(2 * options['num_ant'], 1)\n",
        "        actor_net1.cuda()\n",
        "        actor_net1.apply(init_weights)\n",
        "        actor_net1.apply(init_weights)\n",
        "        critic_net1.cuda()\n",
        "        critic_net1.apply(init_weights)\n",
        "        critic_net1.apply(init_weights)\n",
        "        ounoise = OUNoise((1, options['num_ant']))\n",
        "\n",
        "        env_1 = envCB_1(options['num_ant'], options['num_bits'], beam_id, options, exp_id)\n",
        "        env_2 = envCB_1(options['num_ant'], options['num_bits'], beam_id, options, exp_id)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        actor_net1.cuda()\n",
        "        critic_net1.cuda()\n",
        "        actor_net1.apply(init_weights)\n",
        "        critic_net1.apply(init_weights)\n",
        "\n",
        "\n",
        "        CB_Env_1 = env_1\n",
        "        CB_Env_2 = env_2\n",
        "        critic_criterion = nn.MSELoss()\n",
        "\n",
        "        critic_optimizer1 = optim.Adam(critic_net1.parameters(), lr=1e-3)\n",
        "        actor_optimizer1 = optim.Adam(actor_net1.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "        if train_options['overall_iter'] == 1:\n",
        "            state = (torch.rand((1, options['num_ant'])).float().cuda() * 2 * torch.pi) - torch.pi  # vector of phases\n",
        "            print('Initial State Activated.')\n",
        "        else:\n",
        "            state = train_options['state']\n",
        "        epsi_ones = np.ones([1,95])*0.1\n",
        "        epsilon_ddpg = [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1] + epsi_ones.squeeze().tolist()\n",
        "\n",
        "\n",
        "        best_experiences_pred =  []\n",
        "        best_experiences =  []\n",
        "        # -------------- training -------------- #\n",
        "        replay_memory = train_options['replay_memory']\n",
        "        iteration = 0\n",
        "        num_of_iter = train_options['num_iter']\n",
        "\n",
        "        best_states, best_actions_pred, best_rewards_pred, best_values_pred = None, None, float('-inf'), float('-inf')\n",
        "        best_states, best_actions, best_rewards, best_values = None, None, float('-inf'),float('-inf')\n",
        "        revisit_frequency = 1000\n",
        "        count = 0\n",
        "        bf_gain_pred_prev = float('-inf')\n",
        "        while iteration < num_of_iter:\n",
        "\n",
        "            state = state.float().cuda()\n",
        "            actor_net1.eval()\n",
        "\n",
        "            user_num = 1\n",
        "            action_pred = torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            reward_pred =torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            action_quant_pred =torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            state_1_pred =torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            state_1  = torch.zeros(user_num,options['num_ant']).cuda()\n",
        "            reward = torch.zeros(user_num,).cuda()\n",
        "            bf_gain = torch.zeros(user_num,).cuda()\n",
        "            bf_gain_pred = torch.zeros(user_num,).cuda()\n",
        "\n",
        "            random_number = np.random.rand()> epsilon_ddpg[count]\n",
        "\n",
        "            if random_number  and iteration>1000:  # 10% probability to revisit best state\n",
        "                if len(replay_memory) > train_options['replay_memory_size'] -len(best_experiences)-len(best_experiences_pred):\n",
        "                    replay_memory = replay_memory[len(best_experiences):]\n",
        "                    replay_memory = replay_memory[len(best_experiences_pred):]\n",
        "                print(\"\\n Added best experiences \\n\")\n",
        "                replay_memory = replay_memory+best_experiences\n",
        "                replay_memory = replay_memory+best_experiences_pred\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            action_pred = actor_net1(state)\n",
        "            reward_pred, bf_gain_pred, action_quant_pred, state_1_pred = CB_Env_1.get_reward(action_pred) #!!!!!\n",
        "            critic_net1.eval()\n",
        "            q_pred1 = critic_net1(state, action_quant_pred)\n",
        "            action_pred_noisy = ounoise.get_action(action_pred, t=train_options['overall_iter'])  # torch.Size([1, action_dim])\n",
        "            mat_dist = torch.abs(action_pred_noisy.reshape(options['num_ant'], 1) - options['ph_table_rep'])\n",
        "            action_quant = options['ph_table_rep'][range(options['num_ant']), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
        "\n",
        "            state_1, reward, bf_gain, terminal1 = CB_Env_1.step(action_quant)  # get next state and reward\n",
        "            action = action_quant.reshape((1, -1)).float().cuda()  # best action according0ly\n",
        "            real_time_perf[iteration] = torch.Tensor.cpu(bf_gain_pred.detach()).numpy()\n",
        "            real_time_perf_n[iteration] = torch.Tensor.cpu(bf_gain.detach()).numpy()\n",
        "\n",
        "\n",
        "            if torch.abs(bf_gain_pred -bf_gain_pred_prev)<0.01 and bf_gain_pred<30:\n",
        "                reward -= 2 # Add a penalty for being stuck\n",
        "            bf_gain_pred_prev = bf_gain_pred\n",
        "\n",
        "\n",
        "            if  bf_gain_pred > options['best_value']:\n",
        "\n",
        "                options['best_value'] = bf_gain_pred.clone()\n",
        "                options['best_state'] = state_1_pred.clone()\n",
        "\n",
        "            replay_memory.append((state, action, reward, state_1, terminal1))\n",
        "            replay_memory.append((state, action_pred,reward_pred, state_1_pred, terminal1))\n",
        "\n",
        "            min_SIR = 23\n",
        "            if bf_gain_pred> min_SIR :\n",
        "\n",
        "                best_experiences_pred.append((state.clone(), action_pred.clone(),reward_pred.clone(), state_1_pred.clone(), terminal1))\n",
        "                print(\"best_experiences_pred length\", len(best_experiences_pred ))\n",
        "                if bf_gain_pred > best_values_pred:\n",
        "                    best_states = state\n",
        "                    best_actions_pred = action_quant_pred\n",
        "                    best_rewards_pred = reward_pred\n",
        "                    best_values_pred = bf_gain_pred\n",
        "\n",
        "            if bf_gain > min_SIR:\n",
        "                best_experiences.append((state.clone(), action.clone(),reward.clone(), state_1.clone(), terminal1))\n",
        "                print(\"best_experiences length\", len(best_experiences ))\n",
        "                if bf_gain > best_values:\n",
        "                    best_states = state\n",
        "                    best_actions = action\n",
        "                    best_rewards = reward\n",
        "                    best_values = bf_gain\n",
        "            best_experiences_pred = list(set(best_experiences_pred))\n",
        "            best_experiences = list(set(best_experiences))\n",
        "            print(\"Best gain pred\",best_values_pred,\"Best gain\", best_values)\n",
        "            print(\"Best gain pred and | length\",len(best_experiences)+len(best_experiences_pred))\n",
        "            print(\"Length of the buffer:\", len(replay_memory))\n",
        "            #print(\"Epsilon:\", epsilon_ddpg[count])\n",
        "            if iteration%1000 == 0:\n",
        "                count +=1\n",
        "            #print(\"MAG state\", mag_state.shape)\n",
        "            #print(\"State:\", state,\"\\nAction_pred_quant\", action_quant_pred,\"\\nAction_quant\", action)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            while len(replay_memory) > train_options['replay_memory_size']:\n",
        "                replay_memory.pop(0)\n",
        "\n",
        "            # -------------- Experience Replay -------------- #\n",
        "            minibatch = random.sample(replay_memory, min(len(replay_memory), train_options['minibatch_size']))\n",
        "\n",
        "            # unpack minibatch, since torch.cat is by default dim=0, which is the dimension of batch\n",
        "            state_batch = torch.cat(tuple(d[0] for d in minibatch))  # torch.Size([*, state_dim])\n",
        "            action_batch = torch.cat(tuple(d[1] for d in minibatch))  # torch.Size([*, action_dim])\n",
        "            reward_batch = torch.cat(tuple(d[2] for d in minibatch))  # torch.Size([*, 1])\n",
        "            state_1_batch = torch.cat(tuple(d[3] for d in minibatch))  # torch.Size([*, state_dim])\n",
        "\n",
        "            state_batch = state_batch.detach()\n",
        "            action_batch = action_batch.detach()\n",
        "            reward_batch = reward_batch.detach()\n",
        "            state_1_batch = state_1_batch.detach()\n",
        "\n",
        "            if torch.cuda.is_available():  # put on GPU if CUDA is available\n",
        "                state_batch = state_batch.cuda()\n",
        "                action_batch = action_batch.cuda()\n",
        "                reward_batch = reward_batch.cuda()\n",
        "                state_1_batch = state_1_batch.cuda()\n",
        "\n",
        "            # loss calculation for Critic Network\n",
        "            critic_net1.train()\n",
        "            Q_prime1 = reward_batch\n",
        "            Q_pred1 = critic_net1(state_batch, action_batch)\n",
        "            critic_loss1 = critic_criterion(Q_pred1, Q_prime1.detach())\n",
        "\n",
        "            # Update Critic Network\n",
        "            critic_optimizer1.zero_grad()\n",
        "            critic_loss1.backward()\n",
        "            critic_optimizer1.step()\n",
        "\n",
        "            # loss calculation for Actor Network\n",
        "\n",
        "\n",
        "\n",
        "            actor_net1.train()\n",
        "            critic_net1.eval()\n",
        "            '''\n",
        "\n",
        "\n",
        "            '''\n",
        "            actor_loss1 = torch.mean(-critic_net1(state_batch, actor_net1(state_batch)))\n",
        "            #actor_loss1 = -torch.mean(target_critic_net1(state_batch, actor_net1(state_batch)))\n",
        "            actor_optimizer1.zero_grad()\n",
        "            actor_loss1.backward()\n",
        "            actor_optimizer1.step()\n",
        "\n",
        "            # UPDATE state, epsilon, target network, etc.\n",
        "            state = state_1_pred\n",
        "            iteration += 1\n",
        "            train_options['overall_iter'] += 1  # global counter\n",
        "\n",
        "            if train_options['overall_iter'] % options['save_freq'] == 0:\n",
        "                if not os.path.exists('pretrained_model_actual/'):\n",
        "                    os.mkdir('pretrained_model/')\n",
        "                PATH = 'pretrained_model/beam' + str(beam_id) + '_iter' + str(train_options['overall_iter']) + '.pth'\n",
        "                torch.save(critic_net1.state_dict(), PATH)\n",
        "                torch.save(actor_net1.state_dict(), PATH)\n",
        "\n",
        "            # store: best beamforming vector so far\n",
        "            if train_options['overall_iter'] % options['pf_print'] == 0:\n",
        "                iter_id = np.array(train_options['overall_iter']).reshape(1, 1)\n",
        "                best_state = CB_Env_1.best_bf_vec.reshape(1, -1)\n",
        "                if os.path.exists('pfs_actual/pf_' + str(beam_id) + '.txt'):\n",
        "                    with open('pfs/pf_' + str(beam_id) + '.txt', 'ab') as bm:\n",
        "                        np.savetxt(bm, iter_id, fmt='%d', delimiter='\\n')\n",
        "                    with open('pfs/pf_' + str(beam_id) + '.txt', 'ab') as bm:\n",
        "                        np.savetxt(bm, best_state, fmt='%.5f', delimiter=',')\n",
        "                else:\n",
        "                    np.savetxt('pfs/pf_' + str(beam_id) + '.txt', iter_id, fmt='%d', delimiter='\\n')\n",
        "                    with open('pfs/pf_' + str(beam_id) + '.txt', 'ab') as bm:\n",
        "                        np.savetxt(bm, best_state, fmt='%.5f', delimiter=',')\n",
        "\n",
        "            # plotting\n",
        "            # ax.plot(range(iteration), real_time_perf_true[:iteration], '-k', alpha=0.7, label='DRL Performance')\n",
        "            # ax.plot(range(iteration), real_time_perf_n_true[:iteration], '-b', alpha=0.3, label='Noise Performance')\n",
        "            # # ax.plot(range(iteration), np.ones(iteration) * options['target'], '--m', alpha=1.0, label='Victory Claimed')\n",
        "            # # ax.set_xscale('log')\n",
        "            # ax.grid(True)\n",
        "            # ax.legend()\n",
        "            # plt.draw()\n",
        "            # plt.pause(0.001)\n",
        "            # ax.cla()\n",
        "            print(\n",
        "                \"Beam: %d, Iteration: %d, Q value: %.4f, Reward: %.4f, BF Gain pred: %.2f, BF Gain: %.2f, Critic Loss: %.2f, Policy Loss: %.2f\" % \\\n",
        "                (beam_id, train_options['overall_iter'],\n",
        "                 torch.Tensor.cpu(q_pred1.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(reward_pred).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(bf_gain_pred.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(bf_gain.detach()).numpy().squeeze(),\n",
        "\n",
        "                 torch.Tensor.cpu(critic_loss1.detach()).numpy().squeeze(),\n",
        "                 torch.Tensor.cpu(actor_loss1.detach()).numpy().squeeze()))\n",
        "\n",
        "        # Training Communication Interface\n",
        "        train_options['replay_memory'] = replay_memory  # used for the next loop\n",
        "        train_options['state'] = state  # used for the next loop\n",
        "        train_options['best_state'] = CB_Env_1.best_bf_vec  # used for clustering and assignment\n",
        "\n",
        "        file_name = '/content/drive/MyDrive/Colab_Notebooks/rl_results/rl_result_actual_' + str(exp_id) + '.mat'\n",
        "        scio.savemat(file_name,\n",
        "                     {'agent': real_time_perf,\n",
        "                      'noise': real_time_perf_n})\n",
        "\n",
        "    return train_options\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class envCB_1:\n",
        "\n",
        "    def __init__(self, num_ant, num_bits, idx, options, exp_idx):\n",
        "\n",
        "        self.idx = idx\n",
        "        self.exp_idx = exp_idx\n",
        "        self.num_ant = num_ant\n",
        "        self.num_bits = num_bits\n",
        "        self.cb_size = 2 ** self.num_bits\n",
        "        self.codebook = self.codebook_gen()\n",
        "\n",
        "\n",
        "        self.state = torch.zeros((1, self.num_ant)).float().cuda()\n",
        "        self.bf_vec = self.init_bf_vec()\n",
        "        self.previous_gain = 0\n",
        "        self.previous_gain_pred = 0\n",
        "        self.th_step = 0.01\n",
        "        self.threshold = torch.tensor([-100]).float().cuda()\n",
        "        self.count = 1\n",
        "        self.record_freq = 10\n",
        "        self.record_decay_th = 1000\n",
        "        self.achievement = torch.tensor([0]).float().cuda()\n",
        "        self.gain_record = [np.array(-100.)]\n",
        "        self.N_count = 1\n",
        "        self.best_bf_vec = self.init_best()\n",
        "        # self.opt_bf_gain()\n",
        "        self.options = options\n",
        "\n",
        "        self.ch_bs_irs =torch.tensor( options['ch_bs_irs'][5:]).cuda() #torch.from_numpy(channel).float().cuda()\n",
        "        self.ch_bs_irs_i =torch.tensor( options['ch_bs_irs_i'][5:]).cuda()\n",
        "        self.ch_bs_irs_i2 =torch.tensor( options['ch_bs_irs_i2'][5:]).cuda()\n",
        "        self.channel1 = torch.tensor( options['ch_t3'][5:]).cuda()\n",
        "\n",
        "\n",
        "    def step(self, input_action):  # input_action: (1, num_ant), rep: phase vector\n",
        "        self.state = input_action\n",
        "        reward, bf_gain = self.reward_fn()\n",
        "        terminal = 0\n",
        "        return self.state.clone(), reward, bf_gain, terminal\n",
        "\n",
        "    def reward_fn(self):\n",
        "        y_min = -70\n",
        "        y_max = 37\n",
        "        state_temp = self.state\n",
        "        state_temp = state_temp.reshape(2,-1)\n",
        "        bf_gain  = self.BF_GAIN_signal( state_temp.reshape(1,-1),  self.ch_bs_irs, self.channel1 ) - self.BF_GAIN_interf( state_temp.reshape(1,-1),  self.ch_bs_irs_i,self.ch_bs_irs_i2, self.channel1 )\n",
        "\n",
        "        #bf_gain = self.model_signal(torch.concat((self.prefix.reshape(1,-1),state_temp.reshape(1,-1)),1))\n",
        "        #print(\"bf_gain1\", bf_gain1, bf_gain1.shape)\n",
        "\n",
        "        if bf_gain > self.previous_gain:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif(self.state, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        else:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif(self.state, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        self.previous_gain = self.previous_gain_pred\n",
        "        return reward, bf_gain\n",
        "\n",
        "    def get_reward(self, input_action):\n",
        "        y_min = -70\n",
        "        y_max = 37\n",
        "\n",
        "        inner_state = input_action\n",
        "\n",
        "        self.options['ph_table_rep-1'] = self.options['ph_table'].repeat(self.num_ant, 1)\n",
        "        mat_dist = torch.abs(inner_state.reshape(-1, 1) - self.options['ph_table_rep-1'])\n",
        "        # Quantization Processing\n",
        "        # self.options['ph_table_rep'].cuda()\n",
        "        action_quant = self.options['ph_table_rep-1'][range(self.num_ant), torch.argmin(mat_dist, dim=1)].reshape(1, -1)\n",
        "        #print(\"action quant in reward\", action_quant.shape)\n",
        "        bf_gain  = self.BF_GAIN_signal( action_quant.reshape(1,-1),  self.ch_bs_irs, self.channel1 ) - self.BF_GAIN_interf( action_quant.reshape(1,-1),  self.ch_bs_irs_i,self.ch_bs_irs_i2, self.channel1 )\n",
        "        #print(\"bf_gain1\", bf_gain1, bf_gain1.shape)\n",
        "\n",
        "\n",
        "        if bf_gain > self.previous_gain_pred:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif_get_reward(action_quant, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        else:\n",
        "            if bf_gain > self.threshold:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "                self.threshold = self.threshold_modif_get_reward(action_quant, bf_gain)\n",
        "                # print('threshold reset to: %.1f.' % self.threshold)\n",
        "            else:\n",
        "                reward = torch.from_numpy(np.array([-1]).reshape((1, 1))).float()\n",
        "                # reward = bf_gain - self.previous_gain_pred\n",
        "                # reward = reward.reshape((1, 1))\n",
        "        # if self.count % self.record_freq == 0:\n",
        "        #     self.gain_vs_iter()\n",
        "        #     if self.count == self.record_decay_th:\n",
        "        #         self.record_freq = 1000\n",
        "        self.previous_gain_pred = bf_gain\n",
        "        self.count += 1\n",
        "        return reward, bf_gain, (action_quant.clone()).reshape(1,-1),(action_quant.clone()).reshape(1,-1)\n",
        "\n",
        "    def threshold_modif(self, ph_vec, bf_gain):\n",
        "        self.achievement = bf_gain\n",
        "        self.gain_recording(ph_vec, self.idx)\n",
        "        # self.threshold += self.th_step\n",
        "        self.threshold = bf_gain\n",
        "        return self.threshold\n",
        "\n",
        "    def threshold_modif_get_reward(self, inner_bf, bf_gain):\n",
        "        self.achievement = bf_gain\n",
        "        self.gain_recording(inner_bf, self.idx)\n",
        "        # self.threshold += self.th_step\n",
        "        self.threshold = bf_gain\n",
        "        return self.threshold\n",
        "\n",
        "    def BF_GAIN_signal(self, irs_phase,  ch_bs_irs, ch_signal ):\n",
        "\n",
        "      Theta = torch.exp(1j*irs_phase.reshape(1,-1)).cuda()\n",
        "      B = Theta.reshape(-1,1) * ch_bs_irs.reshape(-1,1)\n",
        "      H = ch_signal.cuda().conj().reshape(1,-1) @ B\n",
        "\n",
        "      channel_gain = 10*torch.log10(torch.abs(H)**2)\n",
        "\n",
        "      return channel_gain\n",
        "\n",
        "    def BF_GAIN_interf(self, irs_phase,  ch_bs_irs1,ch_bs_irs2,  ch_signal ):\n",
        "\n",
        "      Theta = torch.exp(1j*irs_phase.reshape(1,-1)).cuda()\n",
        "      B1 = Theta.reshape(-1,1) * ch_bs_irs1.reshape(-1,1)\n",
        "      H1 = ch_signal.cuda().conj().reshape(1,-1) @ B1\n",
        "\n",
        "      B2 = Theta.reshape(-1,1) * ch_bs_irs2.reshape(-1,1)\n",
        "      H2 = ch_signal.cuda().conj().reshape(1,-1) @ B2\n",
        "      channel_gain = 10*torch.log10(torch.abs(H1)**2+torch.abs(H2)**2)\n",
        "\n",
        "      return channel_gain\n",
        "\n",
        "\n",
        "    def gain_recording(self, bf_vec, idx):\n",
        "        new_gain = torch.Tensor.cpu(self.achievement).detach().numpy().reshape((1, 1))\n",
        "        bf_print = torch.Tensor.cpu(bf_vec).detach().numpy().reshape(1, -1)\n",
        "        if new_gain > max(self.gain_record):\n",
        "            self.gain_record.append(new_gain)\n",
        "            self.best_bf_vec = torch.Tensor.cpu(bf_vec).detach().numpy().reshape(1, -1)\n",
        "            if os.path.exists('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt'):\n",
        "                with open('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', 'ab') as bm:\n",
        "                    np.savetxt(bm, new_gain, fmt='%.2f', delimiter='\\n')\n",
        "                with open('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', 'ab') as bm:\n",
        "                    np.savetxt(bm, bf_print, fmt='%.5f', delimiter=',')\n",
        "            else:\n",
        "                np.savetxt('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', new_gain, fmt='%.2f', delimiter='\\n')\n",
        "                # with open('beams/beams_' + str(idx) + '_max.txt', 'ab') as bm:\n",
        "                #     np.savetxt(bm, new_gain, fmt='%.2f', delimiter='\\n')\n",
        "                with open('beams/beams_' + str(idx) + '_exp_' + str(self.exp_idx) + '_max.txt', 'ab') as bm:\n",
        "                    np.savetxt(bm, bf_print, fmt='%.5f', delimiter=',')\n",
        "\n",
        "    def codebook_gen(self):\n",
        "        angles = np.linspace(0, 2 * np.pi, self.cb_size, endpoint=False)\n",
        "        cb = np.exp(1j * angles)\n",
        "        codebook = torch.zeros((self.cb_size, 2))  # shape of the codebook\n",
        "        for ii in range(cb.shape[0]):\n",
        "            codebook[ii, 0] = torch.tensor(np.real(cb[ii]))\n",
        "            codebook[ii, 1] = torch.tensor(np.imag(cb[ii]))\n",
        "        return codebook\n",
        "\n",
        "    def init_bf_vec(self):\n",
        "        bf_vec = torch.empty((1, 2 * self.num_ant))\n",
        "        bf_vec[0, ::2] = torch.tensor([1])\n",
        "        bf_vec[0, 1::2] = torch.tensor([0])\n",
        "        bf_vec = bf_vec.float().cuda()\n",
        "        return bf_vec\n",
        "\n",
        "    def init_best(self):\n",
        "        ph_book = np.linspace(-np.pi, np.pi, 2 ** self.num_bits, endpoint=False)\n",
        "        ph_vec = np.array([[ph_book[np.random.randint(0, len(ph_book))] for ii in range(self.num_ant)]])\n",
        "        bf_complex = np.exp(1j * ph_vec)\n",
        "        bf_vec = np.empty((1, 2 * self.num_ant))\n",
        "        for kk in range(self.num_ant):\n",
        "            bf_vec[0, 2 * kk] = np.real(bf_complex[0, kk])\n",
        "            bf_vec[0, 2 * kk + 1] = np.imag(bf_complex[0, kk])\n",
        "        return bf_vec\n",
        "\n",
        "\n",
        "class GainPred_Dense(nn.Module):\n",
        "\n",
        "    def __init__(self, M):\n",
        "        super(GainPred_Dense, self).__init__()\n",
        "        self.M = 36\n",
        "        self.input = 77\n",
        "        self.J = 8*self.input\n",
        "        self.K = 6*self.input\n",
        "        self.L = 4*self.input\n",
        "        self.N = 2*self.input\n",
        "\n",
        "        self.fc1 = nn.Linear( self.input, self.J) # 77 616\n",
        "        nn.init.xavier_normal_(self.fc1.weight)\n",
        "        self.bn1 = nn.BatchNorm1d(self.J)\n",
        "        self.dropout = nn.Dropout(0.001)\n",
        "        self.fc2 = nn.Linear(self.J, self.K) # 616 462\n",
        "        nn.init.xavier_normal_(self.fc2.weight)\n",
        "        self.bn2 = nn.BatchNorm1d(self.K)\n",
        "        self.fc3 = nn.Linear(self.K, self.K) # 462 462\n",
        "        self.bn3 = nn.BatchNorm1d(self.K)\n",
        "        self.fc4 = nn.Linear(self.K, self.L) # 462 308\n",
        "        self.bn4 = nn.BatchNorm1d(self.L)\n",
        "        self.fc5 = nn.Linear(self.L, self.L) # 308 308\n",
        "        self.bn5 = nn.BatchNorm1d(self.L)\n",
        "        self.fc6 = nn.Linear(self.L, self.N) # 308 144\n",
        "        self.bn6 = nn.BatchNorm1d(self.N)\n",
        "        self.fc7 = nn.Linear(self.N, 1) #144 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, ph):\n",
        "\n",
        "        device = ph.device\n",
        "        bf_vec = self.phase2bf(ph[:,5:]).cuda()\n",
        "        bf_vec_r, bf_vec_i = bf_vec[:, :self.M], bf_vec[:, self.M:]\n",
        "        bf_vec_cat = torch.cat((bf_vec_r, bf_vec_i), dim=1)\n",
        "        bf_vec = torch.cat((ph[:,:5].real,bf_vec_cat ),1)\n",
        "        x = torch.relu(self.bn1(self.fc1(bf_vec)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn3(self.fc3(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn4(self.fc4(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn5(self.fc5(x)))\n",
        "        x= self.dropout(x)\n",
        "        x = torch.relu(self.bn6(self.fc6(x)))\n",
        "        x= self.dropout(x)\n",
        "        y_min = -90\n",
        "        y_max = 35\n",
        "        out = torch.sigmoid(self.fc7(x))*(y_max-y_min)+y_min\n",
        "        return out\n",
        "\n",
        "    def phase2bf(self, ph_mat):\n",
        "\n",
        "        bf_mat = torch.exp(1j * ph_mat)\n",
        "        bf_mat_r = torch.real(bf_mat)\n",
        "        bf_mat_i = torch.imag(bf_mat)\n",
        "\n",
        "\n",
        "        bf_mat_ = torch.cat((bf_mat_r, bf_mat_i), dim=1)\n",
        "\n",
        "        return bf_mat_\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "#import argparse\n",
        "import scipy.io as scio\n",
        "#from train_ddpg import train\n",
        "import random\n",
        "\n",
        "\n",
        "for i in range(0,10):\n",
        "\n",
        "    print(\"main\")\n",
        "    #parser = argparse.ArgumentParser()\n",
        "\n",
        "    #parser.add_argument(dest='exp_id')\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    options = {\n",
        "        'gpu_idx': 0,\n",
        "        'num_ant': 36,\n",
        "        'num_mat': 36,\n",
        "        'num_bits': 2,\n",
        "        'pf_print': 100,\n",
        "\n",
        "        'path_target': 'drive/MyDrive/Colab_Notebooks/Learning_in_DT_environment/RL_training.mat' ,       #user2\n",
        "\n",
        "        'best_value': torch.tensor(-30).cuda(),\n",
        "\n",
        "        'best_state': None,\n",
        "        'noise' : -90,\n",
        "        'save_freq': 10000\n",
        "\n",
        "    }\n",
        "\n",
        "    train_opt = {\n",
        "        'state': 0,\n",
        "        'best_state': 0,\n",
        "        'num_iter': 10000,\n",
        "        'tau': 1e-1,\n",
        "        'overall_iter': 1,\n",
        "        'replay_memory': [],\n",
        "        'replay_memory_size': 100000,\n",
        "        'minibatch_size': 512,\n",
        "        'gamma': 0.95\n",
        "    }\n",
        "\n",
        "    if not os.path.exists('beams/'):\n",
        "        os.mkdir('beams/')\n",
        "\n",
        "    if not os.path.exists('pfs/'):\n",
        "        os.mkdir('pfs/')\n",
        "\n",
        "    ch_t3 = scio.loadmat(options['path_target'])['HR']\n",
        "    ch_BS = scio.loadmat(options['path_target'])['Gs']\n",
        "    ch_BI = scio.loadmat(options['path_target'])['G1']\n",
        "    ch_BI2 = scio.loadmat(options['path_target'])['G2']\n",
        "\n",
        "\n",
        "    options['ch_bs_irs'] = ch_BS\n",
        "    options['ch_bs_irs_i'] = ch_BI\n",
        "    options['ch_bs_irs_i2'] = ch_BI2\n",
        "\n",
        "    options['ch_t3'] = ch_t3\n",
        "\n",
        "    # Quantization settings\n",
        "    options['num_ph'] = 2 ** options['num_bits']\n",
        "    options['multi_step'] = torch.from_numpy(\n",
        "        np.linspace(int(-(options['num_ph'] - 2) / 2),\n",
        "                    int(options['num_ph'] / 2),\n",
        "                    num=options['num_ph'],\n",
        "                    endpoint=True)).type(dtype=torch.float32).reshape(1, -1)\n",
        "\n",
        "    options['pi'] = torch.tensor(np.pi)\n",
        "    options['ph_table'] = (2 * options['pi']) / options['num_ph'] * options['multi_step']\n",
        "    options['ph_table_rep'] = options['ph_table'].repeat(options['num_ant'], 1)\n",
        "    options['ph_table_rep-1'] = options['ph_table'].repeat(2*options['num_ant'], 1)\n",
        "    options['best_bf_pred_comb']  = float('-inf');\n",
        "    options['best_bf_comb']  = float('-inf');\n",
        "\n",
        "    options['previous_gain_pred']  = float('-inf');\n",
        "    options['previous_gain']  = float('-inf');\n",
        "    options['previous_diff_pred']  = float('inf');\n",
        "    options['previous_diff']  = float('inf');\n",
        "    print(\"options\", len(options))\n",
        "\n",
        "    train(options, train_opt, int(i), int(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU4r56fNPofe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import scipy.io as scio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "def plot_curve(tr):\n",
        "  num_realization = 17\n",
        "  num_iteration = 10000\n",
        "\n",
        "  data_all_agent = np.zeros((num_realization, num_iteration))\n",
        "  data_all_noise = np.zeros((num_realization, num_iteration))\n",
        "\n",
        "  data_all_agent_pred = np.zeros((num_realization, num_iteration))\n",
        "  data_all_noise_pred = np.zeros((num_realization, num_iteration))\n",
        "\n",
        "  data_all_agent_c = np.zeros((num_realization, num_iteration))\n",
        "  data_all_noise_c = np.zeros((num_realization, num_iteration))\n",
        "  mse = np.zeros((num_realization, 1))\n",
        "  mse_noise = np.zeros((num_realization, 1))\n",
        "  data_all_agent_d = np.zeros((num_realization, num_iteration))\n",
        "  data_all_noise_d = np.zeros((num_realization, num_iteration))\n",
        "  data_all = np.zeros((num_realization, num_iteration))\n",
        "  data_all_pred = np.zeros((num_realization, num_iteration))\n",
        "  for ii in range(num_realization):\n",
        "    data = scio.loadmat('./drive/MyDrive/Colab_Notebooks/rl_results/rl_result_'+ tr + '_'+ str(ii) + '.mat')  # 'agent', 'noise'\n",
        "\n",
        "    data_all_agent[ii, :] = data['agent']\n",
        "    data_all_noise[ii, :] = data['noise']\n",
        "\n",
        "    data_all_agent_pred[ii, :] = data['agent_pred']\n",
        "    data_all_noise_pred[ii, :] = data['noise_pred']\n",
        "\n",
        "\n",
        "\n",
        "    mse[ii] = np.mean((data_all_agent - data_all_agent_pred) ** 2)\n",
        "    mse_noise[ii] = np.mean((data_all_noise - data_all_noise_pred) ** 2)\n",
        "    mae = np.mean(np.abs(data_all_agent_pred - data_all_agent))\n",
        "\n",
        "  data_all = np.maximum(data_all_agent, data_all_noise)\n",
        "  print(data_all.shape)\n",
        "  max_all = np.max(data_all)\n",
        "  data_all_pred = np.maximum(data_all_agent_pred,data_all_noise_pred)\n",
        "  max_all_pred =  np.max(data_all_pred)\n",
        "\n",
        "  mse_main = np.mean(mse)\n",
        "  mae_main = np.mean(mae)\n",
        "  print(\"MSE: \", mse_main)\n",
        "  print(\"MAE: \", mae_main)\n",
        "  print(\"Max value DT:\", np.max(data_all_pred))\n",
        "  print(\"Max value actual:\", data_all[np.unravel_index(np.argmax(data_all_pred), data_all_pred.shape)])\n",
        "\n",
        "  flat_indices = np.argsort(data_all, axis=None)[-num_realization*5000:]  # Indices of top 200 max values in flattened array1\n",
        "  indices_2d = np.unravel_index(flat_indices, data_all.shape)  # Convert to 2D indices\n",
        "\n",
        "  array1_top200_values = data_all[indices_2d]\n",
        "  array2_corresponding_values = data_all_pred[indices_2d]\n",
        "\n",
        "  mse = mean_squared_error(array1_top200_values, array2_corresponding_values)\n",
        "  mae = mean_absolute_error(array1_top200_values, array2_corresponding_values)\n",
        "\n",
        "  # Print results\n",
        "  print(\"MSE:\", mse)\n",
        "  print(\"MAE:\", mae)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  fig1 = plt.figure(1)\n",
        "  ax1 = fig1.gca()\n",
        "\n",
        "  fig2 = plt.figure(2)\n",
        "  ax2 = fig2.gca()\n",
        "  data_all_mean = np.nanmean(data_all, axis=0)\n",
        "  data_all_mean_d = np.nanmean(data_all_pred, axis=0)\n",
        "\n",
        "  data_all_std = np.std(data_all, axis=0)\n",
        "  data_all_std_d = np.std(data_all_pred, axis=0)\n",
        "\n",
        "  from scipy.signal import savgol_filter\n",
        "  data_all_meanw = savgol_filter(data_all_mean, 20, 2)\n",
        "  data_all_stdw = savgol_filter(data_all_std_d, 50, 2)\n",
        "  max_1 = 29.51\n",
        "  ax1.plot(  range(num_iteration), np.ones([num_iteration,1])*max_1, '--', color='black', label = 'Optimal gain threshold')\n",
        "  ax1.plot(range(num_iteration), data_all_meanw, '-', color='red', alpha=0.8, label='Actual environment based gain')\n",
        "  #ax1.fill_between(range(num_iteration), data_all_mean - data_all_std, data_all_mean + data_all_std, color='red', alpha=0.2)\n",
        "  ax1.set_title(\"Digital Twin training size =\"+str(tr) +\" million\")\n",
        "\n",
        "  ax1.set_xlabel('Number of iteration')\n",
        "  ax1.set_ylabel('SIR (dB)')\n",
        "  ax1.set_ylim((-10.5, 30.67))  # Adjusted y-axis limit\n",
        "  ax1.set_xlim((0, num_iteration))  # Adjusted x-axis limit\n",
        "  ax1.grid(True)\n",
        "\n",
        "  ax1.legend(loc='lower right')\n",
        "  from scipy.signal import savgol_filter\n",
        "  data_all_meanw = savgol_filter(data_all_mean_d, 20, 2)\n",
        "  max_1 = 29.51\n",
        "  ax2.plot(  range(num_iteration), np.ones([num_iteration,1])*max_1, '--', color='black', label = 'Optimal gain')\n",
        "  ax2.plot(range(num_iteration), data_all_meanw, '-', color='green', alpha=0.8, label='Surrogate environment based gain')\n",
        "  #ax2.fill_between(range(num_iteration), data_all_mean_d - data_all_std_d, data_all_mean_d + data_all_std_d, color='red', alpha=0.2)\n",
        "  ax2.set_xlabel('Number of iteration')\n",
        "  ax2.set_ylabel('SIR (dB)')\n",
        "  ax2.set_ylim((-10.5, 30.67))  # Adjusted y-axis limit\n",
        "  ax2.set_xlim((0, num_iteration))  # Adjusted x-axis limit\n",
        "  ax2.grid(True)\n",
        "  ax2.legend(loc='lower right')\n",
        "  ax2.set_title(\"Digital Twin training size =\" +str(tr)+\" million\")\n",
        "  plt.show()\n",
        "\n",
        "import scipy.io as scio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_curve_active():\n",
        "  num_realization = 25\n",
        "  num_iteration = 10000\n",
        "\n",
        "  data_all_agent = np.zeros((num_realization, num_iteration))\n",
        "  data_all_noise = np.zeros((num_realization, num_iteration))\n",
        "  data_all = np.zeros((num_realization, num_iteration))\n",
        "\n",
        "  for ii in range(num_realization):\n",
        "    data = scio.loadmat('./drive/MyDrive/Colab_Notebooks/rl_results/rl_result_actual_'+ str(ii) + '.mat')  # 'agent', 'noise'\n",
        "\n",
        "    data_all_agent[ii, :] = data['agent']\n",
        "    data_all_noise[ii, :] = data['noise']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    data_all[ii,:] = np.maximum(data_all_agent, data_all_noise)\n",
        "\n",
        "  fig1 = plt.figure(1)\n",
        "  ax1 = fig1.gca()\n",
        "\n",
        "  data_all_mean = np.nanmean(data_all, axis=0)\n",
        "\n",
        "  data_all_std = np.std(data_all, axis=0)\n",
        "\n",
        "  from scipy.signal import savgol_filter\n",
        "  data_all_meanw = savgol_filter(data_all_mean, 20, 2)\n",
        "  data_all_stdw = savgol_filter(data_all_std, 50, 2)\n",
        "  max_1 = 29.51\n",
        "  ax1.plot(  range(num_iteration), np.ones([num_iteration,1])*max_1, '--', color='black', label = 'Optimal gain threshold')\n",
        "  ax1.plot(range(num_iteration), data_all_meanw, '-', color='red', alpha=0.8, label='Actual environment based gain')\n",
        "  #ax1.fill_between(range(num_iteration), data_all_mean - data_all_std, data_all_mean + data_all_std, color='red', alpha=0.2)\n",
        "  ax1.set_title(\"Actual Environment\")\n",
        "\n",
        "  ax1.set_xlabel('Number of iteration')\n",
        "  ax1.set_ylabel('SIR (dB)')\n",
        "  ax1.set_ylim((-10.5, 30.67))  # Adjusted y-axis limit\n",
        "  ax1.set_xlim((0, num_iteration))  # Adjusted x-axis limit\n",
        "  ax1.grid(True)\n",
        "\n",
        "  ax1.legend(loc='lower right')\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6SXzzxKmLzPx",
        "outputId": "30cc7c39-985c-4997-b302-d379c011c777"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17, 10000)\n",
            "MSE:  24.843172987589803\n",
            "MAE:  5.520916524246496\n",
            "Max value DT: 18.865867614746094\n",
            "Max value actual: 3.368669436611583\n",
            "MSE: 22.823711360605646\n",
            "MAE: 3.8086147057146222\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_curve('0.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kL3UK76hMFIr",
        "outputId": "20814d46-14ab-40aa-e6d5-560b5b679202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17, 10000)\n",
            "MSE:  17.487733527629953\n",
            "MAE:  4.294442635698488\n",
            "Max value DT: 19.952499389648438\n",
            "Max value actual: 15.830403466349367\n",
            "MSE: 14.080530975507378\n",
            "MAE: 2.9298108083009238\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAHHCAYAAAB5gsZZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACQXElEQVR4nO3dd1hTVx8H8G/YIBsVXCDuvUdxz6K27letta5a99a6ai2uunfraG2rtXW31dq6invvujfOKi4ciCAEct4/rgm5IQkJARLg+3keHnLvPffek5yQ/DhTIYQQICIiIqI0s7N2BoiIiIiyOgZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZUlKVNnDgRCoUiTeeuXLkSCoUCd+7cSbf83LlzBwqFAitXrky3a2aFe6fFvn37oFAosG/fPrPPtfXnasn7ksyjUCgwceJEzba+v+sGDRqgQYMGmm1bf/9Q1sSAimyG+oNQ/ePi4oL8+fMjNDQUixYtwuvXrzM8D0uWLMnwD9nChQvLnqehH1v4sM+M14Os78qVK2jWrBnc3d3h6+uLrl274unTpyada+j93K9fvwzONZFtUXAtP7IVK1euRM+ePTF58mQEBwdDqVTi0aNH2LdvH8LDwxEYGIgtW7agQoUKmnMSExORmJgIFxcXs++XlJQEpVIJZ2dnTW1CuXLlkDt37jTVmgDSf77BwcFYsWIFevTooTfN5s2bERMTo9netm0b1q5di/nz5yN37tya/bVq1UKRIkVMvrcQAvHx8XB0dIS9vX2a8q/L0tfDGJVKhYSEBDg5OcHOzrz/7TLiuaYnS96Xme2///5D5cqV4eXlhSFDhiAmJgZz5sxBYGAgTpw4AScnJ6PnFy5cGD4+Phg5cqRsf4kSJVCjRo2MzDoA4O3bt3BwcICDgwOA5M+R27dvo3DhwgCgqZ1Sv49t/f1DWZODtTNApKt58+aoVq2aZnvcuHHYs2cPPvzwQ7Rq1QpXrlyBq6srAMg+SM1lb29vlQ/TNm3ayLYfPXqEtWvXok2bNpovgLRQ1+pZy5s3b5ArVy6T09vZ2aU5v9Z+rqmx5H2Z2aZNm4Y3b97g9OnTCAwMBADUqFEDTZs2xcqVK9GnT59Ur1GgQAF88sknGZ1VvdLyPrD19w9lTWzyoyyhUaNGmDBhAu7evYtff/1Vs19fX5W4uDgMGTIEuXPnhoeHB1q1aoUHDx6k2teicOHCuHTpEvbv369ptlD/Z/v8+XN8/vnnKF++PNzd3eHp6YnmzZvj3Llz6f5cR4wYAT8/P2hXHg8ePBgKhQKLFi3S7Hv8+DEUCgWWLl0KQH+/kB49esDd3R0PHjxAmzZt4O7ujjx58uDzzz9HUlKS0XwYez3Ur93+/fsxYMAA5M2bFwULFgQA3L17FwMGDEDJkiXh6uoKPz8/dOjQIUVfNX19qBo0aIBy5crh8uXLaNiwIdzc3FCgQAHMmjVLdq6lzzUqKgpdu3aFp6cnvL290b17d5w7d86kplalUolJkyahePHicHFxgZ+fH+rUqYPw8HBNGt33ZY8ePQw27Wq/J+Pj4xEWFoZixYrB2dkZhQoVwujRoxEfH280T5b4/fff8eGHH2qCKQBo0qQJSpQogQ0bNph8nYSEBLx588ase6vfR4cOHcKQIUOQJ08eeHt7o2/fvkhISMDLly/RrVs3+Pj4wMfHB6NHj4Zuo4rua2gKQ32o9uzZg7p16yJXrlzw9vZG69atceXKFVkaddnevHkTPXr0gLe3N7y8vNCzZ0/ExsaalQ/KXrLGv1BEALp27YovvvgC//zzD3r37m0wXY8ePbBhwwZ07doV7733Hvbv348PPvgg1esvWLAAgwcPhru7O8aPHw8A8Pf3BwDcunULmzdvRocOHRAcHIzHjx/ju+++Q/369XH58mXkz58/fZ4kgLp162L+/Pm4dOkSypUrBwA4ePAg7OzscPDgQQwZMkSzDwDq1atn9HpJSUkIDQ1FzZo1MWfOHOzatQtz585F0aJF0b9/f4PnGXs91AYMGIA8efLgq6++0nyZnjx5EkeOHMFHH32EggUL4s6dO1i6dCkaNGiAy5cvw83NzWh+X7x4gWbNmqFdu3bo2LEjfvvtN4wZMwbly5dH8+bNLX6uKpUKLVu2xIkTJ9C/f3+UKlUKf/75J7p372702moTJ07E9OnT8dlnn6FGjRqIjo7GqVOncObMGTRt2lTvOX379kWTJk1k+3bs2IHVq1cjb968mny1atUKhw4dQp8+fVC6dGlcuHAB8+fPx/Xr17F582aj+YqNjTXpC93e3h4+Pj4AgAcPHuDJkyeyGmG1GjVqYNu2baleD5ACETc3NyQlJSEoKAjDhw/H0KFDTToXkP5hCAgIwKRJk3Ds2DF8//338Pb2xpEjRxAYGIhp06Zh27ZtmD17NsqVK4du3bqZfG1T7dq1C82bN0eRIkUwceJExMXF4ZtvvkHt2rVx5syZFLXHHTt2RHBwMKZPn44zZ87ghx9+QN68eTFz5sx0zxtlEYLIRqxYsUIAECdPnjSYxsvLS1SuXFmzHRYWJrTfxqdPnxYAxLBhw2Tn9ejRQwAQYWFhKe53+/Ztzb6yZcuK+vXrp7jv27dvRVJSkmzf7du3hbOzs5g8ebJsHwCxYsWKVJ5tstmzZ8vy8eTJEwFALFmyRAghxMuXL4WdnZ3o0KGD8Pf315w3ZMgQ4evrK1QqlcF7d+/eXQCQ5VEIISpXriyqVq2aat4MvR7q165OnToiMTFRdiw2NjZF+qNHjwoAYtWqVZp9e/fuFQDE3r17Nfvq16+fIl18fLwICAgQ7du31+yz5Ln+/vvvAoBYsGCBZl9SUpJo1KiRSWVXsWJF8cEHHxhNo/u+1HXjxg3h5eUlmjZtqnn9fvnlF2FnZycOHjwoS7ts2TIBQBw+fNike6b2ExQUpDnn5MmTKV5vtVGjRgkA4u3bt0bv27JlSzFz5kyxefNm8eOPP4q6desKAGL06NFGzxMi+X0UGhqqeR8LIURISIhQKBSiX79+mn2JiYmiYMGCKd6Ppvxd169fX3aevvdPpUqVRN68eUVUVJRm37lz54SdnZ3o1q2bZp/6df70009l+Wjbtq3w8/NL9TlT9sUmP8pS3N3djY7227FjBwCp5kTb4MGDLbqvs7OzpuN0UlISoqKi4O7ujpIlS+LMmTMWXVtXnjx5UKpUKRw4cAAAcPjwYdjb22PUqFF4/Pgxbty4AUCqoapTp45Jw/N1R1zVrVsXt27dsjivvXv3TtEPTd2/DZCax6KiolCsWDF4e3ub9Fq5u7vL+uM4OTmhRo0aJuc3tee6Y8cOODo6ymo57ezsMHDgQJOu7+3tjUuXLmnKwVxv3rxB27Zt4ePjg7Vr12pev40bN6J06dIoVaoUnj17pvlp1KgRAGDv3r1Gr9utWzeEh4en+rN69WrNOXFxcQCk97cudR8jdRpDtmzZgtGjR6N169b49NNPsX//foSGhmLevHn477//THpNevXqJXsf16xZE0II9OrVS7PP3t4e1apVS5f3ra7IyEicPXsWPXr0gK+vr2Z/hQoV0LRpU701dfreZ1FRUYiOjk73/FHWwCY/ylJiYmI0TST63L17F3Z2dggODpbtL1asmEX3ValUWLhwIZYsWYLbt2/L+uT4+flZdG196tatq/kQP3jwIKpVq4Zq1arB19cXBw8ehL+/P86dO4ePP/441Wu5uLggT548sn0+Pj548eKFxfnUfZ0B6Qt4+vTpWLFiBR48eCDr8/Lq1atUr1mwYMEUQaKPjw/Onz+f6rmmPNe7d+8iX758KZoeTX2PTJ48Ga1bt0aJEiVQrlw5NGvWDF27dpWNPjWmd+/eiIiIwJEjR2TvnRs3buDKlSsp8q/25MkTo9ctUqSIWaNCgeTgV18frbdv38rSmEqhUGD48OHYuXMn9u3bZ1Jnde3+WwDg5eUFAChUqFCK/enxvtV19+5dAEDJkiVTHCtdujR27tyZYtCFbp7VzagvXryAp6dnuueRbB8DKsoy/vvvP7x69cri4Cgtpk2bhgkTJuDTTz/FlClT4OvrCzs7OwwbNgwqlSrd71enTh0sX74ct27dwsGDB1G3bl0oFArUqVMHBw8eRP78+aFSqVC3bt1Ur5WRIxn1fdkOHjwYK1aswLBhwxASEgIvLy8oFAp89NFHJr1WhvIrTJjhJTNGbdarVw8RERH4888/8c8//+CHH37A/PnzsWzZMnz22WdGz124cCHWrl2LX3/9FZUqVZIdU6lUKF++PObNm6f3XN3gQldMTIxsOg5D7O3tNUFbvnz5AEg1NLoiIyPh6+urt/YqNeq8Pn/+3KT0hspN335T3geZwZL3KWVPDKgoy/jll18AAKGhoQbTBAUFQaVS4fbt2yhevLhm/82bN026h6Hms99++w0NGzbEjz/+KNv/8uVL2dxR6UUdKIWHh+PkyZMYO3YsAOnLfOnSpcifPz9y5cqFqlWrpvu9taVltu/ffvsN3bt3x9y5czX73r59i5cvX6ZjztIuKCgIe/fuRWxsrKyWytT3CAD4+vqiZ8+e6NmzJ2JiYlCvXj1MnDjRaEB18OBBfP755xg2bBi6dOmS4njRokVx7tw5NG7cOE2v+5w5czBp0qRU0wUFBWlGXBYoUAB58uTBqVOnUqQ7ceJEiqDPVOpmOUO1bbYmKCgIAHDt2rUUx65evYrcuXObNSUI5UzsQ0VZwp49ezBlyhQEBwfr/TJSUwdbS5Yske3/5ptvTLpPrly59H7x29vbp/jPc+PGjXjw4IFJ1zVXcHAwChQogPnz50OpVKJ27doApEArIiICv/32G957770Mn+vI0OthjL7X6ptvvkl1mobMEhoaCqVSieXLl2v2qVQqLF682KTzo6KiZNvu7u4oVqyY0akNIiMj0bFjR9SpUwezZ8/Wm6Zjx4548OCBLF9qcXFxqU5JkJY+VADQvn17/P3337h//75m3+7du3H9+nV06NBBs0+pVOLq1auy2qznz5+nKFelUokZM2bAyckJDRs2NJpnW5EvXz5UqlQJP//8s+z9fvHiRfzzzz9o0aKF9TJHWQZrqMjmbN++HVevXkViYiIeP36MPXv2IDw8HEFBQdiyZYvRCfmqVq2K9u3bY8GCBYiKitJMm3D9+nUAqde4VK1aFUuXLsXUqVNRrFgx5M2bF40aNcKHH36IyZMno2fPnqhVqxYuXLiA1atXm91nxRx169bFunXrUL58eU3/jCpVqiBXrly4fv26Sf2nLGXo9TDmww8/xC+//AIvLy+UKVMGR48exa5duzKkr1latGnTBjVq1MDIkSNx8+ZNlCpVClu2bNE0T6X2HilTpgwaNGiAqlWrwtfXF6dOncJvv/2GQYMGGTxnyJAhePr0KUaPHo1169bJjlWoUAEVKlRA165dsWHDBvTr1w979+5F7dq1kZSUhKtXr2LDhg3YuXOn3ukN1NLShwoAvvjiC2zcuBENGzbE0KFDERMTg9mzZ6N8+fLo2bOnJt2DBw9QunRpdO/eXTN/05YtWzB16lT873//Q3BwMJ4/f441a9bg4sWLmDZtGgICAszOj7XMnj0bzZs3R0hICHr16qWZNsHLy8vsea4oZ2JARTbnq6++AiCN7vL19UX58uWxYMEC9OzZEx4eHqmev2rVKgQEBGDt2rXYtGkTmjRpgvXr16NkyZKpzo781Vdf4e7du5g1axZev36N+vXro1GjRvjiiy/w5s0brFmzBuvXr0eVKlWwdetWTVNcRlAHVHXq1NHsc3BwQEhICHbt2mVS/ylLGXo9jFm4cCHs7e2xevVqvH37FrVr18auXbuMNtVmJnt7e2zduhVDhw7Fzz//DDs7O7Rt2xZhYWGoXbt2qu+RIUOGYMuWLfjnn38QHx+PoKAgTJ06FaNGjTJ4ztOnT5GUlIQRI0akOBYWFoYKFSrAzs4Omzdvxvz587Fq1Sps2rQJbm5uKFKkCIYOHYoSJUpY/Nz1KVSoEPbv348RI0Zg7NixcHJywgcffIC5c+em2n+qfPnyKFOmDH799Vc8ffoUTk5OqFSpEjZs2CCr3coKmjRpgh07diAsLAxfffUVHB0dUb9+fcycOVPv4AsiXVzLj3KEs2fPonLlyvj111+NNhlSzrV582a0bdsWhw4d0jSxEhGZin2oKNvRN2/OggULYGdnl+qs4pQz6L5HkpKS8M0338DT0xNVqlSxUq6IKCtjkx9lO7NmzcLp06fRsGFDODg4YPv27di+fTv69OmT6tBzyhkGDx6MuLg4hISEID4+Hn/88QeOHDmCadOmmT3vEhERwCY/yobCw8MxadIkXL58GTExMQgMDETXrl0xfvz4DB8VR1nDmjVrMHfuXNy8eRNv375FsWLF0L9/f6Mdy4mIjGFARURERGQh9qEiIiIishADKiIiIiILsUOJDpVKhYcPH8LDwyNNyz8QERFR5hNC4PXr18ifPz/s7DK/vogBlY6HDx9yJBgREVEWdf/+fRQsWDDT78uASod6Ju7bt2/D19fXyrnJ2ZRKJf755x+8//77cHR0tHZ2cjSWhe1gWdgOloVtef78OYKDg01aUSMjZJmAaunSpVi6dKlmlfSyZcviq6++QvPmzQFIq9mPHDkS69atQ3x8PEJDQ7FkyRL4+/ubdR91M5+Hhwc8PT3T9TmQeZRKJdzc3ODp6ckPKytjWdgOloXtYFnYFqVSCSD19TgzSpbplF6wYEHMmDEDp0+fxqlTp9CoUSO0bt0aly5dAgAMHz4cf/31FzZu3Ij9+/fj4cOHaNeunZVzTURERDlBlqmhatmypWz766+/xtKlS3Hs2DEULFgQP/74I9asWaNZuHXFihUoXbo0jh07hvfee88aWSYiIqIcIssEVNqSkpKwceNGvHnzBiEhITh9+jSUSiWaNGmiSVOqVCkEBgbi6NGjRgOq+Ph4xMfHa7ajo6MBSFWH6upDsg71689ysD6Whe1gWdgOloVtsXY5ZKmA6sKFCwgJCcHbt2/h7u6OTZs2oUyZMjh79iycnJzg7e0tS+/v749Hjx4Zveb06dMxadKkFPv37t0LNze39Mw+pVF4eLi1s0DvsCxsB8vCdrAsbENsbKxV75+lAqqSJUvi7NmzePXqFX777Td0794d+/fvt+ia48aNw4gRIzTb0dHRKFSoEBo2bAg/Pz9Ls0wWUCqVCA8PR9OmTdnh08pYFraDZWE7WBa2JSoqyqr3z1IBlZOTE4oVKwYAqFq1Kk6ePImFCxeiU6dOSEhIwMuXL2W1VI8fP0ZAQIDRazo7O8PZ2TnFfkdHR/6B2AiWhe1gWdgOloXtYFnYBmuXQZYKqHSpVCrEx8ejatWqcHR0xO7du9G+fXsAwLVr13Dv3j2EhISk6dozZszQ2+T36aefIigoCABw7NgxbN++3eA1unbtqgkAT58+jS1bthhM26lTJ5QpUwaA1LT522+/GUzbrl07VKxYEQBw9epVrF271mDali1bolq1agCAiIgIrFq1ymDa0NBQ1KpVC4A0MdoPP/xgMG2jRo1Qv359AMCjR4+wdOlSg2nr1KmDpk2bApD+g1i0aJHBtDVr1kSLFi0AAK9fv8batWtx8uRJ2Nvbp0hbpUoVtG7dGoA0bcb06dMNXrdcuXLo0KEDAKkP3uTJkw2mLVmyJD7++GPN9uTJk5GUlKQ3bZEiRdC9e3fN9owZMxAXF6c3bcGCBdG7d2/N9ty5czV99nT5+/tjwIABmu1FixYZ/O/L19cXQ4cO1WwvW7YMkZGRetO6u7tj1KhRmu0ff/wR9+7d05vW2dkZX3zxhWZ77969BsvCzs4OYWFhmu1169bhypUreq8LABMmTICDg/Tx8/vvv+P8+fMG044ZM0bzt7hlyxacPn3aYNoRI0bAy8sLALBjxw4cPXrUYNrBgwcjd+7cAIDdu3fjwIEDBtP27dsX+fPnBwAcPHgQu3btMpg2Mz4j7ty5g0mTJuktCyBnfUbMmTPHYNrM+oyYOnWqwbQ56TNi1apViIiI0Js2sz4jrEpkEWPHjhX79+8Xt2/fFufPnxdjx44VCoVC/PPPP0IIIfr16ycCAwPFnj17xKlTp0RISIgICQkx+z6vXr0SAAz+HDx4UJN24cKFRtPu2LFDk3b58uVG0/7++++atGvWrDGa9pdfftGk/fPPP42mXbZsmSZteHi40bRz587VpD1y5IjRtFOmTNGkPXfunNG0Y8eO1aS9ceOG0bSDBw/WpL17967RtL169TK53D766CNNWqVSaTRty5YtZe8JJycng2kbN24sS+vj42Mw7XvvvSdLW6BAAYNpy5cvL0tbokQJg2mLFi0qS1u5cmWDaQMCAmRpa9eubTCth4eHJl1CQoKoVKmSwbT29vay67Zt29boa/z27VtN2i5duhhNGxUVpUnbp08fo2nv37+vSTt8+HCjaa9evapJO378eKNpT58+rUk7bdo0o2kz+jMiISFBjBgxwmjanPIZ8fDhQ6NpM/ozIiEhQWzevJmfEe+8//77BtNmxmfEs2fPBADx6tUrYQ1ZpobqyZMn6NatGyIjI+Hl5YUKFSpg586dmv9q5s+fDzs7O7Rv3142sWda9erVCy4uLin258uXT/O4QoUKGDhwoMFraC9hU6ZMGaNpixQponlcvHhxo2lLlCiheVy4cGGjacuWLat5XLBgQaNpK1WqpHkcEBBgNK36P1oA8PPzM5pWe5Sll5eX0bR169bVPHZzc0OLFi0QFBSkd10m7dpHR0dHo9etWrWq5rFCoTCatnz58rLt/v37IzExUW/akiVLyrY/++wzgx0jCxcuLNvu0aMHXr58qTetujZErUuXLnjy5InetOpaFrWOHTtqahF06U5W265dO1m5a9N9/9eoUQMhISF6y0J3X/PmzVM8B0PpmzZtmmJAiTbtJvkGDRoYrdbPlSuX5nHt2rWRkJBgMK32PWvWrGn0PZEnTx7N46pVqxpNmxmfEfnz50f//v0NrleWkz4jjKXNrM+Ivn37QqVS6U2bkz4jWrVqheLFi+tNm1mfEdakEEIIa2fClkRHR8PLywvPnj1jp3QrUyqV2LZtG1q0aGH1tvGcjmVhO1gWtoNlYVuioqKQO3duvHr1yiornWSZmdKJiIiIbBUDKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKiIiIiILZZmAavr06ahevTo8PDyQN29etGnTBteuXZOladCgARQKheynX79+VsoxERER5RRZJqDav38/Bg4ciGPHjiE8PBxKpRLvv/8+3rx5I0vXu3dvREZGan5mzZplpRwTERFRTuFg7QyYaseOHbLtlStXIm/evDh9+jTq1aun2e/m5oaAgIDMzh4RERHlYFkmoNL16tUrAICvr69s/+rVq/Hrr78iICAALVu2xIQJE+Dm5mbwOvHx8YiPj9dsR0dHAwCUSiWUSmUG5JxMpX79WQ7Wx7KwHSwL28GysC3WLgeFEEJYNQdpoFKp0KpVK7x8+RKHDh3S7P/+++8RFBSE/Pnz4/z58xgzZgxq1KiBP/74w+C1Jk6ciEmTJqXYv2bNGqOBGBEREdmO2NhYfPzxx3j16hU8PT0z/f5ZMqDq378/tm/fjkOHDqFgwYIG0+3ZsweNGzfGzZs3UbRoUb1p9NVQFSpUCJGRkfDz80v3vJPplEolwsPD0bRpUzg6Olo7Ozkay8J2sCxsB8vCtkRFRSFfvnxWC6iyXJPfoEGD8Pfff+PAgQNGgykAqFmzJgAYDaicnZ3h7OycYr+joyP/QGwEy8J2sCxsB8vCdrAsbIO1yyDLBFRCCAwePBibNm3Cvn37EBwcnOo5Z8+eBQDky5cvg3NHREREOVmWCagGDhyINWvW4M8//4SHhwcePXoEAPDy8oKrqysiIiKwZs0atGjRAn5+fjh//jyGDx+OevXqoUKFClbOPREREWVnWSagWrp0KQBp8k5tK1asQI8ePeDk5IRdu3ZhwYIFePPmDQoVKoT27dvjyy+/tEJuiYiIKCfJMgFVan3nCxUqhP3792dSboiIiIiSZZmZ0omIiIhsFQMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgsxoCIiIiKyEAMqIiIiIgtlmYBq+vTpqF69Ojw8PJA3b160adMG165dk6V5+/YtBg4cCD8/P7i7u6N9+/Z4/PixlXJMREREOUWWCaj279+PgQMH4tixYwgPD4dSqcT777+PN2/eaNIMHz4cf/31FzZu3Ij9+/fj4cOHaNeunRVzTURERDmBg7UzYKodO3bItleuXIm8efPi9OnTqFevHl69eoUff/wRa9asQaNGjQAAK1asQOnSpXHs2DG899571sg2ERER5QBZJqDS9erVKwCAr68vAOD06dNQKpVo0qSJJk2pUqUQGBiIo0ePGgyo4uPjER8fr9mOjo4GACiVSiiVyozKPplA/fqzHKyPZWE7WBa2g2VhW6xdDlkyoFKpVBg2bBhq166NcuXKAQAePXoEJycneHt7y9L6+/vj0aNHBq81ffp0TJo0KcX+vXv3ws3NLV3zTWkTHh5u7SzQOywL28GysB0sC9sQGxtr1ftnyYBq4MCBuHjxIg4dOmTxtcaNG4cRI0ZotqOjo1GoUCE0bNgQfn5+Fl+f0k6pVCI8PBxNmzaFo6OjtbOTo7EsbAfLwnawLGxLVFSUVe+f5QKqQYMG4e+//8aBAwdQsGBBzf6AgAAkJCTg5cuXslqqx48fIyAgwOD1nJ2d4ezsnGK/o6Mj/0BsBMvCdrAsbAfLwnawLGyDtcsgy4zyE0Jg0KBB2LRpE/bs2YPg4GDZ8apVq8LR0RG7d+/W7Lt27Rru3buHkJCQzM4uERER5SBZpoZq4MCBWLNmDf788094eHho+kV5eXnB1dUVXl5e6NWrF0aMGAFfX194enpi8ODBCAkJ4Qg/IiIiylBZJqBaunQpAKBBgway/StWrECPHj0AAPPnz4ednR3at2+P+Ph4hIaGYsmSJZmcUyIiIsppskxAJYRINY2LiwsWL16MxYsXZ0KOiIiIiCRZpg8VERERka1iQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkoSwVUB04cAAtW7ZE/vz5oVAosHnzZtnxHj16QKFQyH6aNWtmncwSERFRjpGlAqo3b96gYsWKWLx4scE0zZo1Q2RkpOZn7dq1mZhDIiIiyokcrJ0BczRv3hzNmzc3msbZ2RkBAQGZlCMiIiKiLBZQmWLfvn3ImzcvfHx80KhRI0ydOhV+fn4G08fHxyM+Pl6zHR0dDQBQKpVQKpUZnl8yTP36sxysj2VhO1gWtoNlYVusXQ4KIYSwag7SSKFQYNOmTWjTpo1m37p16+Dm5obg4GBERETgiy++gLu7O44ePQp7e3u915k4cSImTZqUYv+aNWvg5uaWUdknIiKidBQbG4uPP/4Yr169gqenZ6bfP1sFVLpu3bqFokWLYteuXWjcuLHeNPpqqAoVKoTIyEijNVuU8ZRKJcLDw9G0aVM4OjpaOzs5GsvCdrAsbAfLwrZERUUhX758Vguosl2Tn7YiRYogd+7cuHnzpsGAytnZGc7Ozin2Ozo68g/ERrAsbAfLwnawLGwHy8I2WLsMstQoP3P9999/moiViIiIKKNkqRqqmJgY3Lx5U7N9+/ZtnD17Fr6+vvD19cWkSZPQvn17BAQEICIiAqNHj0axYsUQGhpqxVwTERFRdpelAqpTp06hYcOGmu0RI0YAALp3746lS5fi/Pnz+Pnnn/Hy5Uvkz58f77//PqZMmaK3SY+IiIgovWSpgKpBgwYw1od+586dmZgbIiIiIkm27kNFRERElBkYUBERERFZiAEVERERkYUYUBERERFZiAEVERERkYUYUBERERFZiAEVERERkYUYUBERERFZiAEVERERkYUYUBERERFZiAEVERERkYXMXsvvypUrWLduHQ4ePIi7d+8iNjYWefLkQeXKlREaGor27dtzMWIiIiLKUUyuoTpz5gyaNGmCypUr49ChQ6hZsyaGDRuGKVOm4JNPPoEQAuPHj0f+/Pkxc+ZMxMfHZ2S+iYiIiGyGyTVU7du3x6hRo/Dbb7/B29vbYLqjR49i4cKFmDt3Lr744ov0yCMRERGRTTM5oLp+/TocHR1TTRcSEoKQkBAolUqLMkZERESUVZjc5GdKMGVJeiIiIqKsyuxRfq9fv8bp06cRExMDQOpb1a1bN3To0AGrV69O9wwSERER2TqzRvkdOHAAH374IWJiYuDj44O1a9fif//7HwoUKAB7e3v88ccfiI2NRe/evTMqv0REREQ2x6waqi+//BIdOnTA/fv3MWzYMHTq1AmDBg3ClStXcPHiRUyaNAmLFy/OqLwSERER2SSzAqrz589j1KhRKFCgAMaMGYPo6Gh06tRJc/yjjz5CREREumeSiIiIyJaZFVBFR0fD19cXAODk5AQ3Nzd4eHhojnt4eCA2NjZ9c0hERERk48wKqBQKBRQKhcFtIiIiopzIrE7pQgg0btwYDg7SabGxsWjZsiWcnJwAAImJiemfQyIiIiIbZ1ZAFRYWJttu3bp1ijTt27e3LEdEREREWYxFARURERERpWFiTyIiIiKSM7mGqnLlyiZ3QD9z5kyaM0RERESU1ZgcULVp00bz+O3bt1iyZAnKlCmDkJAQAMCxY8dw6dIlDBgwIN0zSURERGTLTA6otPtPffbZZxgyZAimTJmSIs39+/fTL3dEREREWUCa+lBt3LgR3bp1S7H/k08+we+//25xpoiIiIiykjQFVK6urjh8+HCK/YcPH4aLi4vFmSIiIiLKSsyaNkFt2LBh6N+/P86cOYMaNWoAAI4fP46ffvoJEyZMSNcMEhEREdm6NAVUY8eORZEiRbBw4UL8+uuvAIDSpUtjxYoV6NixY7pmkIiIiMjWpXkeqo4dO+Lw4cN4/vw5nj9/jsOHD2d4MHXgwAG0bNkS+fPnh0KhwObNm2XHhRD46quvkC9fPri6uqJJkya4ceNGhuaJiIiIyOSASgiRkfkwyZs3b1CxYkUsXrxY7/FZs2Zh0aJFWLZsGY4fP45cuXIhNDQUb9++zeScEhERUU5ickBVtmxZrFu3DgkJCUbT3bhxA/3798eMGTMszpyu5s2bY+rUqWjbtm2KY0IILFiwAF9++SVat26NChUqYNWqVXj48GGKmiwiIiKi9GRyH6pvvvkGY8aMwYABA9C0aVNUq1YN+fPnh4uLC168eIHLly/j0KFDuHTpEgYNGoT+/ftnZL5TuH37Nh49eoQmTZpo9nl5eaFmzZo4evQoPvroo0zNDxEREeUcJgdUjRs3xqlTp3Do0CGsX78eq1evxt27dxEXF4fcuXOjcuXK6NatG7p06QIfH5+MzLNejx49AgD4+/vL9vv7+2uO6RMfH4/4+HjNdnR0NABAqVRCqVRmQE7JVOrXn+VgfSwL28GysB0sC9ti7XIwe5RfnTp1UKdOnYzIi1VMnz4dkyZNSrF/7969cHNzs0KOSFd4eLi1s0DvsCxsB8vCdrAsbENsbKxV75+maRNsUUBAAADg8ePHyJcvn2b/48ePUalSJYPnjRs3DiNGjNBsR0dHo1ChQmjYsCH8/PwyLL+UOqVSifDwcDRt2hSOjo7Wzk6OxrKwHSwL28GysC1RUVFWvX+2CaiCg4MREBCA3bt3awKo6OhoHD9+3Gh/LmdnZzg7O6fY7+joyD8QG8GysB0sC9vBsrAdLAvbYO0yyFIBVUxMDG7evKnZvn37Ns6ePQtfX18EBgZi2LBhmDp1KooXL47g4GBMmDAB+fPnR5s2bayXaSIiIsr2slRAderUKTRs2FCzrW6q6969O1auXInRo0fjzZs36NOnD16+fIk6depgx44dXF+QiIiIMlSWCqgaNGhgdIJRhUKByZMnY/LkyZmYKyIiIsrp0rz0jD5nzpzBhx9+mJ6XJCIiIrJ5ZgdUO3fuxOeff44vvvgCt27dAgBcvXoVbdq0QfXq1aFSqdI9k0RERES2zKwmvx9//BG9e/eGr68vXrx4gR9++AHz5s3D4MGD0alTJ1y8eBGlS5fOqLwSERER2SSzaqgWLlyImTNn4tmzZ9iwYQOePXuGJUuW4MKFC1i2bBmDKSIiIsqRzAqoIiIi0KFDBwBAu3bt4ODggNmzZ6NgwYIZkjkiIiKirMCsgCouLk6zHItCoYCzs7NsVnIiIiKinMjsaRN++OEHuLu7AwASExOxcuVK5M6dW5ZmyJAh6ZM7IiIioizArIAqMDAQy5cv12wHBATgl19+kaVRKBQMqIiIiChHMSugunPnTgZlg4iIiCjrSteJPYmIiNIN5zWkLMSsGqpFixaZlI5NfkREZJEff5R+PvkE+OwzwMnJ2jkiMsqsgGr+/PmppmEfKiIistjSpdLvn36Sfk6dAl6+lIKsli2BEiWsmj0iXWYFVLdv386ofBAREQFr1wJz5+o/tmgRsGWLlObUqczNl7XFxgJ37gBlylg7J2QA+1AREZHtMBRMAcDVqxlzzwMHgAULbLvP1pw5QLduwNSp1s6Jdd28CaxZAyQlWTsnKZgVUB09ehR///23bN+qVasQHByMvHnzok+fPoiPj0/XDBIRUTb3/DlQrRpgqLuI3buvKnv7jLn/iBHAr78C334LrFplk1/W2LJF+r1nT/K+V6+Ahw8Nn6NUAqtXS0FIdvHRR8C8ecD69dbOSQpmBVSTJ0/GpUuXNNsXLlxAr1690KRJE4wdOxZ//fUXpk+fnu6ZJCKibOiff6Smu/ffl7aPHNGf7t1k0ukeUL16BYSFJW+vWiU1K65Zkz7XF0IKFKtVS79rRkdL1/vhB6BxY6BVK8BQd5xdu4D584FevdLn3qmJjTWcl/R25kzm3McMZgVUZ8+eRePGjTXb69atQ82aNbF8+XKMGDECixYtwoYNG9I9k0REZEV79khNcelZc7NpE/DFF0C/fqmnTUiQftulcy+VrVulH10LF6bP9f/6K/nxvHnpc021ZcuSH588qT/NwYPS7zdv0vfehvTrB3ToAJw/n/H3Ur8nbIhZ784XL17A399fs71//340b95cs129enXcv38//XJHRETWN3q01BH8wIH0u+bXX5ueVv3l6WD2amnGZfQX/3//pZ5GiNTTREcbP56YmPr+8+czvo/Y5cvS72++ydj7ADbZ382sgMrf318z0i8hIQFnzpzBe++9pzn++vVrODo6pm8OiYis5c8/gebNgevXrZ2T9GHKl7cu7S+uuLj0y4u5eVCpAIUifa9r7PWYMsXyGrmffjJ+/PRpqblz9275/v37gY8/BiIipO3QUOPXMZRP7YDq00+BFSuMXye9/Puv4WN79wJ9+gCRkZbdI6sHVC1atMDYsWNx8OBBjBs3Dm5ubqhbt67m+Pnz51G0aNF0zyQRkVVMmQI8fZq+I6tUKumLMrO/EKZNA9q0Sdn8c+sW8N13Uv8XfZTK5McKBbBxI3DtmrR95Ij05ZgZLRNffCEFIGrawZAQaWvWMtaE+OefwM6d5l9T7dUr48cPHQL69gVevADGjAG0B3SNHCkF8QMHStvaZaCPobLTrblKLcBLK6XScP83XaNGSf2fTJwo3CBDtXJWZFZANWXKFDg4OKB+/fpYvnw5li9fDiet2Wt/+uknvK/uXEhElF2k9oVmjp9+Ajp1AlauTL9rmuKPP4AHD4DwcPn+Hj2A5cuBevWkgOX5c+lLXk37iz48HJg5E+jSRdoeMkT6cpw40bQ8PH+e3Czk4mJe/nftkm9rd0r+9lupg7Z2wGUKV1fjx6OizLuetj//lG8HBsq3hw2Tb3/3XcprPHsmTWaaGn0B1e7dKYOc9O6DprZ8uXyEZt68qZ/z+rX0frh7N233TEttawYz69XNnTs3Dhw4gBcvXuDFixdo27at7PjGjRsRpj1igogoJ3v5Uhr1dOdO8n/U6s7ES5Zk/P1VKukLS/vLR/dLVfvLuG9fqQmqaVOpGenuXSnvaoaacm7dkkaeqQMtQ3r0kOZSunrVtC93Y0Gn9sodP/8svb7TpqV+TW2p1RJaUouoG0Cl1v9r/379+w3VPmn7/Xd50K9SSbVeuhQKKRC9eDH1a5pDt+bLUO2R9vvQw0NaVqhjR+DRo5Rpz5yR+n0JIb0X//pLSq9mg1NbpKmHn5eXl979vr6+FmWGiMgmpfW/4SZNkh/Xq5f+I71Ss2yZ9GX36afJ+0zt2H3njlSTps3QtAWvX0u/r12TvkwN3UM9Z9LgwakHCseOGc/r3btA9+7StdTMLaeMHCmm+/xSm6PRUBOhKfMtvX0rvdfUQZmha8XFSU20AHD4MODsnPq11d68Aa5cAapUSRkMe3gkvwcAqebp3r2UQaVubafamTNAixbS44QE4Lffkv9WCheWB/VqNhhQcaZ0IqLUvH1r+TXSc4ScqdQ1B9o1CKY2++j7EtNuftIO0rR16AA8fpxyv3Ztj3aToiGpBX5xccClS6ZNu2BIakHO27epBn52SqVU46YbzMXEmHcv9Wure53Vq42fp6bdh0w7uNGmfW11GcTESE2DqQUoQ4dKr/Xvv6c8pm8wWrt2Ka9pKF9ffZU8xcRPP8n/8dD3PgSyfqd0IqIc6b//AEOTFv/9d/KkjUqlNFGloZoPU78cDTl/HvadOsHHkiVYtL/YdfslaUutZsTQlAP370t9mnSl1kkbAPr3l35365a8r2LF1M9LK3U55c+v//jy5UDDhtJr9uWXwKRJKQKeWhMmwL5HD2DbNvm56oCqbFnptymriCiVlo+kPHFCmk08NeqAeMQIqf/TL78YT3/2rPR706aUxwwFNz//LN82FFAB0msLSDPWm4I1VEREWZT6P/MXL4APP5SaV1QqqUP2vHnSMPD586X/4mfM0N/8pN3vx1QvX0qjDSMjgUGDgLt3UWH58rQ/j927pSaVhARg7FjD6SyZiXr79pT79PWT0TZvnjSj9/Hj8g7OX32V9nwY8uqV9PxNCXKSkoAdO6Sfv/6Szwl17Rrs1X2XwsKAAQOAtm2lfnPqgCogQPodF5d6k+T+/aYFQ2q6weaaNVIeTGnKfPJEqplSl/NvvxlOqx0I6VvqxlAN7pIl8r5hurV2+phaG6wO4v76S+rQbwOd1NN5ljQiIhsmhPTFni9f2q/RtKn0e+RIeT+Q+HhAvVLEli2Aj0/a76FN3Q9Ld9RYWh05Iv1YMoItLVL7wqtcWfqt208rVy7T73HvXuppbtwAOncG3nsvOfBILW9TpiQ/jo0FvLyA589h3727PN2JE9LvDh2S96kDqsREqQZKa2R8CsYCXH26dZPeh2rm9tHTDlwfPZImWx04EPD2Tt6vUkm1dGr6RrwaC4JGjpRqbQHjNVTmUtdQqWu2li/P/D6KOlhDRUQ5x/z5QMuW+pst1GJjpT4xptReaNe6/O9/8mO6zR1p8fy55dcwxJJaLlNs3pz8ODZWWnvOGEMdpN3czLvvwYPGa9c6d5Z+HzuWthnY1X2q9PUl0ke7f5ElS8BUqZJyX7168pFv+hQqZPo9Nm0CZs2S79PNs+5UE+bUDBkLqDw9pd+mrtd4+7Y0slSLvaHFtTMJAyoiyr5OnAA+/zx5CRB1XydjTW+9e0tfUrVrpzz2/ffy/jZpacLTpl44d+pUaboC3eYUU4bMW5t68kld2v1qfvgh9U75hlbZMHe+quHDpZFspnzRq2tWdL/EBwwwfE6nTlL/MVNnbffySg5C5syR+irpBsrqYMKQSpXka/cB0qSjCoXU/GxIvXpSE6Q5/vlHqsFUBz+6fbq0a6v0HddHXRbGAqroaKnZ0VgNno1jQEVEWc+TJ1JwlFqfjLlzgX37pBnCdZtttI0aBbRqJTWrqWcB1+f77+VBT3qteL95s/Ql26qVvPO2uaMLY2KkL8TMWiLG0TFlzZx2Xt4tVaZp8jHGUICS1skoTXkN1E2EugFVarU+n35qOADUVbhwckC1c6e08HLr1vI0qU0wWqSI/HVwdAT8/FI/18kpbTOKDxkivQZJSdIM9docHKT+fN9+C6xaZXhhZm3qZsLUmvxmzEguN3XTehbCgIqIsp7Bg6X+EjNnGk+nXgsNSNkH6dAh6XdcnLS+2MOH8qDLWho3Tq7BMDcw+uIL6adly8wZVp43r+E+TosWSX2J9u1LfXHf1JQoYf45ul/eMTGGRyaq52YCAF9f02pJTF0A2Nk5ZVCjW66pBc7qpkl1P7M6dZKPGavB27VLfxOavuZDXQ8eADVrJo/u087LyJHSpKuLFsn7cBkSFyfN+m7OZLa5c5ue1kYwoCKirEcdKOkuKqsttYBi1ixp5NaxY+mXr/SyeLG0ltvatYbTXLsmzXitPXxcvdTIy5fSMP+M5u4u1Zx88IEUWJUqlTKNdpOrLvWUAqlZs0b/0izGaAdU9+4BDRoYnjurfHlptFifPum/gLCLS+oBZWpTSqhnoJ86VXoOEybIr2+It7fU5Kc7waYli0wrleYvFh4Xp3/mdmNMrQG0IQyoiCjrMjY8XD36x5CHD6WgY9So9M1Teti3D/j4Y6n5zgD77t2lpVzUcz7pfmkbORdDh8q3dedh0p4Hyhh3d+n3pEnS8PhffwWaNTPt3EOHzKtFK1NGqjkKCjJtKoXXr6Wm3X//BZYuNZ7W3V0a+dmnD1CggLTPlFocU5jbB0wff//k3wMGyPtc5coFtG+v/7wlS6S0f/whzYyulju31NcsLQyta5krl9ThX988UlevAufOGb5mo0Yp91WokLb8WREDKiLKXq5fl0Zgbd1q7ZzIlStn+rBuUybBVPvlF+kLS9+Xkpqvb/LoNiA5aACk2gvt5tDAQMNf0Lr0jcxLrbnMzk7qSO3iIq0BaCpXV6lGcv16qa9Zanr3lhYg7t07uS+XsWvr+u679Jn6wsVFPo2CPsWLy7fLl5dPjurhYfz8ceP051V7+gNnZ6lTfK1aUjNdausu6lLPSK+7uLZarlzSdUuVkgaDaL/fPv9cnvbLL+WjD6dOTXm9+vWl8jN14W0bkK0CqokTJ0KhUMh+Sumrgiai7EHddPH0aXJn8Y8/NjyreWYrWjT58Tff6G8SSw+pdaLeuVM+xFzdoRmQvmi1m4AcHOQBRq1aQLFi+oNBfZN1phZQ/fhj8pfkxo3S7549jZ+j5uqaPMVBWJgULBmjHjRw86bxdPqawBQK+XxlBvrrPU2tJsXFBRg9WmpeNuTrr+Xb7u7SPFknTkgTnZrSRKev8766BlGtQQOp35O56+6WKCG9h9T09ct68kSeF2N9q/z8pElqAam/n5OTvJa0Y0fpOX/yifT+M8UXX0CY+o9ABslWARUAlC1bFpGRkZqfQ+qOp0SU/ai/vJs3TzlCztrWrAHWrZO+FA8flmoZ8uRJ2SS2ZUvqTVKWUiiAunWlmpKvvpLP7aRby2RvL/8irlBBeh7vvZfyuvpqmFKb0ykoKPlx/vzSCEBDUy8Y07Il0Lev+eeZQ7szeePG0og29SK+73jfuGH8Gr6+0utvrJN14cLyJlr162pnZ/q8TLrp3n8/9Tm8xo0z7dq5c0t/Y2qmLvsSGqp/v7u79Hpu2ZLcHyxPnuTj2rPFa78XDc1V1qUL0K4dVMamu8gE2S6gcnBwQEBAgOYndxYcKUBEesTGJnfWVlMq5f1wGjfO/HwB0np+o0ZJw77V3NykL1I7u+QvAoUiZfNG/vxA9eoZn0c7O6ljcKtW8i9a3SkkFAp5LZN6AIC+TsL6ak6M9YsaMiT1OZcyQ40apqXTHX2n+9oAuNmunbxmRN3UlSuXtHajdoBpqG+anZ18tKS+xaVTo1tDpVvrpU/79oZrzgYPTn6cmCjVbhmjby1A9Uzyut/D6ibM/PmT863dx0q7+VL79fv4Y/kcYYMHS4HosGHG85ZJsl1AdePGDeTPnx9FihRBly5dcM+UpQiIyHY8fy71f9Kdqfzbb6URWB9/nLxPCPnUCBlBX5PO338nzyg9c6a0vEinTvJgwljTV7Fi6ZtHY/QtgKzdvKI7pF83SFIHH9r71X2Y/vor5bXv3zecF+2+W+nF3OYrwPT18vRNZ6AVuKhGjcKTKlWg0h7Y0LChVOu2fz9QsqT8XH0zeatHHlo6oaWXV/LjokVNH8mnr9IhKEj+vn/71nDtECClLV065X51EK47hUVqNWfatVLagWJCgvR6LVsmBa6dOyfXANqAbLWWX82aNbFy5UqULFkSkZGRmDRpEurWrYuLFy/Cw0Cnvvj4eMRrfXBHvxspo1QqoTQ0moEyhfr1ZzlYX4aWRWwscOeONIoLgH2/flKQtGIFVB07Qryb6dn+zJmUs18LYd5ismmQNGsW7LWaLpI2bpT6gNStCxw9Ku1897ooXryA3bs8JuXKZXBElD2geS5J79LYtWkDhbElcQAgb14oO3QApk2DEAKpjZFLWr5cqvnQkw979WuZkIAkpTJ5W6GQ8vTXX1DcuAHx3nvJ52/dKg2BL1AgublI59r26qkb9FDFx0Ok83vIPg1rEia5uCQ/XySXgS676tWh+PtvwMUluZwSE6F4d25C8+bAvn1QKpVQDBgAxZ07UJUta3gkHCC7LwAk9e6tSa85VrSowTwZNHo07L/4AqpPPoFo0cJoHlLLE9zckOTgoNmvatwYws4uZTo1OzuD+bUXIkVgqu9vw65aNSjeTRKapBPka/KRJ4/0/qlYMXlhaK3rWPu7IlsFVM212ngrVKiAmjVrIigoCBs2bECvXr30njN9+nRM0jO8eu/evXAzdw0pyhDhhkaVUKbTVxb2cXFwf/AAr8z5r1jry7vMypXIc+ECLnXvjmcVKqC+uiPx2bPA2bM4/ewZYgoUQOUnT+CZyX2klG5uOHL4MOq/u+/tFi1w78IF4MIFvel97txBhXdp9+urGXrHt0IFlD91CpE1a+L6tm0AAIciRZA7NBQvihfHezrNNYkuLrjfqBGeVK4Mn6tXUQLAaxMmy9x//77BGiP1c4p1csLJbds02/85OSHiXZ4AANu3pzzZyBD4QnXqoIiBEZaXT5zA03T+0itRsiTyvVuU+Gn58kjw9ITL8+fwu3LF4DmnTp1C4qBBKLdiBSJat8ZL7eerxalIERSqUAEPa9dG3Ls0Ja5eRT51Ge/bB+Dd34Wfn/RjrPM5kl93tf1a91Yfe/34Mc4YyJNR6glK3+XLVEWqVEGhvXs12+cqVED8sWOo8S4/l27dwks7O9Q28Pf38t49nNOTX6eXLxGic86ZwYPxWk/+fIoUQYVdu6B0dcURnWsF1awJ32vX8G+uXICR1yXWyks1KYQwZ2XDrKd69epo0qQJphsY9aOvhqpQoUKIjIyEn/ZIGMp0SqUS4eHhaNq0KRyz4CRv2YmxsrCbOROKTZugGjoUolUr2E2YANG4MUTz5gaXDbEbPhyKx4+R9PPPsK9bFwAgatWCat482Ot0flZNnQrRpAnsxo+HwthEnulENGoE1cSJUGzdChESAgQEaPKkWrgQomZNIycLKNasgShTJnlWa0OePJGaW3Rfo7dvYa/TX0X06gXVuxFtqs2bETduHDw8PaHQE8Am7doFu+++g+qTT5LnL9JD/ZxEgwZQzZgB3LoFxf79EB99lPpSKKk5fx6KR49gpzNfVNK2bWlrojMmNhZ2334LERIC8e69ZDd1qlSzZEDSmjXSci5poH3ttwcPmv0ZZTdxIhRaQVeS1sSymjJp317ejJjREhKgOHkSokoVqand2xt48gT275p2VfPnQ1SpAvv69fWfX768VBuq6/lz2Ot04k8yNJGuENI/UYULp3m6iqioKOTLlw+vXr2CpxX66mWrGipdMTExiIiIQNeuXQ2mcXZ2hrOetmFHR0d+idsIloXt0FsWmzcDCgXsNm+WhmQDUlPY1KnSsYIF5en79dOs7WZ39WpyrZZCAXtHxxS1XHYPHkgjmNSdvDPatGlSPjp2TN73rjO8Xe3aqeehRw/T7mOoP5G9fcp7+PhIeQKQ+K7zskKhgJ1uuo8/hp2PDzB2LFIdGzZnjtQnbehQ6dolS6bs85NWVatKv+vWTZ4fa9w42BkJ8NLMywsYP16+T71wsAF2gYFpn4lbq3zUfwtmfUZ9+KFsCgI77fN++02a56lLF015ZwpHx5Sdzv38NM/TLlcu439/hQrJn4darlwp/56NPS9TBwsYYO3viWwVUH3++edo2bIlgoKC8PDhQ4SFhcHe3h6dtScYI6KMoW8ASJs28oVxnz2Tb2v3lRAi5YgzQOqAamlVfvXq0nxABw+mnlbfh/J77+mfNiAjaNdYffAB8OYN0K5d8j5DnYNdXZPn9jFFw4bST0by9AQ2bAAuX5aeS2ZJbUFgS2rgUpsWIjXGgtbgYPm6gtbk4iKNSvzvP2nWeH3BVM+e0pp/hmZd150Hy0DXm+wiWwVU//33Hzp37oyoqCjkyZMHderUwbFjx5BHe34LIsp8KpUUSOn2vdGeLPLZM6BePf3nr1olzatjrvXrpSYmdRNCQoLxiQLTa7kRS7VrJwWoX32Vcn4h7YDq/feT5y8aOtTykWIZoUiRNDevpdm8ecCIEfqPWdrkaGktaXo3eWYkfaMStZkyf1jt2snL3pi6LFEWla0CqnXr1lk7C0TZW3S0NNN1v36mzyc0YYL0JbJ6dcpj2jNYpzZBorG16XT5+0vLd2jPVA5IAYd2EKJrwQLT75GRvvjC4CGhHTRp9/O0tN9TdqIbmI8fLzUN/vUX8Nlnll27Z0/p/dO6ddqvsWKFdB3tZuXs6pNPkgMq7aVwsqFsFVARUcayb98eiImRRjLt2iXNoJ0afaPEMpqxdfymTZOGcR84IG2XKwdcvCjVomSFkb3aNVTatR0ZPR9XVlOwoNRcBQDvpt4wut6hqQICpPe+vb1ZUxPIlC8vjcTTnswzK/jiC+nvxxzaTei6TYDZTLab2JOIMpB6gr7oaKljramL/WaE0aPTfq72BJxjxki1VtZ8LubQDqi0J3Ns0ybTs2LTLO3rZIypy8EY4+5uMxNSmqxdO6kJzxzqWmJHx7QPBMgiGFAR5VRKpdThOa30zZKdGVq3lmqXOnaUj5QrV076bWxaAzXtgKp0aem/bt3RiLZKu9O69uNChTI/L7YsIwOqnCww0Lz0Hh7SP1/mNNlnUXzHEeVUXbtKfZh275bXdJjq3WSKmW7kyOSmuY0bge+/B+rXB/LlkxZbNaWmxth6c7YuKAhRZcrAo1YtqQnrm2+kWaOzWm1HRqtUSd5Hj9LHZ58B165J0z+YKofM6ciAiiinUn/ZHD+ethF0GaldO2nh0wsXUo400u7n5OQknypAvS5aavr0kZ53hw6W5zWz2dnhYq9eCGzRQpqraPv2bN+UkiYDBkhD/zNzuoacwMtL+ieGUmBARZTTxcVZOwcplSsnNRVUr54x169QQepYbGCNzyzFFqdKsAWensCwYdbOBeUg7ENFlBP8/Tcwc2ZyU5f2ilPffWedPBmjnkRTt+YlPf8z9vIyuDQOEZG5+GlClJ09egTMnQtMnCj1N1LPFK493PvJE+n3wYPSnFFPn6b9fm5uwNixaT8fkCZkzJs35f68eW1n4k0iIh0MqIiysn/+kfoCGfL558DatcnbI0dKS0Xomz9n+HCpP07z5tL227fSYqWmduAuUABYuBD43/+AI0cMpwsNNT4iTXcm6eLFpd8rV5qWDyIiK2AfKqKs6vnz5Bm1v/xSGul17ZrUwVzdlHX1asrzWrdOufZWfHzKdN26AbduSR231ct42NnJmwu1/fln8mMnJ2n0oJ2dfL24DRuSlyHp3Dl5dvQhQ5IXVtZt5tMOCImIbBQDKqKsKiYm+fHUqcmPo6KALl2Mnzt/vnx7zhz59osXUjAFSE2F6oBKpZIPz583T1rypHz5lPdQT8VQs2ZyLZr2HDZJScmP338/OaBKj0kTiYgyGZv8iLKiFy/01yoBUrA0c6Z519u0Sb7dtGmKJMX++CPleV5e0kg8FxfD165WLfmx9mSL2jVd2s189++nklkiItvDgIooKxECOHRICnhmzzacbuNGyzqX61FAvcCpNlPWIsufX/9+7Roq7WY+7dnPiYiyCDb5EaUHlUqa86ZgQf1rzJ05I62DV79+cidvQ0P2L1yQaowGDZJqbs6dk2Yl//RTaQTd3r3J1zRG3bk8I5mymHCTJsCxY8lLw6glJiY/ViikZsVr19JnAVsiokzGgIoorSIipN9Fi0pBkHpk26hR8n5GQkgzcwPAtm3S8ZgYqYO2vv5CPXtKv+PigOnTgV69pG1//+RgKrMZGulnrKlPzd4eCAtLub9wYeDhw+Ttjz9OU9aIiGwBAyqitEhIADp1kh4fPiyfbTw+Xh5oaB8LDwcuXZIe378vBRXatPsVqTuFq507Z3G208puyRL5jlKlpBo2b++0X/TLL4EFC4CPPrIka0RENoEBFVFavHkjf5yQIN/WDqhevUp+rD26Tl/t1LFjyY+fPZPXDGlPS5BenJykeZ7UQZ4Bil9/le9YtUr6bclM43nzAtOmpf18IiIbwk7pRGmhHegoldIkmGra0xkAUt8pU0VFJT+OiwMGDkxb/vQZPjzl6L0dO9I2TYGdHZdtISLSwk9EorS4dy/58du38ikMtGuvAMOLD+ubrXzixOTHCQnAyZNpzmIKPj5SnyztUXeenuZfR3cmcyIiYpMfUZpor2L/9q28yU+3hko3wFLTHuWWGYoVk37//ru0xp+h6QxSo56Ak4iINFhDRTlTbCzwww/AnTtpO187SHrwQKr5UYuJkY7PmgX8+y8wZYr+a+jWUGlPgJneZs0CSpSQHjs6SmvpqZv6nJzMu1apUumbNyKibIABFeVMM2YAy5ZJC/laSnfeqZcvgV9+kaZF6N3b8ASb6lqtVauACRMsz4cxxuZ2Gj0aCAgAxo8H3N2T9/v4AEOHypKqxozJoAwSEWVtbPKjnGnbtrSfa2hxYLVp06SRc6mJigIiI81vQgsLAyZNMu8cY4oUAf7+W3pcp440uWbt2tJcWuq5td4Rbdum332JiLIR1lBRzlSxYtrPTS2gAoAbN1JPM3Ys0LKl+ffXN8nme++Zv36fPnnySEGVemJS7SVhiIjIIAZUlDNZMlLt+vW0nRcYmPZ7atNullNf99tvgcaNgX/+kY8UtJS5/auIiHIoBlRE69ZJvxMSgO3bgRcvpG3t6Q6USmmqhGPHpBFyaeHllfY8enpKTYmdOgENGwJffJF8THu2cl9f4MMPgYULTWt2TA1rqIiITMI+VJS9HTkCHD8uTXOgvb6etjlzpOVPvvwS2LNHqgHq3BlYvlxqRlMqpU7m165Zlhd/f2nNv7RwcADef1/6AYB27ZJnGdcX9NSuLY0aXLRICsDSwcVPP0XNdLkSEVH2w4CKsr4zZ4DLl4EuXVIGTUOGSL/z5pWOq+mma9cuebLOmBgpmAKA9BzVNmyYtJzM2bPmn6tvVvLRo4GffjKcR2dnaSFmS7i6ah5GlS5t2bWIiLIxBlSU9fXpI/3OkwcoX15qAnNzk6fR7ve0aJFUE6VNe+ZzUwUEAIMGSTVbpvD3l+a+qlVLPhGoKfQFVB07Ah06GK55Sw/BwcCAAVB5enKpGSIiI/gJSbbnwQP5Ui6m2r4daNUqeW4p7dF46toVpTJ5YV9LubgAzZoBp06Zll4d+CQlJe87dMj4+WXLSr8HDDB+zYz06acQrVtn/H2IiLIwBlRkW06eBFq3lmpfzHXihPT7yRPpt3bgolJJI+BCQizPo5r2gsjm0M6Xi4v0+48/pFF6uiZNAv78U+poTkRENosBFdmW/v2l3w8eGE8nBHDzprzpTLtJKjZWvrTL3bvykXHpIa2TXKqnT9DOb2Ag0LRp8na3bkD79kBQEFCgQNrzSEREmYIBFaU/lUpaO27Lloy7x+bN0sg83UWK1V6+lAdUv/+e/nno3j358dy5Usf2o0dTP2/uXKBePWDlSvn+996TZi2vV0/qTD9uXOY06RERkcUYUFH6O3lSWsdu8uTkfRERUjNeajVP2s1eEREpjwsBvH4NfP21tK1u5tN19aoUcKVm6VKgTZvU0+njoDWmo359qQbMlHmbgoOBefOAMmXk+93dpddt3ry05YeIiKwmWwZUixcvRuHCheHi4oKaNWvihKEvXcoYb96k3NepE3DrltQ/Stt338lrj7RG5ymeP095nTFjTJtXafTo5L5Uxjg7A7lypZ6OiIjIiGwXUK1fvx4jRoxAWFgYzpw5g4oVKyI0NBRPTPlypfRhb5/8WLvZTe2//4CDB4F9+6T5nqZPl0b1TZ0K/PWXJplwdQWEgOLkSWn28oSElNMdWMrZWTbXkl5ffCFNELpmjWnX1NdXq0gR8/NGRERZRrYLqObNm4fevXujZ8+eKFOmDJYtWwY3Nzf89NNP1s5a1qZSSf2D1MuyqFRAeLg0USUgNcX9/jtw7pw8oNJevkWtTRtg+HDg88+T99WuLfWL0qKIj4fXrVuwGzxY6rBdq1a6PiUAUkClHmlnSO3a0pp2JUoAX30l7VPPWK5Po0Yp9/3wQ9rzSERENi9bTeyZkJCA06dPY9y4cZp9dnZ2aNKkCY4a6CwcHx+PeK05j6KjowEASqUSSn21KzmUYvt22E2aBABI+uEHKC5ehN2CBUChQkjauBG4dAn275ZCEW3aQPFuDijVvn0QzZvDXntOKBMlxsQgePt2CCGgSrdnIpekUEDh4AA7I/lL8vFJrmlr3hyoVEma1NPQ+8PRMcXzTXJ1NZw+C1D/LfBvwvpYFraDZWFbrF0O2SqgevbsGZKSkuDv7y/b7+/vj6tXr+o9Z/r06Zj0LlDQtnfvXrjpzradg5X96SfkfvVK2ujQAa8LFoTHq1fAq1fYv20b8p45g9Lq4z//nHziiBHYLwTqq4+Z4fKRI8jj7o7Xt2+nwzPQ78jBg/C7dAkljeRv//bt5l1U5/le6tYNz7ZtS2sWbUp4eLi1s0DvsCxsB8vCNsTGxlr1/tkqoEqLcePGYcSIEZrt6OhoFCpUCA0bNoSfn58Vc2Zb7MLDofjvP822Z6FC0mg7AC1atIDCxQV2f/+t99wWLVrAXnvEn4mqFi+OiBs34OHpCUU6TR+g+uwz2Gk1vzVt2RIKb2/Y/fNPcqLixYEbNzSbLVq0MPs+9lOmAABEz56o0bdv2jNsI5RKJcLDw9G0aVM4mjKSkTIMy8J2sCxsS1RUlFXvn60Cqty5c8Pe3h6PHz+W7X/8+DECAgL0nuPs7AxnZ+cU+x0dHbPHH8iTJ9Iad5YGJK9eya/h6qrZtnN0lG3rsnN0TNP9HefMQf5Xr6Dw8oJdaue7uUmTeabCztlZlhc7d3dpugLt648eLc0R5eICfPKJlH9zqa+XmAj77PA+eifb/F1kAywL28GysA3WLoNs1SndyckJVatWxe7duzX7VCoVdu/ejZD0XHIkq/jzT6BFC+DHHy27TlQUcOWKfJ92R24hgMREw+dfu2bZ/U3h7m5aOnt7oEoV+bbuH2G+fNICyrNmARUqpC0/6o7p7dql7XwiIspSslVABQAjRozA8uXL8fPPP+PKlSvo378/3rx5g549e1o7a6a7eBF4+NDy67xrdsKyZfL9sbHyhYNTc/y48eMHD0q1OoZ06WL6vdJKe2ShMY6OgLe3fJ9u7ZdDOlTczpwJ7N+fvMwMERFla9mqyQ8AOnXqhKdPn+Krr77Co0ePUKlSJezYsSNFR3Wb9fgx0KOH9PjUqfS//sOHwP/+J81Irg641J48Ae7dA6pVk+/X19x25EjyY60+aFbRrx8QHW3aPFGOjilr0zw85Ns+PpbnSaHghKFERDlItguoAGDQoEEYNGiQtbORNjdvJj9OSjK95kUfOztpvihtW7ZIE2Ru354yoPrwQyn9d98BVavKr2PLPvgAyJ1bWtJl507jgaiTU8rpC8qWlZrmXF2Bzz5LnxoqIiLKUfjNYWu0a4Pi42VLsZhNN5gCpGAqtfTHj8sDKls1a5YUHOXLJ223bSutqTdjhtRPas6clOc4OkqvqzY7O/2zmxMREZnIxqsecjhjwY+aEMCSJcCmTSmPqQMNbW/fJj/WDrjOnNG/X/cca9NeyLhRIyA0VH7c11cKtJo00X++g4O0riAgTdBJRESUDlhDZWu0m6MMBVRKpRQYKBTSCDr1sjpt28rT6ev7pN2cFRcn9fOJjQX69NGfB3U6W1GhQoolavQy1H9JqZSWsfnpJ6BYsXTNGhER5VysobI12s1Ruk1TgBTctGoFqEctxsQkH0tt5J7uAr/qc7WvAaQM5NQ1VC1bGr8+IE3TkB7695dvV6ggrRX4wQfA2LHAhg3Gz9e3Pp+LC1CvnhRoVqhgWXMqERGRFgZUmSUhwbSpCrSDGX01VJcuAU+fSlMrJCXJO4ynto7RkCHybXUgpXsf3dlm1QGVq6vx6wPJiwcDQPny8lqzjh1ND7h69oRKu1/T0KFAUJDUSf9//wOKFDF+vm7t3Lp1wKFDKUf0ERERpQMGVJnh2TOpT8+XX6aeVju40VdDpR00xcXJAyp96dWWLEm5780b/eft2SMFP2rqgEpfrY+2EiWkJsX27aXtgQPlk2bmyQPkzWv4fO20dnYQJUroP5YWbN4jIqIMxIAqM/zzj9RPaefO1NOmFlBp73v1Srq2sfRq6n5W2tSBkr7zbt1KHiUXGSn9dnEx/hzUE4iOGSPlq1o1aZoCNRcX/SMP1bZuBZo1A77/XtrWDhajow2flxp2PiciogzGgCozmDOPk3ZApa8JT7uD+MqV8r5E2oHRkSOpz7auvpahUXzr1klNjOqlfFxcAGMLRqub0+zspNF2gDygcnWVminVdOfB8vEBpk5NXhrG0zP5WNmyxp+LPp98Il1j2jTzzyUiIjIDA6rMoB0Y6etHdetWcg2MdnCjb3087WvpTpWgHVDp9pfSRx1QGavZ+vXX5MepNfnpG1Wo3VSnW0PVvLm8z5Xu+QEBuNG2LVSTJ6et79OwYcCuXcabGYmIiNIBA6rMoF2rpFvrdOuW1F+pY0epdmrvXv1pjxyRggNjixAbC4z0iY1N/Tzt+xkLqL77Tv9+7YAqOhqoWFF+3NnZaBYf1qkD8f77RtMYZeuzvBMRUbbAeagyg3atTEJCcjPY1q1AWJj0+NkzqQ/SjRvJadUB1dOnyTVO5coZvo+5AZW6NszYBKKmBlSGmgK1m/wuX5ae79SpQJky0r6GDaUpDEqXNi3PRERENoj/vpvjyBFg9mypH9CLF1Kn659/Tv083YBKTR1Mqa1aJd9Wp/3tt+R9Fy8avo+5AVVqfagAeZ8nY4v9agdO2rRrqD78UGrWa9YMCAxMPu+nn4BRo0zLMxERkQ1iQGWOIUOA9euBxo2loAAAvvnGcPqnT6VaJt3JOlUq05ZzUdcO3b9vWv7MDai2bZM6nR88aDiNdgCnrqFST0GgXVvm7q7/fO3FnbWnQSAiIspG2OSXFrozi6vNmwd4ewOffgrcvg106CCNTtNuzrp/37QZx4HkJr86deTTIxhy6JA0E7ip7t4Func3nkZ7ugJ1EPjtt1Jfr2bNgOHDpZF0hjqNa9fOpdapnYiIKItiDZWpdOdPatRIvv3kibSsy5Il0oSZ6gDo0iV5zdGAAabfUx1QGeuIru2PP4Bjx0y/vrmqV5d+584tBYseHsAPP0iBpL4RfoC8ydDSyTmJiIhsFAMqU2kHBoC8z5BKJQ96njyR19ioR9OZa/t24N49YPJk088ZNMjwEjcFC6YtHwBQubLhflLGaL9uhoIuIiKiLI4Blal0a4m0h+PHxSXP7g1INUvaAdWDB2m756VLQLt25p9naE0/3Yk0zdG6ddrO0w1EiYiIsiEGVKbSDVK0a4GePgX+/jt5W3ch5NRmLE9vhjq8pzLnk1Hm9M3SVqpU2u9JRESURTCgMpVuDZV20BIVJT+WkCDvN2XJOnSmmDhRvm0ooEqtyc7TE/jll+Q1+dR+/VW+DIw5qlUDJkyQ+pcRERFlUxzlZyrdgOrFi+THq1fLjyUkGJ8sMzWtWgFbtpieXr1unpq6tszNTd5/y8fH+HUGDdI/waYltUwKRdqbC4mIiLII1lCZSrfJ7+zZ5McHDsiPWRpQmTvJpe50BEuWSL91O8N7eQErVgDjxum/jvYSOV9/Lf1OSx8uIiKiHIY1VKYy1NFbnxEjgPLl034vV1dpQkxTO3SbM/qufHnDowAPHAC6dJEeh4ZK/aY4dxQREVGqWENlKnMCKgC4cCFt91HP9VS7tunnODlJ6+Pp8/330jp7a9cm78uXT3/a8ePl266unOqAiIjIBAyoTGXq5JqWGj5c+l2njunnJCUBdevqP1alCrBzJ1C8ePK+PHmk9fN0GZrtnIiIiIxiQGUqc2uo0ko9tYE5AVxSkvlTIlSoYPjeREREZBYGVKbKrBoqdZ8lfVMfrF4tdSwHgPbtgeBgoGRJaRSeg4Xd4cqUkZr4iIiIyGzslG4qfTVUutMSpKZyZeDff+X7SpcGrlxJ3s6TJ/na2taulZrtdu2Sgi118CNE2vs5bdwo9fX64APpGuwvRURElCasoTKVvoCqdGng6FHTzu/dG1i+HNi6NXlKAgB4/Tr5sbt78pI2rVrJz8+VS/qtUMhrkrSDoI4d5eekNplmcLB0H3t7+VI6REREZBZ+i5rqr79S7nNxARwdTTv/8WPpt7+/PCD677/kx9q1Uk5OwN69ydum1B6NHg18/nnydokSpuWNiIiILMKAylS7d6fcZ04nbu2pCrTndgoOTn6s2w/K3R0oV05Kkzevafdp1Mj8vBEREZFFGFClJjoauH8faNxY2taensDeXvq9ZAnQoYPUQdwQ9YSZgDyg+t//gBo1pMfqOajUFAppeoN165LvlZq8eaWlZ3buNC09ERERWYyd0lMzbBhw/nzyduXKwMGD0mP1jOM1akg/t26l7Mekpt2cp91MeP26tIbejh1Ar14pz0tL36aAAPPPISIiojRjQGXInTvAl1/KgylAPvmlSiU/VqQIcOhQykk5g4Lk29pLv9SpI01ZUKaMxVkmIiIi62BAZYD9V18ldyTXltpcTbrr6k2bJq2Jp610aWkKhJgYoEEDi/JJRERE1petAqrChQvj7t27sn3Tp0/H2LFjzb/Ygwf6J8vUDqj0LTKs20RXtGjKBYYVCvnaekQEAEhKSoIys1YlyKKUSiUcHBzw9u1bJJm6gDplCJZF5nJ0dIS9qf2JrSBbBVQAMHnyZPTu3Vuz7ZHe69OlFlDpUs8fRUQGCSHw6NEjvHz50tpZsXlCCAQEBOD+/ftQcDJeq2JZZD5vb28EBATY5Oud7QIqDw8PBGRkp2zt2iZDAVW+fEBkpPSYARVRqtTBVN68eeHm5maTH5a2QqVSISYmBu7u7rDjhLxWxbLIPEIIxMbG4smTJwCAfNpTEdmIbBdQzZgxA1OmTEFgYCA+/vhjDB8+HA5G1rmLj49HfHy8Zjs6OhqAVHgqPQFTkosL7N/tFyoVVPqaJyZOhH2fPlJ6R8fMW1g5m1E3/bAJyPoysiySkpLw4sUL5MmTBz4+Pul+/exGCIGEhAQ4Ozsz8LQylkXmcnZ2hkqlwtOnT+Hj45Oi+c/a3xXZKqAaMmQIqlSpAl9fXxw5cgTjxo1DZGQk5s2bZ/Cc6dOnY9KkSSn2v379Ggo9/3EcOX4cBapVQ8GDB3GmVCnEbtuWIo1DbCxqv3oFANjP+aAsFh4ebu0s0DsZURYODg4ICAiASqXS/ENDqXutvWwVWRXLIvOoVCrExcVh9+7dSExMlB2LNWdt3QygEMKUjkDWM3bsWMycOdNomitXrqBUqVIp9v/000/o27cvYmJi4Gxg5nB9NVSFChXC8woV4KWnZivpwAFpJJ9SaXzZmchIqXmQ/3GnmVKpRHh4OJo2bQpHU5f4oQyRkWXx9u1b3L9/H4ULF4aL7gAOSkEIgdevX8PDw4O1IlbGssh8b9++xZ07d1CoUKEUnxdRUVHIly8fXr16BU9Pz0zPm83XUI0cORI9evQwmqZIkSJ699esWROJiYm4c+cOShqYxdzZ2VlvsKVQKGCn5w/ETt0nKrUvlcBA48fJZI6OjgyobERGlEVSUpL092Znx34oekycOBGbN2/G2bNnAUj/oQPQvGYZdZ+M0qBBA1SqVAkLFixI1+vu27cPDRs2xIsXL+Dt7Z2u1zZEpVJhzZo1+OKLLywaUHHnzh0EBwfj33//RaVKlfSmscbzs0V2dnZQKBR6P4us/T1h8wFVnjx5kCdPnjSde/bsWdjZ2SGvqevgGdOpkzQFAhGRlvv37yMsLAw7duzAs2fPkC9fPrRp0wZfffUV/Pz8zLqWQqHApk2b0KZNG82+zz//HIMHD07nXFvPH3/8YfEXX0YFZUSWsPmAylRHjx7F8ePH0bBhQ3h4eODo0aMYPnw4Pvnkk/Tp6DpqlOXXIKJs5datWwgJCUGJEiWwdu1aBAcH49KlSxg1ahS2b9+OY8eOwdfX16J7uLu7w93dPZ1ybH2Wvh7pKSEhAU66kzETpVG2qV93dnbGunXrUL9+fZQtWxZff/01hg8fju+//97aWSOibGrgwIFwcnLCP//8g/r16yMwMBDNmzfHrl278ODBA4wfP16TtnDhwpgyZQo6d+6MXLlyoUCBAli8eLHsOAC0bdsWCoVCsz1x4kRZM1DPnj3RpUsXTJ8+Hf7+/vD29sbkyZORmJiIUaNGwdfXFwULFsSKFStkeR0zZgxKlCgBNzc3FClSBBMmTDB7VNSWLVtQvHhxuLi4oGHDhvj555+hUCg0zV1RUVHo3LkzChQoADc3N5QvXx5rdSYxbtCgAYYNGyZ73tOmTcOnn34KDw8PBAYGGv3c7tGjB/bv34+FCxdCoVBAoVDgzp07muOnT59GtWrV4Obmhlq1auHatWuaY+rX8ocffkBwcLCmD87Lly/x2WefIU+ePPD09ESjRo1w7tw5zXnnzp3T/LPu6emJqlWr4tSpU7J87dy5E6VLl4a7uzuaNWuGSPXUOZCaBidPnoyCBQvC2dkZlSpVwo4dO4y+1tu2bUOJEiXg6uqKhg0byp4j2aZsE1BVqVIFx44dw8uXLxEXF4fLly9j3LhxBjujE5Hte/PmjcGft2/fmpw2Li7OpLTmeP78OXbu3IkBAwbAVWdJqoCAAHTp0gXr16+H9rif2bNno2LFivj3338xduxYDB06VDNy8uTJkwCAFStWIDIyUrOtz8GDB/Hw4UMcOHAA8+bNQ1hYGD788EP4+Pjg+PHj6NevH/r27Yv//vtPc46HhwdWrlyJy5cvY+HChVi+fDnmz59v8vO9ffs2/ve//6FNmzY4d+4c+vbtKwsYAanDcNWqVbF161ZcvHgRffr0QdeuXXHixAmj1547dy6qVauGf//9FwMGDED//v1lgZC2hQsXIiQkBL1790ZkZCQiIyNRqFAhzfHx48dj7ty5OHXqFBwcHPDpp5/Kzr958yZ+//13/PHHH5r+Yh06dMCTJ0+wfft2nD59GlWqVEHjxo3x/PlzAECXLl1QsGBBnDx5EqdPn8bYsWNlzZaxsbGYM2cOfvnlFxw4cAD37t3D559/Lsvz3LlzMWfOHJw/fx6hoaFo1aoVbty4ofc53r9/H+3atUPLli1x9uxZfPbZZ2lb8YMylyCZV69eCQDiRcWKQlStmvxDmS4hIUFs3rxZJCQkWDsrOV5GlkVcXJy4fPmyiIuLS3EMgMGfFi1ayNK6ubkZTFu/fn1Z2ty5c+tNZ45jx44JAGLTpk16j8+bN08AEI8fPxZCCBEUFCSaNWsmS9OpUyfRvHlz2fPVvV5YWJioWLGiZrtbt26iUKFCQqlUavaVLFlS1K1bV7OdmJgocuXKJdauXWsw/7NnzxZVtT7bdO+ja8yYMaJcuXKyfePHj5c+L1+8MHjeBx98IEaOHKnZrl+/vhg6dKhmOygoSHzyySeabZVKJfLmzSuWLl1q8Jq61xBCiL179woAYteuXZp9W7duFQA0762wsDDh6Ogonjx5oklz8OBB4enpKd6+fSu7XtGiRcV3330nhBDCw8NDrFy5MkU+kpKSxOLFiwUAcfPmTc3+xYsXC39/f812/vz5xddffy07t3r16mLAgAFCCCFu374tAIh///1XCCHEuHHjRJkyZWTpx4wZk+prnRMY+7x49uyZACBevXplhZwJkW36UBERWYMwY+aZkJCQFNtp6VhdqlQp2Qg/f39/lCtXTrNtb28PPz8/zazSALB+/XosWrQIERERiImJQWJiollDy69du4bq1avL9tWoUUO2nZSUhGnTpmHDhg148OABEhISEB8fDzc3N6PXrlChguaxQqFAQECALO/m0L6WejbtJ0+eIPDdyOugoCDZQKdz584hJiYmxQCCuLg4REREAABGjBiBzz77DL/88guaNGmCDh06oKjWICU3NzfZdr58+TT5j46OxsOHD1G7dm3Z9WvXri1rVtR25coV1KxZU7ZP971DtocBFRHZrJiYGIPHdGdJNvYFrDu9QHr0RylWrBgUCgWuXLmCtm3bpjh+5coV+Pj4pHmUsjG6o+TUw8h196mnWDh69Ci6dOmCSZMmITQ0FF5eXli3bh3mzp2brvmaPXs2Fi5ciAULFqB8+fLIlSsXhg0bhoSEBLOfjzrv5tK+lnpuKO1r5dJZDiwmJgb58uXDvn37UlxLPT3BxIkT8fHHH2Pr1q3Yvn07wsLCsG7dOrRu3dpg/s0JtCl7YEBFRDZL98vPGmkN8fPzQ9OmTbFkyRIMHz5c1o/q0aNHWL16Nbp16yab8PHYsWOyaxw7dgylS5fWbDs6OiIpKcnivOk6cuQIgoKCZH2e7t69a9Y1SpYsiW06K0Po9vM6fPgwWrdujU8++QSAFMhcv34dZcqUSWPO9XNyckq316lKlSp49OgRHBwcNAMB9ClRogRKlCiB4cOHo3PnzlixYoUmoDLG09MT+fPnx+HDh1G/fn3N/sOHD6eo4VMrXbo0tmzZItun+94h25NtOqUTEWW2b7/9FvHx8QgNDcWBAwdw//597NixA02bNkWBAgXw9ddfy9IfPnwYs2bNwvXr17F48WJs3LgRQ4cO1RwvXLgwdu/ejUePHuHFixfpls/ixYvj3r17WLduHSIiIrBo0SJs2rTJrGv07dsXV69exZgxY3D9+nVs2LABK1euBJBcE1S8eHGEh4fjyJEjuHLlCvr27YvHjx+n2/NQK1y4MI4fP447d+7g2bNnaa7NAoAmTZogJCQEbdq0wT///IM7d+7gyJEjGD9+PE6dOoW4uDgMGjQI+/btw927d3H48GGcPHlSFginZtSoUZg5cybWr1+Pa9euYezYsTh79qys7LX169cPN27cwKhRo3Dt2jWsWbNG81qT7WJARUSURsWLF8epU6dQpEgRdOzYEUWLFkWfPn3QsGFDHD16NMWcSyNHjsSpU6dQuXJlTJ06FfPmzUNoaKjm+Ny5cxEeHo5ChQqhcuXK6ZbPVq1aYfjw4Rg0aBAqVaqEI0eOYMKECWZdIzg4GL/99hv++OMPVKhQAUuXLtXUeKlHU3/55ZeoUqUKQkND0aBBAwQEBMgmKU0vn3/+Oezt7VGmTBnkyZMH9+7dS/O1FAoFtm3bhnr16qFnz54oUaIEPvroI9y9exf+/v6wt7dHVFQUunXrhhIlSqBjx45o3ry53jVgDRkyZAhGjBiBkSNHonz58tixY4dmCgp9AgMD8fvvv2Pz5s2oWLEili1bhmnTpqX5OVLmsPm1/DJbdHQ0vLy88KJiRXhrr+WnM+cIZTylUolt27ahRYsWVl9SIKfLyLJ4+/Ytbt++LZsXKDsqXLgwhg0bJpuDKS3Ui0h7enpafamer7/+GsuWLcP9+/etmg9rsaWyyCmMfV5ERUUhd+7cXMvPJrm5AVZevZqIyFYsWbIE1atXh5+fHw4fPozZs2dj0KBB1s4WkU1gQGXMzJnA4MHAiBHWzgkRkdXduHEDU6dOxfPnzxEYGIiRI0di3Lhx1s4WkU1gQGVA0rp1QKlSwJEjANd6IiILZYelQ+bPn2/W7OpEOQkbfQ1Rzx3DYIqIiIhSwYCKiIiIyEIMqIiIiIgsxICKiIiIyEIMqIiIiIgsxICKiIiIyEIMqIiIsimFQoHNmzdbNQ/79u2DQqHAy5cvrZoPW2ELZaJP4cKFsWDBggy9x507d6BQKHD27NkMvY+1MKAiIrLQ0aNHYW9vjw8++MDsczPji8yaatWqhcjISHh5eVk7Kxkqu5djeihUqBAiIyNRrlw5a2clQzCgIiKy0I8//ojBgwfjwIEDePjwobWzY1OcnJwQEBAAhUKh93hSUhJUKlUm54qswd7eHgEBAXBwyJ5zijOgIiKyQExMDNavX4/+/fvjgw8+wMqVK1Ok+euvv1C9enW4uLggd+7caNu2LQCgQYMGuHv3LoYPHw6FQqEJOiZOnIhKlSrJrrFgwQIULlxYs33y5Ek0bdoUuXPnhpeXF+rXr48zZ86YlXeVSoXp06cjODgYrq6uqFixIn777TfNcXVz3e7du1GtWjW4ubmhVq1auHbtGgDg+vXrUCgUuHr1quy68+fPR9GiRWXXUDf5rVy5Et7e3tiyZQvKlCkDZ2dn3Lt3Dy9evEC3bt3g4+MDNzc3NG/eHDdu3NBcU33ezp07Ubp0abi7u6NZs2aIjIzUpOnRowfatGmDadOmwd/fH97e3pg8eTISExMxatQo+Pr6omDBglixYoUsv/fv30fHjh3h7e0NX19ftG7dWjazvfq6c+bMQb58+eDn54eBAwdCqVQCABo1aqS3HA2JjIxE8+bN4erqiiJFishecwAYM2YMSpQoATc3NxQpUgQTJkzQ3AsAzp07h4YNG8LDwwOenp6oWrUqTp06pTl+6NAh1K1bF66urihUqBCGDBmCN2/eaI4/efIELVu2hKurK4KDg7F69Wqj+QWAxMREDBkyBN7e3vDz88OYMWPQvXt3tGnTRpNmx44dqFOnjibNhx9+iIiICM1x3Sa/1N5fWQ0DKiKyPUIAcXHW+RHCrKxu2LABpUqVQsmSJfHJJ5/gp59+gtC6xtatW9G2bVu0aNEC//77L3bv3o0aNWoAAP744w8ULFgQkydPRmRkpCw4SM3r16/RvXt3HDp0CMeOHUPx4sXRokULvH792uRrTJ8+HatWrcKyZctw6dIlDB8+HJ988gn2798vSzd+/HjMnTsXp06dgoODAz799FMAQIkSJVCtWrUUX8irV6/Gxx9/bPC+sbGxmDlzJn744QdcunQJefPmRY8ePXDq1Cls2bIFR48ehRACLVq0kAUSsbGxmDNnDn755RccOHAA9+7dw+effy679p49e/Dw4UMcOHAA8+bNQ1hYGD788EP4+Pjg+PHj6NevH/r27Yv//vsPAKBUKhEaGgoPDw8cPHgQhw8f1gRrCQkJmuvu3bsXERER2Lt3L37++WesXLlSEzz/9ttvZpXjhAkT0L59e5w7dw5dunTBRx99hCtXrmiOe3h4YOXKlbh8+TIWLlyI5cuXy5b86dKlCwoWLIiTJ0/i9OnTGDt2LBwdHQEAERERaNasGdq3b4/z589j/fr1OHTokGwR6x49euD+/fvYu3cvfvvtNyxZsgRPnjwxmueZM2di9erVWLFiBQ4fPozo6OgUfcHevHmDESNG4NSpU9i9ezfs7OzQtm3bVGsgDb2/shxBMq9evRIAxLNnz6ydlRwvISFBbN68WSQkJFg7KzleRpZFXFycuHz5soiLi0veGRsrRNWq1vmJjTUr/7Vq1RILFiwQQgihVCpF7ty5xd69ezXHQ0JCRJcuXQyeHxQUJObPny/bFxYWJipWrCjbN3/+fBEUFCSSkpLEixcvRFJSkux4UlKS8PDwEH/99ZdmHwCxadMmvfd9+/atcHNzE0eOHJHt79Wrl+jcubMQQoi9e/cKAGLXrl2a41u3bhUANOU1f/58UbRoUc3xa9euCQDiypUrsmu8ePFCCCHEihUrBABx9uxZzTnXr18XAMThw4c1+549eyZcXV3Fhg0bZOfdvHlTk2bx4sXC399fs929e3fNa6RWsmRJUbduXc12YmKiyJUrl1i7dq0QQohffvlFlCxZUqhUKk2a+Ph44erqKnbu3Cm7bmJioiZNhw4dRMeOHTVloa8c9QEg+vXrJ9tXs2ZN0b9/f4PnzJ49W1StWlWz7eHhIVauXKk3ba9evUSfPn1k+w4ePCjs7OxEXFycpnxOnDihOX7lyhUBwGj+/f39xezZszXbiYmJIjAwULRu3drgOU+fPhUAxIULF4QQQty+fVsAEP/++68QwrT3ly69nxfvPHv2TAAQr169MpinjMQaKiKiNLp27RpOnDiBzp07AwAcHBzQqVMn/Pjjj5o0Z8+eRePGjdP93o8fP0bv3r1RvHhxeHl5wdPTEzExMbh3755J59+8eROxsbFo2rQp3N3dNT+rVq2SNdMAQIUKFTSP8+XLBwCaGo2PPvoId+7cwbFjxwBItVNVqlRBqVKlDN7byclJds0rV67AwcEBNWvW1Ozz8/NDyZIlZTU3bm5umqZEdV50a1bKli0LO7vkrzZ/f3+UL19es21vbw8/Pz/NeefOncPNmzfh4eGheQ18fX3x9u1b2etQtmxZ2Nvby+799OlTg8/RmJCQkBTb2s9z/fr1qF27NgICAuDu7o4vv/xSVq4jRozAZ599hiZNmmDGjBmyfJ47dw4rV66UlWloaChUKhVu376tea2rVq2qOadUqVLw9vY2mN9Xr17h8ePHmppVQHodta8BADdu3EDnzp1RpEgReHp6apqoU3tPGnt/ZSXZs2cYEWVtLi7AwYPWu7eJfvzxRyQmJiJ//vyafUIIODs749tvv4WXlxdcXV3NzoKdnZ2s2RCArOkLkJptnj9/joULFyIoKAjOzs4ICQmRNVMZExMTA0BqkixQoIDsmLOzs2xb3ZwEQNM/SN2MExAQgEaNGmHNmjV47733sGbNGvTv39/ovV1dXVPtZ6SPdj7UedF9nfSl0bdPnf+YmBhUrVpVbz+iPHnyGL1uRnSmP3r0KLp06YJJkyYhNDQUXl5eWLduHebOnatJM3HiRHz88cfYunUrtm/fjrCwMKxbtw5t27ZFTEwM+vbtiyFDhqS4dmBgIK5fv57ueVZr2bIlgoKCsHz5cuTPnx8qlQrlypVL9T1p7P2VlTCgIiLbo1AAaQhEMlNiYiJWrVqFuXPn4v3335cda9OmDdauXYt+/fqhQoUK2L17N3r27Kn3Ok5OTkhKSpLty5MnDx49egQhhOYLRnfuniNHjmDJkiVo0aIFAKlj9bNnz0zOv3aH8Pr165t8nj5dunTB6NGj0blzZ9y6dQsfffSRWeeXLl0aiYmJOH78OGrVqgUAiIqKwrVr11CmTBmL8paaKlWqYP369cibNy88PT3TfB195WjIsWPH0K1bN9l25cqVAUjlGhQUhPHjx2uO3717N8U1SpQogRIlSmD48OHo3LkzVqxYgbZt26JKlSq4fPkyihUrpvfepUqVQmJiIk6fPo3q1asDkGpajc0T5uXlBX9/f5w8eRL16tUDII3OPHPmjGbwhLq8li9fjrp16wKQOsfnJGzyIyJKg7///hsvXrxAr169UK5cOdlP+/btNc1+YWFhWLt2LcLCwnDlyhVcuHABM2fO1FyncOHCOHDgAB48eKAJiBo0aICnT59i1qxZiIiIwOLFi7F9+3bZ/YsXL45ffvkFV65cwfHjx9GlSxezasM8PDzw+eefY/jw4fj5558RERGBM2fO4JtvvsHPP/9s1mvRrl07vH79Gv3790fDhg1lNXamKF68OFq3bo3evXvj0KFDOHfuHD755BMUKFAArVu3Nuta5urSpQty586N1q1b4+DBg7h9+zb27duHIUOGaDqum0JfORqyceNG/PTTT7h+/TrCwsJw4sQJTafx4sWL4969e1i3bh0iIiKwaNEibNq0SXNuXFwcBg0ahH379uHu3bs4fPgwTp48idKlSwOQRggeOXIEgwYNwtmzZ3Hjxg38+eefmuuXLFkSzZo1Q9++fXH8+HGcPn0an332WarvncGDB2P69On4888/ce3aNQwdOhQvXrzQBPw+Pj7w8/PD999/j5s3b2LPnj0YMWKEya9fdsCAiogoDX788Uc0adJE74SV7du3x6lTp3D+/Hk0aNAAGzduxJYtW1CpUiU0atQIJ06c0KSdPHky7ty5g6JFi2qamEqXLo0lS5Zg8eLFqFixIk6cOJFiNNvy5cvx4sULVKlSBV27dsWQIUOQN29es57DlClTMGHCBEyfPh2lS5dGs2bNsHXrVgQHB5t1HQ8PD7Rs2VIzai0tVqxYgapVq+LDDz9ESEgIhBDYtm1biqa29Obm5oYDBw4gMDAQ7dq1Q+nSpdGrVy+8ffvWrBorfeVoyKRJk7Bu3TpUqFABq1atwtq1azU1ca1atcLw4cMxaNAgVKpUCUeOHMGECRM059rb2yMqKgrdunVDiRIl0LFjRzRv3hyTJk0CIPVH2r9/P65fv466deuicuXK+Oqrr2RB7ooVK5A/f37Ur18f7dq1Q58+fVJ974wZMwadO3dGt27dEBISoumb5fKuidzOzg7r1q3D6dOnUa5cOQwfPhyzZ882+fXLDhRCtwE6h4uOjoaXlxeePXsGPz8/a2cnR1Mqldi2bRtatGiR4R+qZFxGlsXbt29x+/ZtBAcHaz6cyTCVSoXo6Gh4enrKOl9T5svJZaFSqVC6dGl07NgRU6ZMybT7Gvu8iIqKQu7cufHq1SuLmm/Tin2oiIiIyKi7d+/in3/+Qf369REfH49vv/0Wt2/fNjrfWE6Ts0JqIiIiMpudnR1WrlyJ6tWro3bt2rhw4QJ27dql6btFrKEiIiKiVBQqVAiHDx+2djZsGmuoiIiIiCzEgIqIiIjIQgyoiMgmcMAxEaXGlj8nskxA9fXXX6NWrVpwc3MzuObQvXv38MEHH8DNzQ158+bFqFGjkJiYmLkZJSKzqKdhiI2NtXJOiMjWqT8nbHEqnSzTKT0hIQEdOnRASEiIbOFRtaSkJHzwwQcICAjAkSNHEBkZiW7dusHR0RHTpk2zQo6JyBT29vbw9vbWLIbq5uaWpnXecgqVSoWEhAS8ffs2x819ZGtYFplHCIHY2Fg8efIE3t7esoWqbUWWCajUs8CuXLlS7/F//vkHly9fxq5du+Dv749KlSphypQpGDNmDCZOnAgnJ6dMzC0RmSMgIABA1lxhPrMJIRAXF5fmBYYp/bAsMp+3t7fm88LWZJmAKjVHjx5F+fLl4e/vr9kXGhqK/v3749KlS5qFJ3XFx8cjPj5esx0dHQ1Amhlad3V3ylzq15/lYH2ZURa5c+eGj48PEhMTbbqfhLUlJibiyJEjqFWrFhwcss1HeJbEssg8CoUCDg4OsLe3N9iVx9rfFdnmHfDo0SNZMAVAs/3o0SOD502fPl1T+6Vt7969cHNzS99MUpqEh4dbOwv0DsvCdhw4cMDaWaB3WBa2wdr9MK0aUI0dO1a26ro+V65cQalSpTIsD+PGjZOtiB0dHY1ChQqhYcOGXMvPypRKJcLDw9G0aVOb7ICYk7AsbAfLwnawLGxLVFSUVe9v1YBq5MiR6NGjh9E0RYoUMelaAQEBshXcAeDx48eaY4Y4OzvD2dk5xX5HR0f+gdgIloXtYFnYDpaF7WBZ2AZrl4FVA6o8efIgT5486XKtkJAQfP3113jy5Any5s0LQGqe8PT0RJkyZdLlHkRERET6ZJk+VPfu3cPz589x7949JCUl4ezZswCAYsWKwd3dHe+//z7KlCmDrl27YtasWXj06BG+/PJLDBw4UG8NlCHqzrCvX7+2erSb0ymVSsTGxiI6OpplYWUsC9vBsrAdLAvb8vr1awBWnPxTZBHdu3cXAFL87N27V5Pmzp07onnz5sLV1VXkzp1bjBw5UiiVSrPuExERofc+/OEPf/jDH/7wx/Z/IiIi0jkCMY1CCI5P1vby5Uv4+Pjg3r178PLysnZ2cjT1AIH79+/D09PT2tnJ0VgWtoNlYTtYFrbl1atXCAwMxIsXLwyuqJKRskyTX2ZRz3br5eXFPxAb4enpybKwESwL28GysB0sC9tirVnrOVc+ERERkYUYUBERERFZiAGVDmdnZ4SFhZk1MpAyBsvCdrAsbAfLwnawLGyLtcuDndKJiIiILMQaKiIiIiILMaAiIiIishADKiIiIiILMaAiIiIishADKi2LFy9G4cKF4eLigpo1a+LEiRPWzlKWNn36dFSvXh0eHh7Imzcv2rRpg2vXrsnSvH37FgMHDoSfnx/c3d3Rvn17PH78WJbm3r17+OCDD+Dm5oa8efNi1KhRSExMlKXZt28fqlSpAmdnZxQrVgwrV67M6KeXpc2YMQMKhQLDhg3T7GNZZK4HDx7gk08+gZ+fH1xdXVG+fHmcOnVKc1wIga+++gr58uWDq6srmjRpghs3bsiu8fz5c3Tp0gWenp7w9vZGr169EBMTI0tz/vx51K1bFy4uLihUqBBmzZqVKc8vq0hKSsKECRMQHBwMV1dXFC1aFFOmTJGtB8eyyBgHDhxAy5YtkT9/figUCmzevFl2PDNf940bN6JUqVJwcXFB+fLlsW3bNvOfkFUWvLFB69atE05OTuKnn34Sly5dEr179xbe3t7i8ePH1s5alhUaGipWrFghLl68KM6ePStatGghAgMDRUxMjCZNv379RKFChcTu3bvFqVOnxHvvvSdq1aqlOZ6YmCjKlSsnmjRpIv7991+xbds2kTt3bjFu3DhNmlu3bgk3NzcxYsQIcfnyZfHNN98Ie3t7sWPHjkx9vlnFiRMnROHChUWFChXE0KFDNftZFpnn+fPnIigoSPTo0UMcP35c3Lp1S+zcuVPcvHlTk2bGjBnCy8tLbN68WZw7d060atVKBAcHi7i4OE2aZs2aiYoVK4pjx46JgwcPimLFionOnTtrjr969Ur4+/uLLl26iIsXL4q1a9cKV1dX8d1332Xq87VlX3/9tfDz8xN///23uH37tti4caNwd3cXCxcu1KRhWWSMbdu2ifHjx4s//vhDABCbNm2SHc+s1/3w4cPC3t5ezJo1S1y+fFl8+eWXwtHRUVy4cMGs58OA6p0aNWqIgQMHaraTkpJE/vz5xfTp062Yq+zlyZMnAoDYv3+/EEKIly9fCkdHR7Fx40ZNmitXrggA4ujRo0II6Q/Ozs5OPHr0SJNm6dKlwtPTU8THxwshhBg9erQoW7as7F6dOnUSoaGhGf2UspzXr1+L4sWLi/DwcFG/fn1NQMWyyFxjxowRderUMXhcpVKJgIAAMXv2bM2+ly9fCmdnZ7F27VohhBCXL18WAMTJkyc1abZv3y4UCoV48OCBEEKIJUuWCB8fH035qO9dsmTJ9H5KWdYHH3wgPv30U9m+du3aiS5dugghWBaZRTegyszXvWPHjuKDDz6Q5admzZqib9++Zj0HNvkBSEhIwOnTp9GkSRPNPjs7OzRp0gRHjx61Ys6yl1evXgEAfH19AQCnT5+GUqmUve6lSpVCYGCg5nU/evQoypcvD39/f02a0NBQREdH49KlS5o02tdQp2HZpTRw4EB88MEHKV4vlkXm2rJlC6pVq4YOHTogb968qFy5MpYvX645fvv2bTx69Ej2Wnp5eaFmzZqy8vD29ka1atU0aZo0aQI7OzscP35ck6ZevXpwcnLSpAkNDcW1a9fw4sWLjH6aWUKtWrWwe/duXL9+HQBw7tw5HDp0CM2bNwfAsrCWzHzd0+tziwEVgGfPniEpKUn2RQEA/v7+ePTokZVylb2oVCoMGzYMtWvXRrly5QAAjx49gpOTU4pVwbVf90ePHuktF/UxY2mio6MRFxeXEU8nS1q3bh3OnDmD6dOnpzjGsshct27dwtKlS1G8eHHs3LkT/fv3x5AhQ/Dzzz8DSH49jX0mPXr0CHnz5pUdd3BwgK+vr1llltONHTsWH330EUqVKgVHR0dUrlwZw4YNQ5cuXQCwLKwlM193Q2nMLRcHs1ITpdHAgQNx8eJFHDp0yNpZyZHu37+PoUOHIjw8HC4uLtbOTo6nUqlQrVo1TJs2DQBQuXJlXLx4EcuWLUP37t2tnLucZcOGDVi9ejXWrFmDsmXL4uzZsxg2bBjy58/PsiCzsIYKQO7cuWFvb59iRNPjx48REBBgpVxlH4MGDcLff/+NvXv3omDBgpr9AQEBSEhIwMuXL2XptV/3gIAAveWiPmYsjaenJ1xdXdP76WRJp0+fxpMnT1ClShU4ODjAwcEB+/fvx6JFi+Dg4AB/f3+WRSbKly8fypQpI9tXunRp3Lt3D0Dy62nsMykgIABPnjyRHU9MTMTz58/NKrOcbtSoUZpaqvLly6Nr164YPny4piaXZWEdmfm6G0pjbrkwoALg5OSEqlWrYvfu3Zp9KpUKu3fvRkhIiBVzlrUJITBo0CBs2rQJe/bsQXBwsOx41apV4ejoKHvdr127hnv37mle95CQEFy4cEH2RxMeHg5PT0/NF1JISIjsGuo0LLtkjRs3xoULF3D27FnNT7Vq1dClSxfNY5ZF5qldu3aKKUSuX7+OoKAgAEBwcDACAgJkr2V0dDSOHz8uK4+XL1/i9OnTmjR79uyBSqVCzZo1NWkOHDgApVKpSRMeHo6SJUvCx8cnw55fVhIbGws7O/lXob29PVQqFQCWhbVk5uuebp9bZnVhz8bWrVsnnJ2dxcqVK8Xly5dFnz59hLe3t2xEE5mnf//+wsvLS+zbt09ERkZqfmJjYzVp+vXrJwIDA8WePXvEqVOnREhIiAgJCdEcVw/Vf//998XZs2fFjh07RJ48efQO1R81apS4cuWKWLx4MYfqm0B7lJ8QLIvMdOLECeHg4CC+/vprcePGDbF69Wrh5uYmfv31V02aGTNmCG9vb/Hnn3+K8+fPi9atW+sdMl65cmVx/PhxcejQIVG8eHHZkPGXL18Kf39/0bVrV3Hx4kWxbt064ebmlqOH6uvq3r27KFCggGbahD/++EPkzp1bjB49WpOGZZExXr9+Lf7991/x77//CgBi3rx54t9//xV3794VQmTe63748GHh4OAg5syZI65cuSLCwsI4bYKlvvnmGxEYGCicnJxEjRo1xLFjx6ydpSwNgN6fFStWaNLExcWJAQMGCB8fH+Hm5ibatm0rIiMjZde5c+eOaN68uXB1dRW5c+cWI0eOFEqlUpZm7969olKlSsLJyUkUKVJEdg/STzegYllkrr/++kuUK1dOODs7i1KlSonvv/9edlylUokJEyYIf39/4ezsLBo3biyuXbsmSxMVFSU6d+4s3N3dhaenp+jZs6d4/fq1LM25c+dEnTp1hLOzsyhQoICYMWNGhj+3rCQ6OloMHTpUBAYGChcXF1GkSBExfvx42TB7lkXG2Lt3r97viO7duwshMvd137BhgyhRooRwcnISZcuWFVu3bjX7+SiE0JoOloiIiIjMxj5URERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVEGerOnTtQKBQ4e/astbOicfXqVbz33ntwcXFBpUqV9KZp0KABhg0blqn5MoVCocDmzZutnQ0i0sGAiiib69GjBxQKBWbMmCHbv3nzZigUCivlyrrCwsKQK1cuXLt2LcUaXmp//PEHpkyZotkuXLgwFixYkEk5BCZOnKg32IuMjETz5s0zLR9EZBoGVEQ5gIuLC2bOnIkXL15YOyvpJiEhIc3nRkREoE6dOggKCoKfn5/eNL6+vvDw8EjzPQyxJN8AEBAQAGdn53TKDRGlFwZURDlAkyZNEBAQgOnTpxtMo69GZMGCBShcuLBmu0ePHmjTpg2mTZsGf39/eHt7Y/LkyUhMTMSoUaPg6+uLggULYsWKFSmuf/XqVdSqVQsuLi4oV64c9u/fLzt+8eJFNG/eHO7u7vD390fXrl3x7NkzzfEGDRpg0KBBGDZsGHLnzo3Q0FC9z0OlUmHy5MkoWLAgnJ2dUalSJezYsUNzXKFQ4PTp05g8eTIUCgUmTpyo9zraTX4NGjTA3bt3MXz4cCgUClnN3qFDh1C3bl24urqiUKFCGDJkCN68eaM5XrhwYUyZMgXdunWDp6cn+vTpAwAYM2YMSpQoATc3NxQpUgQTJkyAUqkEAKxcuRKTJk3CuXPnNPdbuXKlJv/aTX4XLlxAo0aN4OrqCj8/P/Tp0wcxMTEpymzOnDnIly8f/Pz8MHDgQM29iCh9MKAiygHs7e0xbdo0fPPNN/jvv/8sutaePXvw8OFDHDhwAPPmzUNYWBg+/PBD+Pj44Pjx4+jXrx/69u2b4j6jRo3CyJEj8e+//yIkJAQtW7ZEVFQUAODly5do1KgRKleujFOnTmHHjh14/PgxOnbsKLvGzz//DCcnJxw+fBjLli3Tm7+FCxdi7ty5mDNnDs6fP4/Q0FC0atUKN27cACA1mZUtWxYjR45EZGQkPv/881Sf8x9//IGCBQti8uTJiIyMRGRkJACppqtZs2Zo3749zp8/j/Xr1+PQoUMYNGiQ7Pw5c+agYsWK+PfffzFhwgQAgIeHB1auXInLly9j4cKFWL58OebPnw8A6NSpE0aOHImyZctq7tepU6cU+Xrz5g1CQ0Ph4+ODkydPYuPGjdi1a1eK++/duxcRERHYu3cvfv75Z6xcuVIToBFROjF7OWUiylK6d+8uWrduLYQQ4r333hOffvqpEEKITZs2Ce2PgLCwMFGxYkXZufPnzxdBQUGyawUFBYmkpCTNvpIlS4q6detqthMTE0WuXLnE2rVrhRBC3L59WwCQrfCuVCpFwYIFxcyZM4UQQkyZMkW8//77snvfv39fANCsLl+/fn1RuXLlVJ9v/vz5xddffy3bV716dTFgwADNdsWKFUVYWJjR69SvX18MHTpUsx0UFCTmz58vS9OrVy/Rp08f2b6DBw8KOzs7ERcXpzmvTZs2qeZ79uzZomrVqpptfeUhhBAAxKZNm4QQQnz//ffCx8dHxMTEaI5v3bpV2NnZiUePHgkhksssMTFRk6ZDhw6iU6dOqeaJiEznYN1wjogy08yZM9GoUSOTamUMKVu2LOzskiu3/f39Ua5cOc22vb09/Pz88OTJE9l5ISEhmscODg6oVq0arly5AgA4d+4c9u7dC3d39xT3i4iIQIkSJQAAVatWNZq36OhoPHz4ELVr15btr127Ns6dO2fiMzTduXPncP78eaxevVqzTwgBlUqF27dvo3Tp0gCAatWqpTh3/fr1WLRoESIiIhATE4PExER4enqadf8rV66gYsWKyJUrl2Zf7dq1oVKpcO3aNfj7+wOQysze3l6TJl++fLhw4YJZ9yIi4xhQEeUg9erVQ2hoKMaNG4cePXrIjtnZ2UEIIdunr5+No6OjbFuhUOjdp1KpTM5XTEwMWrZsiZkzZ6Y4li9fPs1j7cDBFsTExKBv374YMmRIimOBgYGax7r5Pnr0KLp06YJJkyYhNDQUXl5eWLduHebOnZsh+bS0fIgodQyoiHKYGTNmoFKlSihZsqRsf548efDo0SMIITSdrtNz7qhjx46hXr16AIDExEScPn1a09enSpUq+P3331G4cGE4OKT9Y8nT0xP58+fH4cOHUb9+fc3+w4cPo0aNGhbl38nJCUlJSbJ9VapUweXLl1GsWDGzrnXkyBEEBQVh/Pjxmn13795N9X66SpcujZUrV+LNmzeaoO3w4cOws7NLUb5ElLHYKZ0ohylfvjy6dOmCRYsWyfY3aNAAT58+xaxZsxAREYHFixdj+/bt6XbfxYsXY9OmTbh69SoGDhyIFy9e4NNPPwUADBw4EM+fP0fnzp1x8uRJREREYOfOnejZs2eqQYWuUaNGYebMmVi/fj2uXbuGsWPH4uzZsxg6dKhF+S9cuDAOHDiABw8eaEYfjhkzBkeOHMGgQYNw9uxZ3LhxA3/++WeKTuG6ihcvjnv37mHdunWIiIjAokWLsGnTphT3u337Ns6ePYtnz54hPj4+xXW6dOkCFxcXdO/eHRcvXsTevXsxePBgdO3aVdPcR0SZgwEVUQ40efLkFE0+pUuXxpIlS7B48WJUrFgRJ06csKivla4ZM2ZgxowZqFixIg4dOoQtW7Ygd+7cAKCpVUpKSsL777+P8uXLY9iwYfD29pb11zLFkCFDMGLECIwcORLly5fHjh07sGXLFhQvXtyi/E+ePBl37txB0aJFkSdPHgBAhQoVsH//fly/fh1169ZF5cqV8dVXXyF//vxGr9WqVSsMHz4cgwYNQqVKlXDkyBHN6D+19u3bo1mzZmjYsCHy5MmDtWvXpriOm5sbdu7ciefPn6N69er43//+h8aNG+Pbb7+16LkSkfkUQrfTBBERERGZhTVURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkIQZURERERBZiQEVERERkof8D4cXhQc+hc8UAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlQAAAHHCAYAAAB5gsZZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACYoElEQVR4nOzdd1gUx/8H8PcBR+9FQAXB3hULiL1jr9GYGFuMGnuvsZtYYyz5WqJJLLFrbDE2bNiw997FKFiQItIObn9/8LuV4woHB9wh79fz8Dy3s7O7czsH92FmdkYiCIIAIiIiIso2E0MXgIiIiCi/Y0BFREREpCcGVERERER6YkBFREREpCcGVERERER6YkBFREREpCcGVERERER6YkBFREREpCcGVERERER6YkBF+dr06dMhkUiydezatWshkUjw7NmzHCvPs2fPIJFIsHbt2hw7Z364dnacOHECEokEJ06cyPKxxv5e9flcUtZIJBJMnz5d3Fb3e92wYUM0bNhQ3Db2zw/lTwyoyGgo/hAqfiwtLVG4cGEEBQVh6dKl+PDhQ66XYfny5bn+R9bHx0fpfWr6MYY/9nlxP8jw7t69ixYtWsDW1hbOzs7o0aMH3r59q9Oxmj7P33//fS6Xmsi4SLiWHxmLtWvXok+fPpg5cyZ8fX0hk8kQERGBEydOIDg4GN7e3ti7dy8qV64sHpOSkoKUlBRYWlpm+XqpqamQyWSwsLAQWxMqVqwIV1fXbLWaAGn/+fr6+mLNmjXo3bu32jy7d+9GXFycuL1//35s3rwZixYtgqurq5heu3ZtFC9eXOdrC4KApKQkSKVSmJqaZqv8Gel7P7SRy+VITk6Gubk5TEyy9r9dbrzXnKTP5zKv/ffff/Dz84ODgwOGDRuGuLg4/Pzzz/D29saFCxdgbm6u9XgfHx84OTlh9OjRSumlS5eGv79/bhYdAJCYmAgzMzOYmZkB+PR35OnTp/Dx8QEAsXVK8Tk29s8P5U9mhi4AUUYtW7ZEjRo1xO2JEyfi2LFjaNOmDdq1a4e7d+/CysoKAJT+kGaVqampQf6YdujQQWk7IiICmzdvRocOHcQvgOxQtOoZysePH2FjY6NzfhMTk2yX19DvNTP6fC7z2uzZs/Hx40dcvnwZ3t7eAAB/f380a9YMa9euRf/+/TM9R5EiRfDNN9/kdlHVys7nwNg/P5Q/scuP8oXGjRtjypQpeP78OTZs2CCmqxurkpCQgGHDhsHV1RV2dnZo164dXr58melYCx8fH9y+fRshISFit4XiP9v3799jzJgxqFSpEmxtbWFvb4+WLVvi+vXrOf5eR40aBRcXF6RvPB46dCgkEgmWLl0qpr1+/RoSiQQrVqwAoH5cSO/evWFra4uXL1+iQ4cOsLW1hZubG8aMGYPU1FSt5dB2PxT3LiQkBIMGDUKhQoVQtGhRAMDz588xaNAglClTBlZWVnBxcUGXLl1UxqqpG0PVsGFDVKxYEXfu3EGjRo1gbW2NIkWKYP78+UrH6vteIyMj0aNHD9jb28PR0RG9evXC9evXdepqlclkmDFjBkqVKgVLS0u4uLigbt26CA4OFvNk/Fz27t1bY9du+s9kUlISpk2bhpIlS8LCwgJeXl4YN24ckpKStJZJH3///TfatGkjBlMA0LRpU5QuXRrbtm3T+TzJycn4+PFjlq6t+BydPn0aw4YNg5ubGxwdHTFgwAAkJycjOjoaPXv2hJOTE5ycnDBu3Dhk7FTJeA91oWkM1bFjx1CvXj3Y2NjA0dER7du3x927d5XyKOr20aNH6N27NxwdHeHg4IA+ffogPj4+S+Wgz0v++BeKCECPHj0wadIkHD58GP369dOYr3fv3ti2bRt69OiBWrVqISQkBK1bt870/IsXL8bQoUNha2uLH374AQDg7u4OAHjy5Al2796NLl26wNfXF69fv8Zvv/2GBg0a4M6dOyhcuHDOvEkA9erVw6JFi3D79m1UrFgRAHDq1CmYmJjg1KlTGDZsmJgGAPXr19d6vtTUVAQFBSEgIAA///wzjhw5goULF6JEiRIYOHCgxuO03Q+FQYMGwc3NDVOnThW/TC9evIizZ8+iW7duKFq0KJ49e4YVK1agYcOGuHPnDqytrbWWNyoqCi1atECnTp3QtWtX7NixA+PHj0elSpXQsmVLvd+rXC5H27ZtceHCBQwcOBBly5bFnj170KtXL63nVpg+fTrmzJmD7777Dv7+/oiNjcWlS5dw5coVNGvWTO0xAwYMQNOmTZXSDh48iI0bN6JQoUJiudq1a4fTp0+jf//+KFeuHG7evIlFixbhwYMH2L17t9ZyxcfH6/SFbmpqCicnJwDAy5cv8ebNG6UWYQV/f3/s378/0/MBaYGItbU1UlNTUaxYMYwcORLDhw/X6Vgg7R8GDw8PzJgxA+fOncOqVavg6OiIs2fPwtvbG7Nnz8b+/fuxYMECVKxYET179tT53Lo6cuQIWrZsieLFi2P69OlISEjAr7/+ijp16uDKlSsqrcddu3aFr68v5syZgytXruD3339HoUKFMG/evBwvG+UTApGRWLNmjQBAuHjxosY8Dg4Ogp+fn7g9bdo0If3H+PLlywIAYcSIEUrH9e7dWwAgTJs2TeV6T58+FdMqVKggNGjQQOW6iYmJQmpqqlLa06dPBQsLC2HmzJlKaQCENWvWZPJuP1mwYIFSOd68eSMAEJYvXy4IgiBER0cLJiYmQpcuXQR3d3fxuGHDhgnOzs6CXC7XeO1evXoJAJTKKAiC4OfnJ1SvXj3Tsmm6H4p7V7duXSElJUVpX3x8vEr+0NBQAYCwfv16Me348eMCAOH48eNiWoMGDVTyJSUlCR4eHkLnzp3FNH3e699//y0AEBYvXiympaamCo0bN9ap7qpUqSK0bt1aa56Mn8uMHj58KDg4OAjNmjUT799ff/0lmJiYCKdOnVLKu3LlSgGAcObMGZ2umdlPsWLFxGMuXryocr8Vxo4dKwAQEhMTtV63bdu2wrx584Tdu3cLf/zxh1CvXj0BgDBu3DitxwnCp89RUFCQ+DkWBEEIDAwUJBKJ8P3334tpKSkpQtGiRVU+j7r8Xjdo0EDpOHWfn6pVqwqFChUSIiMjxbTr168LJiYmQs+ePcU0xX3+9ttvlcrRsWNHwcXFJdP3TJ8vdvlRvmJra6v1ab+DBw8CSGs5SW/o0KF6XdfCwkIcOJ2amorIyEjY2tqiTJkyuHLlil7nzsjNzQ1ly5bFyZMnAQBnzpyBqakpxo4di9evX+Phw4cA0lqo6tatq9Pj+RmfuKpXrx6ePHmid1n79eunMg5NMb4NSOsei4yMRMmSJeHo6KjTvbK1tVUaj2Nubg5/f3+dy5vZez148CCkUqlSK6eJiQkGDx6s0/kdHR1x+/ZtsR6y6uPHj+jYsSOcnJywefNm8f5t374d5cqVQ9myZfHu3Tvxp3HjxgCA48ePaz1vz549ERwcnOnPxo0bxWMSEhIApH2+M1KMMVLk0WTv3r0YN24c2rdvj2+//RYhISEICgrCL7/8gv/++0+ne9K3b1+lz3FAQAAEQUDfvn3FNFNTU9SoUSNHPrcZhYeH49q1a+jduzecnZ3F9MqVK6NZs2ZqW+rUfc4iIyMRGxub4+Wj/IFdfpSvxMXFiV0k6jx//hwmJibw9fVVSi9ZsqRe15XL5ViyZAmWL1+Op0+fKo3JcXFx0evc6tSrV0/8I37q1CnUqFEDNWrUgLOzM06dOgV3d3dcv34dX3/9dabnsrS0hJubm1Kak5MToqKi9C5nxvsMpH0Bz5kzB2vWrMHLly+VxrzExMRkes6iRYuqBIlOTk64ceNGpsfq8l6fP38OT09Pla5HXT8jM2fORPv27VG6dGlUrFgRLVq0QI8ePZSePtWmX79+ePz4Mc6ePav02Xn48CHu3r2rUn6FN2/eaD1v8eLFs/RUKPAp+FU3RisxMVEpj64kEglGjhyJQ4cO4cSJEzoNVk8/fgsAHBwcAABeXl4q6Tnxuc3o+fPnAIAyZcqo7CtXrhwOHTqk8tBFxjIrulGjoqJgb2+f42Uk48eAivKN//77DzExMXoHR9kxe/ZsTJkyBd9++y1mzZoFZ2dnmJiYYMSIEZDL5Tl+vbp162L16tV48uQJTp06hXr16kEikaBu3bo4deoUChcuDLlcjnr16mV6rtx8klHdl+3QoUOxZs0ajBgxAoGBgXBwcIBEIkG3bt10uleayivoMMNLXjy1Wb9+fTx+/Bh79uzB4cOH8fvvv2PRokVYuXIlvvvuO63HLlmyBJs3b8aGDRtQtWpVpX1yuRyVKlXCL7/8ovbYjMFFRnFxcUrTcWhiamoqBm2enp4A0lpoMgoPD4ezs7Pa1qvMKMr6/v17nfJrqjd16bp8DvKCPp9T+jwxoKJ846+//gIABAUFacxTrFgxyOVyPH36FKVKlRLTHz16pNM1NHWf7dixA40aNcIff/yhlB4dHa00d1ROUQRKwcHBuHjxIiZMmAAg7ct8xYoVKFy4MGxsbFC9evUcv3Z62Znte8eOHejVqxcWLlwopiUmJiI6OjoHS5Z9xYoVw/HjxxEfH6/USqXrZwQAnJ2d0adPH/Tp0wdxcXGoX78+pk+frjWgOnXqFMaMGYMRI0age/fuKvtLlCiB69evo0mTJtm67z///DNmzJiRab5ixYqJT1wWKVIEbm5uuHTpkkq+CxcuqAR9ulJ0y2lqbTM2xYoVAwDcv39fZd+9e/fg6uqapSlBqGDiGCrKF44dO4ZZs2bB19dX7ZeRgiLYWr58uVL6r7/+qtN1bGxs1H7xm5qaqvznuX37drx8+VKn82aVr68vihQpgkWLFkEmk6FOnToA0gKtx48fY8eOHahVq1auz3Wk6X5oo+5e/frrr5lO05BXgoKCIJPJsHr1ajFNLpdj2bJlOh0fGRmptG1ra4uSJUtqndogPDwcXbt2Rd26dbFgwQK1ebp27YqXL18qlUshISEh0ykJsjOGCgA6d+6Mffv24cWLF2La0aNH8eDBA3Tp0kVMk8lkuHfvnlJr1vv371XqVSaTYe7cuTA3N0ejRo20ltlYeHp6omrVqli3bp3S5/3WrVs4fPgwWrVqZbjCUb7BFioyOgcOHMC9e/eQkpKC169f49ixYwgODkaxYsWwd+9erRPyVa9eHZ07d8bixYsRGRkpTpvw4MEDAJm3uFSvXh0rVqzAjz/+iJIlS6JQoUJo3Lgx2rRpg5kzZ6JPnz6oXbs2bt68iY0bN2Z5zEpW1KtXD1u2bEGlSpXE8RnVqlWDjY0NHjx4oNP4KX1puh/atGnTBn/99RccHBxQvnx5hIaG4siRI7ky1iw7OnToAH9/f4wePRqPHj1C2bJlsXfvXrF7KrPPSPny5dGwYUNUr14dzs7OuHTpEnbs2IEhQ4ZoPGbYsGF4+/Ytxo0bhy1btijtq1y5MipXrowePXpg27Zt+P7773H8+HHUqVMHqampuHfvHrZt24ZDhw6pnd5AITtjqABg0qRJ2L59Oxo1aoThw4cjLi4OCxYsQKVKldCnTx8x38uXL1GuXDn06tVLnL9p7969+PHHH/HFF1/A19cX79+/x6ZNm3Dr1i3Mnj0bHh4eWS6PoSxYsAAtW7ZEYGAg+vbtK06b4ODgkOV5rqhgYkBFRmfq1KkA0p7ucnZ2RqVKlbB48WL06dMHdnZ2mR6/fv16eHh4YPPmzdi1axeaNm2KrVu3okyZMpnOjjx16lQ8f/4c8+fPx4cPH9CgQQM0btwYkyZNwsePH7Fp0yZs3boV1apVw7///it2xeUGRUBVt25dMc3MzAyBgYE4cuSITuOn9KXpfmizZMkSmJqaYuPGjUhMTESdOnVw5MgRrV21ecnU1BT//vsvhg8fjnXr1sHExAQdO3bEtGnTUKdOnUw/I8OGDcPevXtx+PBhJCUloVixYvjxxx8xduxYjce8ffsWqampGDVqlMq+adOmoXLlyjAxMcHu3buxaNEirF+/Hrt27YK1tTWKFy+O4cOHo3Tp0nq/d3W8vLwQEhKCUaNGYcKECTA3N0fr1q2xcOHCTMdPVapUCeXLl8eGDRvw9u1bmJubo2rVqti2bZtS61Z+0LRpUxw8eBDTpk3D1KlTIZVK0aBBA8ybN0/twxdEGXEtPyoQrl27Bj8/P2zYsEFrlyEVXLt370bHjh1x+vRpsYuViEhXHENFnx118+YsXrwYJiYmmc4qTgVDxs9Iamoqfv31V9jb26NatWoGKhUR5Wfs8qPPzvz583H58mU0atQIZmZmOHDgAA4cOID+/ftn+ug5FQxDhw5FQkICAgMDkZSUhJ07d+Ls2bOYPXt2luddIiIC2OVHn6Hg4GDMmDEDd+7cQVxcHLy9vdGjRw/88MMPuf5UHOUPmzZtwsKFC/Ho0SMkJiaiZMmSGDhwoNaB5URE2jCgIiIiItITx1ARERER6YkBFREREZGeOKAkA7lcjlevXsHOzi5byz8QERFR3hMEAR8+fEDhwoVhYpL37UUMqDJ49eoVnwQjIiLKp168eIGiRYvm+XUZUGWgmIn76dOncHZ2NnBpCjaZTIbDhw+jefPmkEqlhi5Ogca6MB6sC+PBujAu79+/h6+vr04rauSGfBNQrVixAitWrBBXSa9QoQKmTp2Kli1bAkhbzX706NHYsmULkpKSEBQUhOXLl8Pd3T1L11F089nZ2cHe3j5H3wNljUwmg7W1Nezt7fnHysBYF8aDdWE8WBfGRSaTAch8Pc7ckm8GpRctWhRz587F5cuXcenSJTRu3Bjt27fH7du3AQAjR47EP//8g+3btyMkJASvXr1Cp06dDFxqIiIiKgjyTQtV27ZtlbZ/+uknrFixAufOnUPRokXxxx9/YNOmTeLCrWvWrEG5cuVw7tw51KpVyxBFJiIiogIi3wRU6aWmpmL79u34+PEjAgMDcfnyZchkMjRt2lTMU7ZsWXh7eyM0NFRrQJWUlISkpCRxOzY2FkBa06Gi+ZAMQ3H/WQ+Gx7owHqwL48G6MC6Grod8FVDdvHkTgYGBSExMhK2tLXbt2oXy5cvj2rVrMDc3h6Ojo1J+d3d3REREaD3nnDlzMGPGDJX048ePw9raOieLT9kUHBxs6CLQ/2NdGA/WhfFgXRiH+Ph4g14/XwVUZcqUwbVr1xATE4MdO3agV69eCAkJ0eucEydOxKhRo8Tt2NhYeHl5oVGjRnBxcdG3yKQHmUyG4OBgNGvWjAM+DYx1YTxYF8aDdWFcIiMjDXr9fBVQmZubo2TJkgCA6tWr4+LFi1iyZAm+/PJLJCcnIzo6WqmV6vXr1/Dw8NB6TgsLC1hYWKikS6VS/oIYCdaF8WBdGA/WhfFgXRgHQ9dBvgqoMpLL5UhKSkL16tUhlUpx9OhRdO7cGQBw//59hIWFITAwMFvnnjt3rtouv2+//RbFihUDAJw7dw4HDhzQeI4ePXqIAeDly5exd+9ejXm//PJLlC9fHkBa1+aOHTs05u3UqROqVKkCALh37x42b96sMW/btm1Ro0YNAMDjx4+xfv16jXmDgoJQu3ZtAGkTo/3+++8a8zZu3BgNGjQAAERERGDFihUa89atWxfNmjUDkPYfxNKlSzXmDQgIQKtWrQAAHz58wObNm3Hx4kWYmpqq5K1WrRrat28PIG3ajDlz5mg8b8WKFdGlSxcAaWPwZs6cqTFvmTJl8PXXX4vbM2fORGpqqtq8xYsXR69evcTtuXPnIiEhQW3eokWLol+/fuL2woULxTF7Gbm7u2PQoEHi9tKlSzX+9+Xs7Izhw4eL2ytXrkR4eLjavLa2thg7dqy4/ccffyAsLExtXgsLC0yaNEncPn78uMa6MDExwbRp08TtLVu24O7du2rPCwBTpkyBmVnan5+///4bN27c0Jh3/Pjx4u/i3r17cfnyZY15R40aBQcHBwDAwYMHERoaqjHv0KFD4erqCgA4evQoTp48qTHvgAEDULhwYQDAqVOncOTIEY158+JvxLNnzzBjxgy1dQEUrL8RP//8s8a8efU34scff9SYtyD9jVi/fj0eP36sNm9e/Y0wKCGfmDBhghASEiI8ffpUuHHjhjBhwgRBIpEIhw8fFgRBEL7//nvB29tbOHbsmHDp0iUhMDBQCAwMzPJ1YmJiBAAaf06dOiXmXbJkida8Bw8eFPOuXr1aa96///5bzLtp0yatef/66y8x7549e7TmXblypZg3ODhYa96FCxeKec+ePas176xZs8S8169f15p3woQJYt6HDx9qzTt06FAx7/Pnz7Xm7du3r8711q1bNzGvTCbTmrdt27ZKnwlzc3ONeZs0aaKU18nJSWPeWrVqKeUtUqSIxryVKlVSylu6dGmNeUuUKKGU18/PT2NeDw8Ppbx16tTRmNfOzk7Ml5ycLFStWlVjXlNTU6XzduzYUes9TkxMFPN2795da97IyEgxb//+/bXmffHihZh35MiRWvPeu3dPzPvDDz9ozXv58mUx7+zZs7Xmze2/EcnJycKoUaO05i0ofyNevXqlNW9u/41ITk4Wdu/ezb8R/6958+Ya8+bF34h3794JAISYmBjBEPJNC9WbN2/Qs2dPhIeHw8HBAZUrV8ahQ4fE/2oWLVoEExMTdO7cWWliz+zq27cvLC0tVdI9PT3F15UrV8bgwYM1niP9Ejbly5fXmrd48eLi61KlSmnNW7p0afG1j4+P1rwVKlQQXxctWlRr3qpVq4qvPTw8tOZV/EcLAC4uLlrzpn/K0sHBQWveevXqia+tra3RqlUrFCtWTO26TOlbH6VSqdbzVq9eXXwtkUi05q1UqZLS9sCBA5GSkqI2b5kyZZS2v/vuO40DI318fJS2e/fujejoaLV5Fa0hCt27d8ebN2/U5lW0sih07dpVbEXIKONktZ06dVKq9/Qyfv79/f0RGBioti4yprVs2VLlPWjK36xZM5UHStJL3yXfsGFDrc36NjY24us6deogOTlZY9701wwICND6mXBzcxNfV69eXWvevPgbUbhwYQwcOFDjemUF6W+Etrx59TdiwIABkMvlavMWpL8R7dq1Q6lSpdTmzau/EYYkEQRBMHQhjElsbCwcHBzw7t07Dko3MJlMhv3796NVq1YG7xsv6FgXxoN1YTxYF8YlMjISrq6uiImJMchKJ/lmpnQiIiIiY8WAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhPDKiIiIiI9MSAioiIiEhP+SagmjNnDmrWrAk7OzsUKlQIHTp0wP3795XyNGzYEBKJROnn+++/N1CJiYiIqKDINwFVSEgIBg8ejHPnziE4OBgymQzNmzfHx48flfL169cP4eHh4s/8+fMNVGIiIiIqKMwMXQBdHTx4UGl77dq1KFSoEC5fvoz69euL6dbW1vDw8Mjr4hEREVEBlm8CqoxiYmIAAM7OzkrpGzduxIYNG+Dh4YG2bdtiypQpsLa21niepKQkJCUliduxsbEAAJlMBplMlgslJ10p7j/rwfBYF8aDdWE8WBfGxdD1IBEEQTBoCbJBLpejXbt2iI6OxunTp8X0VatWoVixYihcuDBu3LiB8ePHw9/fHzt37tR4runTp2PGjBkq6Zs2bdIaiBEREZHxiI+Px9dff42YmBjY29vn+fXzZUA1cOBAHDhwAKdPn0bRokU15jt27BiaNGmCR48eoUSJEmrzqGuh8vLyQnh4OFxcXHK87KQ7mUyG4OBgNGvWDFKp1NDFKdBYF8aDdWE8WBfGJTIyEp6engYLqPJdl9+QIUOwb98+nDx5UmswBQABAQEAoDWgsrCwgIWFhUq6VCrlL4iRYF0YD9aF8WBdGA/WhXEwdB3km4BKEAQMHToUu3btwokTJ+Dr65vpMdeuXQMAeHp65nLpiIiIqCDLNwHV4MGDsWnTJuzZswd2dnaIiIgAADg4OMDKygqPHz/Gpk2b0KpVK7i4uODGjRsYOXIk6tevj8qVKxu49ERERPQ5yzcB1YoVKwCkTd6Z3po1a9C7d2+Ym5vjyJEjWLx4MT5+/AgvLy907twZkydPNkBpiYiIqCDJNwFVZmPnvby8EBISkkelISIiIvok38yUTkRERGSsGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6SnfBFRz5sxBzZo1YWdnh0KFCqFDhw64f/++Up7ExEQMHjwYLi4usLW1RefOnfH69WsDlZiIiIgKinwTUIWEhGDw4ME4d+4cgoODIZPJ0Lx5c3z8+FHMM3LkSPzzzz/Yvn07QkJC8OrVK3Tq1MmApSYiIqKCwMzQBdDVwYMHlbbXrl2LQoUK4fLly6hfvz5iYmLwxx9/YNOmTWjcuDEAYM2aNShXrhzOnTuHWrVqGaLYREREVADkm4Aqo5iYGACAs7MzAODy5cuQyWRo2rSpmKds2bLw9vZGaGioxoAqKSkJSUlJ4nZsbCwAQCaTQSaT5VbxSQeK+896MDzWhfFgXRgP1oVxMXQ95MuASi6XY8SIEahTpw4qVqwIAIiIiIC5uTkcHR2V8rq7uyMiIkLjuebMmYMZM2aopB8/fhzW1tY5Wm7KnuDgYEMXgf4f68J4sC6MB+vCOMTHxxv0+vkyoBo8eDBu3bqF06dP632uiRMnYtSoUeJ2bGwsvLy80KhRI7i4uOh9fso+mUyG4OBgNGvWDFKp1NDFKdBYF8aDdWE8WBfGJTIy0qDXz3cB1ZAhQ7Bv3z6cPHkSRYsWFdM9PDyQnJyM6OhopVaq169fw8PDQ+P5LCwsYGFhoZIulUr5C2IkWBfGg3VhPFgXxoN1YRwMXQf55ik/QRAwZMgQ7Nq1C8eOHYOvr6/S/urVq0MqleLo0aNi2v379xEWFobAwMC8Li4REREVIPmmhWrw4MHYtGkT9uzZAzs7O3FclIODA6ysrODg4IC+ffti1KhRcHZ2hr29PYYOHYrAwEA+4UdERES5Kt8EVCtWrAAANGzYUCl9zZo16N27NwBg0aJFMDExQefOnZGUlISgoCAsX748j0tKREREBU2+CagEQcg0j6WlJZYtW4Zly5blQYmIiIiI0uSbMVRERERExooBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGeGFARERER6YkBFREREZGe8lVAdfLkSbRt2xaFCxeGRCLB7t27lfb37t0bEolE6adFixaGKSwREREVGPkqoPr48SOqVKmCZcuWaczTokULhIeHiz+bN2/OwxISERFRQWRm6AJkRcuWLdGyZUuteSwsLODh4ZFHJSIiIiLKZwGVLk6cOIFChQrByckJjRs3xo8//ggXFxeN+ZOSkpCUlCRux8bGAgBkMhlkMlmul5c0U9x/1oPhsS6MB+vCeLAujIuh60EiCIJg0BJkk0Qiwa5du9ChQwcxbcuWLbC2toavry8eP36MSZMmwdbWFqGhoTA1NVV7nunTp2PGjBkq6Zs2bYK1tXVuFZ+IiIhyUHx8PL7++mvExMTA3t4+z6//WQVUGT158gQlSpTAkSNH0KRJE7V51LVQeXl5ITw8XGvLFuU+mUyG4OBgNGvWDFKp1NDFKdBYF8aDdWE8WBfGJTIyEp6engYLqD67Lr/0ihcvDldXVzx69EhjQGVhYQELCwuVdKlUyl8QI8G6MB6sC+PBujAerAvjYOg6yFdP+WXVf//9J0asRERERLklX7VQxcXF4dGjR+L206dPce3aNTg7O8PZ2RkzZsxA586d4eHhgcePH2PcuHEoWbIkgoKCDFhqIiIi+tzlq4Dq0qVLaNSokbg9atQoAECvXr2wYsUK3LhxA+vWrUN0dDQKFy6M5s2bY9asWWq79IiIiIhySr4KqBo2bAhtY+gPHTqUh6UhIiIiSvNZj6EiIiIiygsMqIiIiIj0xICKiIiISE8MqIiIiIj0xICKiIiISE8MqIiIiIj0xICKiIiISE8MqIiIiIj0xICKiIiISE8MqIiIiIj0xICKiIiISE9ZXsvv7t272LJlC06dOoXnz58jPj4ebm5u8PPzQ1BQEDp37szFiImIiKhA0bmF6sqVK2jatCn8/Pxw+vRpBAQEYMSIEZg1axa++eYbCIKAH374AYULF8a8efOQlJSUm+UmIiIiMho6t1B17twZY8eOxY4dO+Do6KgxX2hoKJYsWYKFCxdi0qRJOVFGIiIiIqOmc0D14MEDSKXSTPMFBgYiMDAQMplMr4IRERER5Rc6d/npEkzpk5+IiIgov8ryU34fPnzA5cuXERcXByBtbFXPnj3RpUsXbNy4MccLSERERGTssvSU38mTJ9GmTRvExcXByckJmzdvxhdffIEiRYrA1NQUO3fuRHx8PPr165db5SUiIiIyOllqoZo8eTK6dOmCFy9eYMSIEfjyyy8xZMgQ3L17F7du3cKMGTOwbNmy3CorERERkVHKUkB148YNjB07FkWKFMH48eMRGxuLL7/8UtzfrVs3PH78OMcLSURERGTMshRQxcbGwtnZGQBgbm4Oa2tr2NnZifvt7OwQHx+fsyUkIiIiMnJZCqgkEgkkEonGbSIiIqKCKEuD0gVBQJMmTWBmlnZYfHw82rZtC3NzcwBASkpKzpeQiIiIyMhlKaCaNm2a0nb79u1V8nTu3Fm/EhERERHlM3oFVERERESUjYk9iYiIiEiZzi1Ufn5+Og9Av3LlSrYLRERERJTf6BxQdejQQXydmJiI5cuXo3z58ggMDAQAnDt3Drdv38agQYNyvJBERERExkzngCr9+KnvvvsOw4YNw6xZs1TyvHjxIudKR0RERJQPZGsM1fbt29GzZ0+V9G+++QZ///233oUiIiIiyk+yFVBZWVnhzJkzKulnzpyBpaWl3oUiIiIiyk+yNG2CwogRIzBw4EBcuXIF/v7+AIDz58/jzz//xJQpU3K0gERERETGLlsB1YQJE1C8eHEsWbIEGzZsAACUK1cOa9asQdeuXXO0gERERETGLtvzUHXt2hVnzpzB+/fv8f79e5w5cybXg6mTJ0+ibdu2KFy4MCQSCXbv3q20XxAETJ06FZ6enrCyskLTpk3x8OHDXC0TERERkc4BlSAIuVkOnXz8+BFVqlTBsmXL1O6fP38+li5dipUrV+L8+fOwsbFBUFAQEhMT87ikREREVJDoHFBVqFABW7ZsQXJystZ8Dx8+xMCBAzF37ly9C5dRy5Yt8eOPP6Jjx44q+wRBwOLFizF58mS0b98elStXxvr16/Hq1SuVliwiIjJeYTFhkKXKDF0MoizReQzVr7/+ivHjx2PQoEFo1qwZatSogcKFC8PS0hJRUVG4c+cOTp8+jdu3b2PIkCEYOHBgbpZbxdOnTxEREYGmTZuKaQ4ODggICEBoaCi6deuWp+UhIqKsC3kWgtGHR6Oud10sbrHY0MUh0pnOAVWTJk1w6dIlnD59Glu3bsXGjRvx/PlzJCQkwNXVFX5+fujZsye6d+8OJyen3CyzWhEREQAAd3d3pXR3d3dxnzpJSUlISkoSt2NjYwEAMpkMMhn/QzIkxf1nPRge68J4fO518df1vyAIAk49P2X07/Fzr4v8xtD1kOWn/OrWrYu6devmRlkMYs6cOZgxY4ZK+vHjx2FtbW2AElFGwcHBhi4C/T/WhfH4XOsi7EUYYuJjAAD79+83cGl087nWRX4THx9v0Otna9oEY+Th4QEAeP36NTw9PcX0169fo2rVqhqPmzhxIkaNGiVux8bGwsvLC40aNYKLi0uulZcyJ5PJEBwcjGbNmkEqlRq6OAUa68J4fO51seffPYh8HQkA2Ji0EY+jHuPg1wfhaOlo2IKpkZd18fD9Q/x2+Tf0r9YfpV1K5+q18qvIyEiDXv+zCah8fX3h4eGBo0ePigFUbGwszp8/r3U8l4WFBSwsLFTSpVLpZ/nHKj9iXRgP1oXx+FzrwszUDBKJBADwJPoJJBIJWm5uiUv9L+X6tY8+OYqdd3diQfMFsJbq3kORF3UxMngkIuMjcea/M1jacilqe9XO1evlR4b+fcj2PFSGEBcXh2vXruHatWsA0gaiX7t2DWFhYZBIJBgxYgR+/PFH7N27Fzdv3kTPnj1RuHBhdOjQwaDlJiIi3UhNDPelOP7IeJx/eR4/nfwpR86XmJKIddfW4Vn0M73PFRn/qfVl2IFhCIsJ0/uclLPyVUB16dIl+Pn5wc/PDwAwatQo+Pn5YerUqQCAcePGYejQoejfvz9q1qyJuLg4HDx4kOsLEhEZKbkgR41VNfD9vu8BAFJT9QGVXJADAHbe3Ym+e/oiNik218r0PuF9jpznz6t/4tcLv+KLbV/kyPnSexr1NMfPSfrJVwFVw4YNIQiCys/atWsBABKJBDNnzkRERAQSExNx5MgRlC7NvmYiImO19dZWAMClV5cgF+QwlZiqzbfy0koAwOxTs3H99XVsuLEhR64vCALGHB6DccHjxLSLry7myLlvvbklvn4S9SRHzqmQKqRmmufx+8eYfmI6/ov9L0evTerlaEB15coVtGnTJidPSUREn6Hfr/yOGqtq4Oabm2JaVEIUUuQpavP/efVPpe2cmvgzJikGJ56dwLGnx3LkfOlZmn3qHRlzeEyOnlvRYgcAJ5+fxMB9A/E67rVSnolHJ2Lfg30YfXh0jl6b1MtyQHXo0CGMGTMGkyZNwpMnaRH3vXv30KFDB9SsWRNyuTyTMxARUUF0LeIa9tzbA+BTi9Phx4fF/UEbgvAi9oVO57IwU32YKKvkghy77+3W+zyapA+ocnrM06sPr8TXow6NwsVXFzHr5CylPIpWscfvH+fotUm9LAVUf/zxB1q2bIm1a9di3rx5qFWrFjZs2IDAwEB4eHjg1q1b+WbeECIiAp5HP8eV8Ct6n+fWm1tYdXmVxhYmAPhu73eYdXIWrkdc15hHW+Ax4cgE8XVODF7/34X/4X8X/qf3eTRxtnLOkfPEJceppC09vxTJqclK6+xmtx5zaq3eeFk8nkY9RUxiDD4mf0TX7V0xM2Rmjpw7P8hSQLVkyRLMmzcP7969w7Zt2/Du3TssX74cN2/exMqVK1GuXLncKicREelh/fX1GHFwhEpXWedtndH/n/56t6D03t0bqy6vQq3fa2HLrS0q+9N3UWV3TM+RJ0fE16Ympvjr+l+osapGtrvr1l9fr3Hf3NNzlcqcHfYW9nodr/Dbpd/Upi89v1SpRc/d1l1tPiDtiUN1aqyqgZqra6p0F2ZH1+1d0WV7FzRZ3wRjg8fiSdQT7L2/V2P+VHmq3vfYmGQpoHr8+DG6dOkCAOjUqRPMzMywYMECFC1aNFcKR0REOWPp+aU4HXYaBx8dFNPSt0w8ev8IAPAi5gXuvr2r17V+PvuzStqJZyfE14ceH9LpPINqDtK4733Ceyw5vwQAlAaU55Qdd3bg8qvL2T4+/EM4Vl1elSNlUdRNRltubcHyi8vF7YpuFZGUkoQr4VdUglp1rYLp63/IgSF6lzMi7tMybxdeXtCaVy7I0X1nd/Tc1fOzCaqyFFAlJCSIy7FIJBJYWFgozUpORETGLToxWnwd8jxEfD31eNr0Mx23dkSPXT3w9uNbcd+bj29w5+0dtedLTk3W6brpg6yoxKhM87cv0x7fVP4GX5RXP+XAppublLZfxLxA+IdwncqSIEvQ6Uv8Q/IHnc6nTsbWLzsLu2yfq0KhChr3pW+1O/DoACYfm4z+//RXCWrPvDiDL3d8iTVX14itUelbrXJzGopUueoTiVEJUXj0/hHuvbuHCy8vIF6WtmyMLFWGg48O4mPyR7Xnuvn6JpZfXK7z5y4vZXmm9N9//x22trYAgJSUFKxduxaurq5KeYYNG5YzpSMiohwlk3/q8ks/IDwxJRHBjz+tSfci9gXcbNwAACMOjsCDyAfY0GkDXK1dYSO1gZXUChtvbMSvF37F8tafWkkU4mXxSrONv/n4Rnzd2Kdxpq1gUxpMAQB8Uf4L7LizQ2teNxs3dNzaEQBwsd9FcaZ1dSLjI9FyY0vUKloLJhITrYGVPl/aGcvgYeuhtH3g4QHsurcLMxrOgKed9oaJrDzRePzZcbXpigB02ftlWHZxGS70u4C5p+eK++t519P5GlmVlJoEa5O0z8KTqCc4++IsGvs2FvcP2T8Evk6+2N5lO3bd24X5Z+bDWmqNk31Oqpyrz54+AABzU3N8V+27XCtzdmQpoPL29sbq1avFbQ8PD/z1119KeSQSCQMqIiIjkj5okODTF/3D9w+V8k08OlF8fejRIVTzrAYAeBD5AACw/+F+bLq5CS7WLvjnq3+w6NwiAED/f/qrXHPkwZH4re2nsT/l3MqJQdTa62t1LruN1CbTPOlb017EvoC3g7fGvAcfHYRckOPsi7NKZVInQZagczkzMjc1V9rOGJz9eOpHJKUk4ceTP2JsnbHwcfQR94V/CMfwg8PxRfkv0LVCV2y8uTHb5dDkv9j/8O/Df8VtMxMz3H93H2Vcy+T4tZJSksTguuv2rgBUHz5QTFQ6/8x8ABBbrBQ23tgoft4AIPRFqNEFVFnq8nv27BmePn2q9UcxlQIREakSBEEpAMiK9wnv8b+L/8PrJN0HEKfIU9B3b19xe9nFZWL3TrPizTQep65b7vzL8wDSWnkWn1us9bpXIq4oTWxZxuXTF7Wm7hx1bMwzD6jSy2zCz6TUJJ3LoRijFRYThklHJymNZYpJjMHA/QNxIUb9WKGMAVXGVqaklLRynH95Hl9s+wItNrTAF9u+wMfkj9h8azOeRD3B/DPz1XaX5YROWzspbe+4swPdd3ZH6IvQbJ1P25OC6lr6dt7dqTavolU0o/TBFAC4WLtkoXR5I1/NlE5ElN/9fuV3tNzYEn/f+TvLxy67sAwbbm7A4rDFOuUPfRGKWr/Xws3XN5XSG69rjHfx73DxpeYZwYs7FQeg/EWZfj6jbbe3ab22IAjovbs3nkc/B5C11p7mJZqLr7OySDGQ1qX0z/1/lNIUUznceH1DaRB3Zk82xiXHIUWegrHBY3H48WF029EN8bJ4CIKA36/8jqsRV7E1YquY//Kry+ITjBkDocy6D9/Fv8Oz6GfYc38PohI+BbPpW5Gyyr+IP1qVapWlY/Y/zN7UR+m7kjNKH8RmxsveS6d8uRVo6iNLXX5Lly7VKR+7/IiIPpELcphI0v5//e1yWjfYnNNz0Ll85yydJ/S/tNaDZLluY3uGHhiqcV+LDS20HqsYvK7v4N+Q5yHo6dgTCSnaAyr/Iv7ik2FTG0wV081MzOBh66H0BJk21yKu4VrENRSxLwI/Dz8sOrcIu+/txqbOm7Dg7AK1x3St0FVjgLjp5ialQLL+mvoAoDQGCEjrFh2wbwAAoKh9UZWJR3W9j0eeHFEab6XrPE52Fnb4kKQ8iN5aaq00uagushL8KLyLfyeuxaiOpikbMsoYJCXIEmBqYgpzU3OYmZgpzXGmbb4zQ8lSQLVo0aJM83AMFRHRJ3ff3kWPXT1gZmKGX1v+mu3zTD0+VWlgd0YHHx3E5GOTMbjmYEhNpbj06lK2rwWkdQFlNhhcnXF1xonjYIC06RqAzFuo0o+VyhgE7Pt6H+68vYOeu3rqXI7+//THpHqTxMHYM07M0Ji3U7lOaFa8GT4kf0DNwjWx6NwisUtKUf6MMnYX3n5zW3ytbp6t2KRYfLvnW0yqNwklnUtqLMuN1zdw4/UNzW9MA3VrIFqZWcHCNGszyiu6ItMTBAGvPrxCYbvCagf8d9zaUWv9qjunOhmDznpr0gbKn/vuHGzNbZWeUNVlLcO8lqWA6ulTrm5NRKSLsJgwbL21FVtvp3UJpchTMPDfgVk+z603t/A06mmmXTGTj00GkDZGylA8bD2UBlcraApKyriWwf139wEAVlIrrefOTovE7FOzxddXI65qzGcttVYKcsbUHqNxjI9CdFK0+LrDtg7oULaD2ny9q/bG2mtrAaQFS8MODMP+7jm/ooi6QMdKapXlJXrUtVCtvrIaqy6vQv/q/dG/uuoDCJkFy4pzZtZNJ5PLVMaeAVD7j4QxdvlxDBURURZFJ0arHYQrS5Xh4suLSE5Nxnd7vxODKXWkplLcfH0TG25sUBozk1Hv3b0xI0Rz64oxiYiLgJWZ9sAovdGBnxbtNTPR/v+9ukAtp2Qss7mpudZZx4G0QekKEXERWHNtjdp8DhYOStvaWhkzk9XxZNZS6yy3UGVsJUpKSRInKF11eZVSK5GuFC1UmXUnJqUkqQ2U2m1up3LdFHkKjjw5ghEHR6h0dRpKlgKq0NBQ7Nu3Tylt/fr18PX1RaFChdC/f38kJWW9/5WIyBh9SPqAsy/OYvap2eKXwrWIa2i6vim67+yO4MfB+Of+Pzj74iwA4KdTP2HgvwMx/8x8vE94r/XcslQZ+uzpg8XnFqPZX80ybRHJDyoWqpilrphqntWwvPVy7P1K8/IkCvYW9vj3638xvs54Me2Pdn9kq5wZqWsdy2wplozr62kaI+Vg6aA2PTumN5wOZytn/BL0CwKKBKBftX4o5VIKowNHK02HofDm4xudWqjSB2o3Xt/Ayeef5n9StK4p/HTypyyXWxFIZdb1J5PLMh1rp/BR9hETjkzA6bDTmHJ8SpbLlBuyFFDNnDkTt29/6ie+efMm+vbti6ZNm2LChAn4559/MGfOnBwvJBFRXnsY+RCN1jXCsAPDsPPuTqy7vg7Ap8e3H0Q+wMSjEzEjZAaGHRiGxJRE7HuQ9g/n7nu7s3y99N1TCjm1aG1mLvXP/nirqh5Vxde1vWqjtEvpLB3vX8Qfhe0Ko3O5zuK2Ju627ijmWEzcruxeOWuF1SCrrTiA+gWL1VG3QHJWpo1Ir7FvYxzucRj1i9XHstbLMKDGAGzuvBlfVfpKbf63H9+qHZRez7ueWK6GPg1V8ow6NAp/3/kbCbIErL6yWmnfjTfax3cVtiuskqYYlK4p6FTMIv88+rnSVBvapJ+n6nTYabyMfanTcbkpSwHVtWvX0KRJE3F7y5YtCAgIwOrVqzFq1CgsXboU27Zpf5SWiCg/2Hxrs9K2YuLB9IOP06v7Z129rzkrZJbStq5Ln2gLvNR9waXX0KehSlpWxt0sbflpfJSLlUuWu6UUKhSqgAPdD2Q6cL9m4ZroX70/fm7+s9YZ0bMip86jjrr70XtP7xy/TsVCFVXSelTpoTFYPND9AH5v9ztmN5mttrt1zuk54qDw9Cq4KS+Dk7HVydLMEuXdyqvNo6nLT9Flp+2p1IwytgAvDF2o87G5JUsBVVRUFNzdP/Urh4SEoGXLluJ2zZo18eLFC3WHEhHluYeRDzHq0CiVeZh0kfG/9uAnwWLXXm7Zc3+P0rauUwVknPQwvb86/qVxH6A6fggA7Mx1X3dOaiLFzEYz0alcJ7Qp3Ubn49Rxs3GDqYnq02rpSSQS9K/eX20gmF8ogvOcNLn+ZJU0D1sPjcGxqYkpqnpUhbmpeZYCyowBVcaxTQIElQljFYFUTq6/l3Em9fTdlIaSpYDK3d1dfNIvOTkZV65cQa1atcT9Hz58gFQqzdkSEhHpKF4Wj19CfxEfO//z6p84+fwkxh8Zn8mRaV1uffb0QY1VNXDz9U21/9kPO5D7U8Kkn1Fb22Dblx9eYun5pQiLCVNZKDi9zMbwqBs/lJWFfM1MzNCqVCtMqjdJ/PL+9+vsT0aZXcMCcrZutC1fkxUZ1/DLLsU8Zpo4WzkjqESQUpqDhYPa6RQyBlBZ6YLMGBTFJMUobQuCgLreyq21YguVmjFU1TyrqW3F+9bvW3xV8SutXcDGJksBVatWrTBhwgScOnUKEydOhLW1NerV+9QkeOPGDZQoUSLHC0lEpIvfr/yOTTc34ds93wJIa1UCMn+y6nXca+y8u1Nsyeqzpw/+uqG9ZSe7+lTto3V/4B+B4peWttmnO2/vjPXX12PEwRF6lUfdF7W6R9c1Ude64W7rrjR4PDcd7nEY27pswzeVv9H5mIl1J2aa59eWv6KUSykAaRONru2wVqdzBxQJwPqO67Guwzosa7Us0y5XXTT2bYw/2/+Zab70M8x/6/ct3G3ddZpuQmqaeUNIy5JpvVHJqclIlafiesR1yFJlKi1UH2Uf4evki93ddouztIuD0tV0+f3c/GeV1iYA6FK+C0bXHo1J9SZlWjZjkaWAatasWTAzM0ODBg2wevVqrF69Gubmn37x/vzzTzRv3lzLGYiI9Bcvi8fJ5ydV/lt+EZP5kAN13Q5Xwq/kWNm0aVemHQb7D850Udf119cDUF3/TZ3Mlk8BgKbFm2rcp+4LNzuDtDNSN6bn5+Y/I7BoIICce0LP2coZxZ2Kw0Rigkv9LyGkd0img+y9HLzEpXU0KWJfBJs7b8a5786hXZl2qFioIi71v4SuFboq5Wvlqry0S6tSrVDerTwqFKqAgKIBALS3ds1tOlclrVbRWqjtVRtAWmAxv9l8tfczo3rF6iGoRBCG+A/BoJqDAKiv34yfK3VPCGbkaecJIO3359cLv6Lv3r6Yc3oOBv07SCmfYp3KovZF4WjpCOBTy1TG372vK30Newt7tddTtHZmJbg//uy4znlzQ5YCKldXV5w8eRJRUVGIiopCx44dlfZv374d06ZNy9ECEhFlNOHIBIw6NEplwsj0C+mqG6i95dYW1P6jNmqsqoHfr/yOeafn4c3HN1n6o51d9hb2GBU4CoByq9CebntU8p54dgJxyXEYeWhkjlx7TO0x6FG5h9K8T9ooyqmPcm7llLqg2pVph4Y+DbG05VKc7XsWVTyq6H0NdRSfAU1f1EDa+Li5TefC3NQcPSr30Hq+jAO263krD9R2kCp3qarrLv2tzW/oWUV1lndzU3OVYPe3Nr9hfrP5mNNkDsbVGYfB/oO1li89E4kJfmryE3pX7S2mpV9EWNGC1aOK8nvWZQyV4ndEJpeJC1Dvva99ugtFl7XiCVlFQGVuao5eVXphQPW0pXqa+DZROdbW3BaA8ljG8m7lMTxguMbr/Xjqx0zfR27K0kzpCg4O6vvknZ1VHw8lIsppisHhW25twZjaY8T09IGRuvlsfj77s/h65aWVAIDtd7bnVjGVHPzmoFi+9I/cF7IppJL33rt74hQM2WFpZqm0fpqrtSuG1xoOQRDgbuuOsq5l0W5zOwCfliwxNTFFqjwVnct1RsVCFTGr0Sy8/vgaKy+tzPa6aT81+QnhceG48foGvqzwJYC0L++8CGDTv/9qntWUWiEtzSxR3Kk4TvQ+keWyBHoFYnnr5dhwYwPKuZTD/dv3Mz3GzcYN7cu0F1seFRTB2s/Nf8aEIxMwuf5kVC9cXdyfsTUsOwKLBqJ/9f4o61oWdbzqYFjAMJVxXepaqFytXfEu/p24rbhPF19pXlAbABr5NBJf//Pg0yLVSSlJGHUoLVBPTk3G0IBPT/QN9h+Mo0+PittHex4V/+lIXz/mpuboUaUHvqn8DWqurqm1HIaQrYCKiMjYyAU5pCafxoKkX08OAPxXG2ZwazXPapjecLrSF0P6mdGlplLULFxT5YsqffCnq5G1RuJt/FtU86yGUYdGqQRrEolEXNTXRGICuSBHk+JprQNnvz2LiLgIFLEvAgBoWSptzMyJZyd0nhtInZVtViIyPlLsMsor6buXVrVdhRqraojbii7N7AZ2/kX84V/EHzKZDEseLwHS9WQVsSui9hh1T9vJBTmAtKkrTvY5mSuBpuKpSAV1g+TTt1BZmlnif63+h8j4SKWHORRl09St3rJkSzTybSR26SqOUdSDts9zxidqFa1T6a8LfGrZVQTlOfnUYE7g0jNElO9tvbUVDdY2wMmwT49OZ2zhUXx5ZdXiFotV0r6q+BVO9jmJHpV7oF+1fhgWMAy1itbCkZ5HVPL+2vJXlYHJGZ+MWtFmBdqVaZet8ikUtS+K7pW7Y0StEahfrD5+a/MbNnTaoDH/jq47sLjFYvEpKlMTUzGYSm92k9loVaoVlrXK3hqB5qbmeR5MAZ8mCE0/EWhucJG6KG2XcFb/YJa6YCl9K1petNppkr6F6mSfk6jqUVUlyHn0/pHWc9QsUhONfRsrPTU6veF08fWue7s0Hutq7aq0nX7qjPTd4+mfWEwfTOV2HeuKARUR5WsLzy7EgrMLkCBLQPiH8Bw/v7qZv9uWaQtrqTWG1xqOATUGoGeVnvhfq/+Jg3DTU/cElbrZsyfUnaAyPked8bXVPz3nZOWktF29cHW111HwdvBWebxdncJ2hTGz0UyVyRqN3axGs/Bdte+wvNVylX3qulmzy1mq21AXG6lN5pkMJH2rlSKASR9QdSjbAVXctY95UzcHmWLgf8bP4bg645S2TSQmWsdGKaQPtNL/Xs5pYhwrtLDLj4jytYwzmuc0dRNfalte5fd2vyP8QziOPDkCJysntdMSDPEfgnhZPL4o/4WYZm5qrraFSLHPw9YDowJHwcJE/RN4TpZOatNzSlZmTzcGLtYu+L7G92r3qZt7Sx9tS7fFvof7lGaNz8iY79+EuhMw6+QsfF/90/1Kf4+6VeymtXutmmc1rdNvZDy2nGs5lbxty7TFyssrUc2jmsbrpL/GL0G/YPKxyfiq4ldZWpA7NzGgIiLSIqtfvlU9qqKqR1VxDJI6rtaumN9svkp6+rFV6ZVwLiHOeH7vzT0xfVydceJYscE1dX8aLDvULU9CaX6o+wN+qP+DTvM5GaPSLqVVZtRPP1u+g4WD1jnRMnbZKSjGNGYMqNI/javgaOmIoz2Pau36TB9Qedh64Pd2vwMAIuMjNR6Tl9jlR0RG48eTP2Lo/qFaxztlNklnTsvLQKJLhS5q0+++vSu+LuFUAi1dW2JKvSlKcxNp697LCZnN1F3Q6RJMLWu1LN8smZN+ugUnKye13dkKmn5HFK1yGQMqdQs2K9K1fc7UzfoOpK0GYC21NnhAy98QIjIKckGO3fd2I/S/UDyMfKgxX6uNrTTuyymKGbLVUUy4mBuqelRVO9FjRk1dmqJ1qdZKaXnRpTS94XQM9R8qljGvZkP/XAQUDcDPzX9GNU/N3VrGwlpqjc2dN2Nbl20wMzHT2q2W/inC9NQFTj6OPtlejkdTsGVmYoYtX2zB8paq4+XyEttwicigklKScPfdXaXZpDVNNPhf7H95UqYlLZZg+cXl4txJa9qvwdv4tyjpXBKetrn7xFpWWjDSP52VF0+JpR94HNI7RG3XjbGa23Qu5p2ZZxQDmL+p/A2uhF9B/WL1DV0UrdL/Y6Hpd7KMaxkUtS+qdp+6Gfe3ddmW7dbOCoUqaNxX2K4wLJINO06NARURGdSPJ3/EgUcHxOAFAE49P6Uy8PvVh1fosKVDnpSpkE0hpUe+K7lXypPrAmndGpXdKyMuOQ41CtfAttvbVJ6KUkjfzaepOyS35KdgCkhbfqeJbxOdZgXPbfWL1cfOL3fmenCeF7R20ZmofiazE0xt7rwZZ16cwdeVvs7ysXmJARURGdSBRwcAAFtvbxXTVlxagU7lOilNBXDi2Yksn9vZyhnvE97rnL+YYzF0Ltc5y9fJSRKJRBxsK4EEvav21viYv7utO35s/CPszO2MIlAwdsZ0j7St75effFNJ90Wps6uUSymt3fDGgmOoiMhgDj06pHHf1ONTlbazs2DvqrarVBbB7Vaxm8ZxUH93/dso/gs2kZjARGICiUSS6ZxJLUq2QB3vOnlUMiJl6QevZ2ZSvUm5WBLD+6wCqunTp0MikSj9lC1b1tDFIioQUuQpmHd6Ho49PaZ2/4uYF+i6vStWXV4lpv1w7AeN5wv9LxTf7PwGC88uxKsPr5AqpOpcln7V+mF04Gj4OPrg5+Y/o7FvY2zotAGX+l/CmNpjlJ5YUgywnlJ/is7nJ6K0FQOqe1bPPOP/83H0yb3CGIHPrsuvQoUKOHLk0/IPZmaf3VskMkr7H+7H9jvbsf3Odlzqf0lpX+iLUAw9kLYY6qrLqyBLlWl8Mii9e+/u4d67e9h8azMG1Rykc1kG1BggvvZ28FaZ8+mL8l9g/8P9qF+sPpoWb4oz354x6okXiYxBJfdKuPn6JoC04Gh07dGZHlPbq7a4mHn6Nfo+R59dtGFmZgYPj+w9kklE2TczZKbGfYpgSmHNtTVYc21Nls5/Nfyq0vbsJrMx6Wj2uhAqu1fG/u774WKV1l3BYIooc/Obzsfe+3vh5+mHsq669f70rNJTDKjSTxb6OfqsuvwA4OHDhyhcuDCKFy+O7t27IywszNBFIvpsPI1+itWXVyNeFi+m/fvgX9RYVUMpX41VNXDw0UEkpybjdNjpHLn2m3jlCT2bl2iu1/kK2RRS+xQSEannZuOGvtX6oppnNVhLrXU6RjFbOgDYWXzeAdVn1UIVEBCAtWvXokyZMggPD8eMGTNQr1493Lp1C3Z26isyKSkJSUlJ4nZsbCwAQCaTQSbTPNU+5T7F/Wc9GJ6iDnrs6oEUIQXvPr7DmMAxAFQHjyv8cPQHWJpZIjElMUvXcrN2w9v4tyrpjyI/rXY/oNoAyGQyCIKgtbyfI/5eGA/WhQ7kEH9PpZDm6r0ydD1IBE1/kT4D0dHRKFasGH755Rf07dtXbZ7p06djxowZKumbNm2CtbVuEThRQTH6ftqYCXcLd4zzGaeUllOq2VfDldgrGvfXc6qHDoU6AAAefHyA3/77DZ3dO+Pv13+LeRaWWZijZSKi7JELcmwM3wgHMwe0K9QuV68VHx+Pr7/+GjExMbC3t8/Va6nzWQdUAFCzZk00bdoUc+aonx1XXQuVl5cXwsPD4eKi++OglPNkMhmCg4PRrFkzSKX5c9FRYycIgk5z8yjqYkb4DEgkEnjZe2H7F9sBALX+rJWjZfqm0jfwsvfCnDPqf2dNJCY42+esSvq0kGk49PgQWpdqjSn1Pt8n9vh7YTxYF8YlMjISnp6eBguoPqsuv4zi4uLw+PFj9OjRQ2MeCwsLWFioDkiVSqX8BTESrIvccezpMYwLHofpDaejTek2SEpJwqP3j1DOrZzG2YwV05GYmJjAzMwM44+Mz/HJEm0sbPBFxS/QwLcBWm5sqbL/u2rfqf08TG04FS1LtYR/EX9IzT7/zwt/L4wH68I4GLoOPqtB6WPGjEFISAiePXuGs2fPomPHjjA1NcVXX31l6KIRGZ1xwWlddtNPTBe3e+3uhX/u/6PT8cFPgjXOOaUPxYKqbjZuCO4RrLL/W79vNR5Xr1g9PrFHRAbxWQVU//33H7766iuUKVMGXbt2hYuLC86dOwc3NzdDF43IIC6+vIjv932Ph5EPM8175sUZAMCOuzsyzZsqpGZ7yoKh/kNV0rpV7Ca+Tr9CvZOVE9a0V55eQWrKlgAiMj6fVZffli1bDF0EojwlCAIECBq76FZfWY0r4Vcw7sg49KvWDx+TP+KL8l/g4quLSvlkqZ+ejrn79m6m130Z+zLbZVYXEPX164stt9J+f81NzZX25eXCxERE2fVZtVARFSSCIKDm6prwX+0PuSBXm+dKeNrTci9iXmDq8amYd2Ye/ov9D5OPTVbKt+/Bvlwvb3rVPKspbduY24ivI+MjVfLv+nIX2pRug9C+obleNiKi7GBARZRPhceFi6/DYnSfwDYhJQHvE94rpaXIU3KsXArDAoaJr9MvRSM1kWJBswVKedO3SiWlJiEjLwcvTG84nd19RGS0GFAR5UPJqcnovbu3uH32xVlMPjYZf179M9NjE2QJqFioolLavDPzlLZHHhyJq+FXcezpMbE7UFMrWEY9KvdAVY+q+KbyN7jU/5LKun7mpuZwsHTAry1/RSGbQljacqnS/qwstkpEZCw+qzFURAXF3vt7lVqZfgn9RXyteAruwssLao+Nl8WjvFt53HpzS+P5T4WdwqmwUwCAQTUHoUfFHkgVUlXymZmYYV7TeZh0bBKSUtJalobXGq617IXtCgMAAr0Csb/7fjH936//xZOoJwgoGqD1eCIiY8QWKqJ85JfQX1BjVQ3svb9XY57k1GQAwKB/B6ndP/TAUMQlx+l8zd33dgMAUgTVbsH+1fujgU8DmEoyXxNvcYvFGBYwDDUK11C7393WHYFegTqXi4jImDCgIjJy99/dx5ZbWyAX5Nh0cxMA4M7bOxrzL7+4HOf/O6/1nMFPVOd30kQxvkpdC9WLmBcAgEY+jQCoPqGXXl3vuuhZpWeOTwRKRGQM2OVHZOS67+wOALC30G0phQ03NmDDjQ1a86SfJiEzihYvdS1UXSp0AQAMqDEASalJ6Fmlp87nJSL6nDCgIjIickGOFhtaIEWegqM9jyq15tx8fdMgZYpOjEbI8xA8iH+gsq+QTSEAaeOi5jadm9dFIyIyGgyoiLJJMehbAgn8PP1gZqL/r9OJZyfEweb/PPgHbUq3EffZmtvqff7sGn90PGJiYuDg4KCUrq2Lj4ioIGFARZQNsUmxSoO+O5XrhN5Ve8PKzApOVk7ZPq8gCOLr3fd2o6FPQ3FbJte9my67ulXsJs5YrgsbqU3mmYiICgAGVETZ0GNXD6XtnXd3YufdnShkUwgtS7ZEA58GqOxeWdy/8OxCvPn4BmYmZnC1dsXIwJFqz5t+4sq45Dh8SPogbmc2Lioz3/p9q3WeqpqFa2JM7TEo61pWXDBZk6M9jwIATE0yf7qPiKggYEBFlA2a1rJ78/EN1l1fh3XX14kTWsoFOTbf2qyUb1DNQbAws1A5/mPyR/H1k6gn4oLFOSGwaCC6V+qOJuubiGmbOm/Cyksr0dCnIZqXaA5At248B0uHTPMQERUknDaBKJelD5IUMi79AqR1I96PvK+UNv/MfI3nrVm4ZpbK8SL2BRwsHZSexCvtUhq/BP2CdmXawdLMEgByZCwYEVFBw7+cVKClylOVuq1kqTKd1our5llNXHg4M0+inqikRSVGwdPOUyntm53f4NWHVzqdc1TgKLQp3QaN1zXWKT8AMWAaFjAMnraeKtdXkIDzRBERZRVbqKjAevvxLZr91UxsBbr86jIarG2Abbe3qc2ffi07XYMpuSBH3719VdJ77uqpNAAdgM7BVGPfxvi60tc6z0ul4GHrIb7uUqEL6nrXVZvvXfy7LJ2XiIgYUFEBtvX2VsQmxWLb7W2osaoGpp6YiuTUZLXdbL9d+g2N1zVGWExYlq6RIEvQuC88LhxA2sSZugZoAFDUvqj4WjHw3dHSEd9V+07jMQNrDFQaJK/NxVcXxde/t/tdZf/577TPwk5EVBCxy48KrIxdW6/jXmvMu/rKagBAp62dcKrPKZ2vkZSapHHfh6QPgB0w48QMHHp8SKfz1S9WH339PrV4/a/V/3A1/Cr8i/hDairF71dUAyAA6FtNtZVMkxG1RuDY02MAPnUTKhz8+iCf7CMiUoMBFRVYJhLdGmg33tiotF1vTT2djhMEAYkpiRr3d9/ZHSG9Q3QOpgDgl6BflLatpdao411H5+N1UdiuMHZ32w1rqTWiE6OV9jlaOubotYiIPhfs8qMCK+Pgc3XTGADAonOLNJ6jX7V+GvdFxEUgNilWaxkarG2gdX96x3odyzTPli+2oEXJFjqfU5Oi9kXhbOWsNG6MiIg0Y0BFBdbJ5yeVtpNSPnXPZRwwrkn/6v1Ryb2S2n1tN7fFNzu/yVbZJtefjPnNPo3l2tBpg06D0Es6l8Tk+pOzdU11FGv1ERGRdgyoqECKTozGnbd3NO5PTk0GAIQ8C9GYp12ZdpBIJPij3R+oUKgCAGBH1x06dyVqcy3iGhr7Nkb/6v0xrcE0lHUtq/OxObm+nr2FPQKKBOTY+YiIPlccQ0UFSlRCFKISo7DswjKt+RJSEmBuao7Rh0drzHPi2QlMbTAVJhIT/NHuD8QmxcLZyhkWZhZqn+47/e1pyFJl+Hrn1wj/EK71+tZSawBpLWBZZSIxwabOmxAvi8eOOztQ3bN6ls+R3g/1f0D7ze1Rzb6aXuchIvqcMaAioyIX5Fhybgn8PP1Qp4jmwdZ33t6BrbktvB28ldLPvjgLdxt3lHAuIaYJgoAPyR+w+95urL22NtNxTQAQL4vPtKUp/XnMTMzgbOUMALAwVQ2oqnpUhaWZJSzNLLG3217UXK19lvPva3yfaRm1Ke1SWryuvgrbFcbRb47i2OHMx3ARERVUDKjIqIS+CMXGmxux8eZGhPYJVZvnzcc36LkrbfmUQ98cwsLQhTj8+DC2fLEFww4MAwBxHb3lF5fj34f/wt7CHg8jH+pcjqNPjiIqMUopzd3WXWlqhfJu5dUeq67LzcrMSnwtkUjQpXwXbL+zXe3xmztvzvKknbnNSmoFiYQzqBMRacKAioyCLFWG/2L/w/OY52Kapkkxn0Y9FV8POTBEDJQUQRYAXHp1Cfse7MO+B/sAaJ9jqpFPIxx/dlwpLeR5CJ5FP1NKc7BwUDrPqMBRas+Xce4mIC0gSW9snbFqA6rGvo1RyqWUxrISEZFx4qB0Mgprrq1Bl+1dsO76OjGty99dkCqkquSVyWXi6/StToqB5ADw/b7vxWBKm5mNZsLF2kUlvVnxZipzMGXkZu2mNl3d9AsZW61MJCaY3WS2Ulpo31ClJ/uIiCj/YEBFBiMX5Oj/T39MPT4Vqy6vAgBExkeK+9/Fv8OHlA9K+d98fIM3H9+oPV8Z1zJZLoOtua3aLroFZxeopGXsArSzsFN7TnVdi5deXVJJa1q8qdK2LosyExGRcWJARdlyJfwKOm3thLMvzmb7HI/eP8KV8CvY/3C/xjwpQgoA4EnUE/iv9kerja0w+9RstXnvv7uf5TJYmlmqLEGjjqOlI+p6KS8mrO84JxOJCZa3Xq7XOYiIyDgwoKJsmX5iOsJiwsRB4LrIzqzbMiGte6/r9q5ZPlYXlmaWEKB9Es8ZDWdgf/f9GlukdKFpbij/Iv7Y3HkzTvY5qXY/ERHlDwyoKFtefXilcZ9ckGPe6XnYc2+PmDbgnwHwX+2PGqtq4PKrywB0W0svWZ6MsJgw/QusgaWZJWSpMq15rKXWWZoss36x+ipp2rrzSrmUEuedIiKi/IkBFWVJqjxtkLi2YOjCywvYfmc7Zp2chRqraqD+mvq4HH5Z3D9g3wAAyDSQAYCPqR/R9e/caZ0C0uaMSj+YfWqDqSp5bMxtACg/vVejcA2N5xziP0QlzdXKVZ9iEhGRkWNARTo7+uQo6q2ph2NPj2mdkyjjdAfxsniVPNOOT0OPXT0yveYfL//IekEzUc3z04zflmaWaFaiGYC0ViR1T+4p5pDqVrEbitoXRVnXsvi5+c8az+9o6aiSNq3hND1LTURExozzUJFOwmLCMP7IeADAuOBxkJpKkQrlKQ2SU5Pxz/1/YGaS+cfq34f/5ko5MzOh7gRceHlB3LYws0BAkQDMbjIbfh5+Kk/yAZ9aqOwt7LG72+5Mr6FusHphu8LZLzQRERk9BlSUqaiEKHTa2kkpLeOTcceeHsO44HF5WaxssZHaKG2bm5pDIpGgeYnmAAAHSweVY9LPcq6LjAFl+zLts1hKIiLKbz7LLr9ly5bBx8cHlpaWCAgIwIULFzI/yEjIBTl+Pf8rTj43nqe+dt3bpZJmamKqtJ0fgikgbd6p9OO/LEyVJ+E0NzVHUIkgpTRFC1VW9KzSE5XdKyOkdwgm15+cvcISEVG+8dm1UG3duhWjRo3CypUrERAQgMWLFyMoKAj3799HoUKFDF28TAU/Dsa66+tge9cWJ3qfyLPrPoh8ADMTMxR3Kq6yL0WeopKWvoUqO9MhGIqtua1S2TMGhoDqhJ1ZbaECgGEBuk8nQURE+d9n10L1yy+/oF+/fujTpw/Kly+PlStXwtraGn/++aehi6aTd/HvAABxyXEQBO3zI+WUeFk8vv77a3Td3hX99vZTeuoNAHwcfVSOSd/K8yHpg8p+hTpedcTXObHgb22v2uLrioUqiq+9Hbx1Ot5aap3pIr+cwZyIiLLqs2qhSk5OxuXLlzFx4kQxzcTEBE2bNkVoaKjaY5KSkpCUlCRux8bGAgBkMhlksswf689pVqZWYiAVmxCbJ/MTvf3wVrzmlfArGLhvIIbVHIbybuUBAM+jnqsEd7JUmZj2OPKx2uCvWfFmcLZyFvd523vj5pubmZanZcmWOPDogHhc+nObwETcbuLTBDdfp51vRcsVaLW5VabnLmpbFHK5XDyHujqu7FoZjX0a4+jToxrzFDSKe8B7YXisC+PBujAuhq6HzyqgevfuHVJTU+Hu7q6U7u7ujnv37qk9Zs6cOZgxY4ZK+vHjx2FtnfeTLV6NvYqYmBgAwD8H/oGdWfZn5xYEAW9lb+EqddU6b9R72XvxmgAQEhOCkPshWFhmIQDgbtRdpf2WJpaIkX/a7rpR/TxRYU/CEGkaKR77NuUtYj7GqM2rMK3ENNjH2eO2/DZufkgLlhRBLgA8f/ocMXFp5zh84bB47rPHzmKqx1REy6Ix79k8lLUpixsfbqic/8ihI3gS/gQxsWnH7d+vftkbeZRcPLemPAVRcHCwoYtA/491YTxYF8YhPl51ip689FkFVNkxceJEjBo1StyOjY2Fl5cXGjVqBBcXlzwvT8qDFOw/nfYFXq9RPb0et998ezNWnl+JLuW7YHSt0RrzPY95DodI1afbWrRsAROJCaJvRSPkQgiqelTFtYhrAAALWKjkz6hUqVJwtXbFletX0ra9SyEiLELrMS2bt4SDpQNKRpZEj909EBsbC3t7e7GbroRPCYQ9S5s5fUDzAbhz6A4AoE3rNuI5Oso6wsLMArXX1FY6d83CNdGqRStcCLmAx48fAwBatVLfqtVQ1hDP/nmGOl510Kpm5i1fnzuZTIbg4GA0a9YMUim7QA2JdWE8WBfGJTIy0qDX/6wCKldXV5iamuL169dK6a9fv4aHh4faYywsLGBhoRocSKVSg/yCCBJBDB5SkKJXGVZcXgGJRIIdd3dgQr0JOPLkCCYcmYA/2v2BKh5V1F4zvSQhCfbm9giLDYNEIoGDpYPW8UcWZhZISvnUfRqVFIUiDkXEY6zNrVHFowpuvFZtOVKwtrSGVCpFBY8KmNtkLgbtGgSJRAKJRIImvk1QxP7T+eoUq4OfmvyEUs6llO6T4nXGssohh1QqxYOoB+I+TffXQeqAHV/u0FjOgspQvxekinVhPFgXxsHQdfBZDUo3NzdH9erVcfToUTFNLpfj6NGjCAwMNGDJdJd+OZbElES1eW69uYX/Yv/L9Fzpn2aLSYzBhCMTAAB99/ZVvqZcfb/z4ceHce/dPey5n7Ymn5259u7HH+r9oLQtF+SQmnz6gFuYWmQ66Wf6/AFFAgAALlYuWN9xPWY1noWeVXqiWfFmWNJiCSQSCVqUbIESziXUnmt9x/Vo5NMI3/p9C2crZ0yom/b+O5btqLUMREREWfVZtVABwKhRo9CrVy/UqFED/v7+WLx4MT5+/Ig+ffoYumg6SR/cJKQkqOwP/xCO3rt7AwAu9b+k9VzpW2iarG+iMV/6VqX05p6ei75+n4KvjNMJZORqrbxe3cP3D1GraC1x29zUHM9jnms9R/qAy1pqjR9L/oi2rdrCxtJGPMecpnO0nkOhvFt5LGi+AAAwqOYgMb1bxW7wtPVEJfdKOp2HiIgoM59dQPXll1/i7du3mDp1KiIiIlC1alUcPHhQZaC6sUrfQqUu0Hka/VR8nSJP0djiE5ccpzFQUrmmhhaqYo7FlFrJMnvisJxrOaXtXlV6wVTyaZ4na6k1zE3NtZ4jYzedlalVpsdklYnEBI18G+XoOYmIqGD7rLr8FIYMGYLnz58jKSkJ58+fR0BAgKGLpLP0wU1SqmpAlL4bLzYpVmW/QmYzlx9/elx8nXHeKYWidkWV9mVc9DgjW3Nb/NXxL9QvVh/DAoahW8VuSnM42ZjbIFWequUMRERE+dNnGVDlZ+kDmPStQ4p5kz7KPqrdn1H6BYDVGRs8Vu0104uXxSu1GKmbVTw9iUSCcm7l8EvQL+hZpSdMJCZKrVo2UhsIUJ2vqrhTcViYWaBFyRZaz09ERGSsGFAZEbkgx+mw0+K2ImD6kPQB7be0x9zTc5VaibQFVFmhKaD6KPsIJ0snpXzj6qhv+dIUbKWf/ypeFo92pdsBUB5v5WHrgZDeIZjVaFaWy05ERGQMGFDlgaiEKNRYVQPtt7TXmu/w48N4EvVE3FYETNtub8OrD6+w484OpSAqtwOqeFm8UrdjYNFAdCnfRW3eNe3XqE1/GPlQfP0g8gG6VuiKryt9jRWtV4jpEkhgZmKW6ZIwRERExooBVR74+ezPAICXsS+15jv29JjS9p9X/8TOuzux4tKn4CN9l9+i0EU5Ur70A+HT+y/2P3Fge3m38qjrXVcp6NFljbtuFbuJr8u4loGLtQtGBY6Cr5OvmK5tFnciIqL8gN9keviY/BFyQZ5pvlIupXQ6n625rdJ2dGI05p6eq5SmWDwZAK5GXMXrOOVJTLND3eB3hc23NgMAGhRroNKClH7OKC97L7XHu1i7iAsX967aW20eR0vHLJSWiIjI+DCgyqaIuAg0+6sZRh/SvKSLQvoWmKSUJKy6vAq33txSyvMy9iX23t+rlNbYt7HK3E7pAyoAeJ/wPqtFFymCQUULVVnXshrzWppZqqQJEHC4x2H889U/Wueo2vnlTlzqf0mlJWpqg6mo7F4ZQwOGZqf4RERERoMBVTYdenQIyanJOBV2KtO8iif0AGDjzY1YdXmVODmnwqRjk1SOi06MRqqgPM1AxoBKMfnnh6QP2HNvDz4kfdD1LYhjsBQtVKWcNbekhf4XqpImF+RwtnKGp52nztdMr12Zdviz/Z9wtnLO1vFERETG4rOb2DOvZAx0ElMScfjxYUQnRuObyt8otcaknypg5aWV4uvYpFjYW9gDAJ5GfZqwUyEmKQYp8hSlNMXixOmvCwDTTkzDyecnceLZCSxqodvYqsSURDyLfoZVl1cBABwsHfBn+z8hF+Q4/Pgwtt3eJuZVdNullz5QJCIiKsgYUGVT+rFTqfJU1P2zrrhd1aMqKrtXFrfTP0WX/rg7b++gVtFaOBN2BvGyeJVrxCSqBlQZnX1xFsGPg3Hy+UkA0NpiVturNs6+OCtuJ6YkoueunkrlVJT7/H/nlY7tU1V16R51c0oREREVRAyocsDD9w+VtuOS45S2NU1v8MeVPzA2eKzKDOTVPKvhSvgVxCTFZPoE3JZbW1TSNA2Un1J/ClpubKmxXDde3xBfZ1xzz8HSQefrEBERFTQcQ5VN6bu7wmLClPYp1p47+uQoDjw8oHFNvasRV9Uu59KrSi8AaYPFdV2PL7377+6rTXezccPIWiPF7YwB1dv4t+Lre+/uKe1Tt55e+mVwiIiICjIGVNmUvrtLMc+UQnJqMpJSkjD+yHhMOT5FKVDRhZuNm05zPGnSY1cPAOrnd+peubv4OmPr1vJWy8XXs5vM1nh+xRN/lQpVynYZiYiIPifs8sum9N1dGbu+klOTEZMUI25HJURl6dy25rawMrPSOOGmrsxMzDC5/mRMPT4V/av3V9m//+F+8fXaDmtRwrmEuF3WtSyWt16Oh5EPUb9YfaXj1nVYh623t+K7at/pVT4iIqLPBQOqbErfFRedGK20Lzk1GZdeXRK3IxMixdeV3CuhQ5kOmHVS87p1tua2ObIMS3JqMlqVaoVaRWsprcmnTnGn4ipp/kX84V/EXyW9hHMJTKqnOs0DERFRQcUuv2zSNrt4UkqSUutSRFyE+NpGaoPSLqW1ntvW3FZlfNLy1stxod8FVPOshsa+jbNUVmcrZ6UAbXnr5Sp5rKXWWTonERERfcIWqmzStjBxcmqy0iDu9NMmtCndRu2s4wpNfJvARGKi0kKlaCla1TZtzqg7b+8oTXmQFRmXuCEiIiL9sIUqmzILqNIvYqwwPGA4gkoEaQyoJBIJ5jWbp9P1y7uVV0kr5lhMaXt0oPplcbQtMUNERERZx4AqmzILqN5+VH2yr6FPQ0gkElhJrdQeV8zhU0Cky5QEowJHia/tLeyxuu1qpf2+Tr5qjzORmGBBswXi9h/t/sj0WkRERKQZu/yyICYxBvsf7kcj30Z49P6Rxny/XvhVXFImPUVXm7p9AJRmV29Tug3WX1+vtTxfV/oat97cwuHHh9HXry+szJQDNQtTC43HNvJthPPfnYdEIsl08lAiIiLSjgFVFjRZ3wQAsDB0YaZ5Y5NiVdJszG0AqJ8fCoDSpJvf1/g+04AKAGY0nIEelXugjGsZAGndhopJRy3MNAdUAGBqYprp+YmIiChzbJrQkbYuPl2pm21coWeVnrCzsNMpb3pSUynKuZWDicQEJhITpVYqbYPfiYiIKOcwoNJRxrmmssrd1l3rfnWLIH9Z4UsAaU/+6Sr9IssMqIiIiPIGu/x0lNXZzjP6od4PWvf7OPqopA2vNRy1vWqjmme1bF1T2xgqIiIiyjkMqHT0vwv/07o/qEQQElMSEfI8RO1+G6mN0vbRnkfx992/Ud6tPK6GX0W7Mu1UjjE3NUcd7zrZLnNmY6iIiIgoZ7DLT0fnX57Xut/R0hENfBpo3K8YkK7gYOmAb/2+Ra2itTCw5kCYmeRMbDui1gjxNbv8iIiI8gYDKj2kD1hszG20BjAZW6hyS2DRQPF1TgVpREREpB2/cXUwK0T9QsZuNm54EfMCAGBlZqUyD1R6GVuocksJ5xL4ufnPcLfRPgieiIiIcg4DKh3sub9HbbqjpaMYUJmamBpFCxWQNiM7ERER5R12+WVCLsg17rMz/zRvlKnEVOsgcE6iSURE9PliQJWJ5NRkjfvST75pIjERZygnIiKigoUBVSaSUpI07ku/gLGdhR28HLwgkagualzFvUqulI2IiIiMA8dQZSIpVXNABaStv3fh5QU0L9Ec5qbm2N5lO+68vYOpx6cCADZ13gRXa9e8KCoREREZCAOqTGRsoVrddjX6/dMPACBAQPfK3dG9cndxv4+jD4o5FENEXAR8HH1Q2qV0npaXiIiI8h4DqkxkHENV1aOq+FrTgHWJRIJv/b7NzWIRERGREfmsAiofHx88f/5cKW3OnDmYMGFCts+ZsctP3RgpooJAIpEgKSkJqamphi5KgSaTyWBmZobExETWhYGxLvKWVCqFqanxPjH/WQVUADBz5kz069dP3Lazs9OSO3PaBqUTFQSCIOD169fw9PREWFgY/6kwMEEQ4OHhgRcvXrAuDIx1kfccHR3h4eFhlPf7swuo7Ozs4OHhkWPny2xQOtHnLiIiArGxsfDw8ICzs7NR/4dYEMjlcsTFxcHW1hYmJnxQ25BYF3lHEATEx8fjzZs3AABPT08Dl0jVZxdQzZ07F7NmzYK3tze+/vprjBw5EmZmmt9mUlISkpI+BU2xsbEAgLtv7mLliZWoVKiS0vxSMplM3E5NTYVMJsuld0KKe8t7bDipqamIioqCm5sbpFIpLC0tjfI/w4JEEAQkJyfDwsKCdWFgrIu8ZWFhAblcjrdv38LJyUnlnztDf1d8VgHVsGHDUK1aNTg7O+Ps2bOYOHEiwsPD8csvv2g8Zs6cOZgxY4ZK+vDdwxFjGoNzj8+JaT5WPti/fz9iYmIAAE9SnmD//v05/0ZISXBwsKGLUGCZmZnBw8MDcnnaAxgfPnwwcIlIgXVhPFgXeUculyMhIQFHjx5FSkqK0r74+HgDlSqNRDDy6b0nTJiAefPmac1z9+5dlC1bViX9zz//xIABAxAXFwcLC/XLwqhrofLy8kLArwGQST9Fu1JTKY50PwILMwvMPTMXu+/vxp9t/0R5t/LZfGeUGZlMhuDgYDRr1gxSqdTQxSmQEhMT8eLFCxQrVgwymQx2dnb8T9zABEHAhw8fWBdGgHWR9xITE/Hs2TN4eXnB0lJ5/dzIyEh4enoiJiYG9vb2eV42o2+hGj16NHr37q01T/HixdWmBwQEICUlBc+ePUOZMmXU5rGwsFAbbCWmJMLM/NPtSZGnwNbKFgAwucFkjKw9Erbmtjq+C9KHVCplQGUgqampkEgk4peFRCLhWJF0pk+fjt27d+PatWt5dh1Fa2Fu1EXDhg1RtWpVLF68OEfP+7nKzbog9UxMTCCRSNR+Lxj6e8LoAyo3Nze4ubll69hr167BxMQEhQoVytEySSQSBlNE+cCLFy8wbdo0HDx4EO/evYOnpyc6dOiAqVOnwsXFJUvnkkgk2LVrFzp06CCmjRkzBkOHDs3hUhvOzp07Df6lRJRfGX1ApavQ0FCcP38ejRo1gp2dHUJDQzFy5Eh88803cHJy0vv8f7b/MwdKSUR55cmTJwgMDETp0qWxefNm+Pr64vbt2xg7diwOHDiAc+fOwdnZWa9r2Nrawtb28/nnSt/7QVSQfTZtlBYWFtiyZQsaNGiAChUq4KeffsLIkSOxatWqHDl/WVfVMVpEZLwGDx4Mc3NzHD58GA0aNIC3tzdatmyJI0eO4OXLl/jhhx/EvD4+Ppg1axa++uor2NjYoEiRIli2bJnSfgDo2LEjJBKJuD19+nRUrVpVzNe7d2906NABs2fPhru7OxwdHTFz5kykpKRg7NixcHZ2RtGiRbFmzRqlso4fPx6lS5eGtbU1ihcvjilTpmT5iaW9e/eiVKlSsLS0RKNGjbBu3TpIJBJER0cDSBtf8tVXX6FIkSKwtrZGpUqVsHnzZqVzNGzYECNGjFB637Nnz8a3334LOzs7eHt759jfVKLPzWcTUFWrVg3nzp1DdHQ0EhIScOfOHUycOFHjYPSskpqwGZwovY8fP2r8SUxM1DlvQkKCTnmz4v379zh06BAGDRoEKysrpX0eHh7o3r07tm7dqjQlyoIFC1ClShVcvXoVEyZMwPDhw8UnTC9evAgAWLNmDcLDw8VtdY4dO4ZXr17h5MmT+OWXXzBt2jS0adMGTk5OOH/+PL7//nsMGDAA//33n3iMnZ0d1q5dizt37mDJkiVYvXo1Fi1apPP7ffr0Kb744gt06NAB169fx4ABA5QCRiBtMG/16tXx77//4tatW+jfvz969OiBCxcuaD33woULUaNGDVy9ehWDBg3CwIEDcf/+fZ3LRlRQfDYBVW4yNzXnExxEGSi6u9T9dO7cWSlvoUKFNOZt2bKlUl4fHx+1+bLi4cOHEAQB5cqVU7u/XLlyiIqKwtu3b8W0OnXqYMKECShdujSGDh2KL774QgxqFOM4FbM0axvX6ezsjKVLl6JMmTL49ttvUaZMGcTHx2PSpEkoVaoUJk6cCHNzc5w+fVo8ZvLkyahduzZ8fHzQtm1bjBkzBtu2bdP5/f72228oU6YMFixYgDJlyqBbt24qD/MUKVIEY8aMQdWqVVG8eHEMHToULVq0yPQ6rVq1wqBBg1CyZEmMHz8erq6uOH78uM5lIyooPpsxVLlJasrWKaL8KCuzwgQGBqpsZ+dptwoVKig98eXu7o6KFSuK26ampnBxcRFnfAaArVu3YunSpXj8+DHi4uKQkpKSpce+79+/j5o1ayql+fv7K22npqZi9uzZ2LZtG16+fInk5GQkJSXB2tpa67krV64svpZIJPDw8FAqOxGlYUClAwnYOkWUUVxcnMZ9GWcw1vYFnPFx82fPnulVLgAoWbIkJBIJ7t69i44dO6rsv3v3LpycnLL9BLE2GZ+SUzzinTFN8ch9aGgounfvjhkzZiAoKAgODg7YsmULFi5cmKPlWrBgAZYsWYLFixejUqVKsLGxwYgRI5CcnJzl96MoOxF9woBKB3KBfzyIMrKxsTF4Xk1cXFzQrFkzLF++HCNHjlQaRxUREYGNGzeiZ8+eSl35586dUzrHuXPnlLoMpVIpUlNT9S5bRmfPnkWxYsWUxjw9f/48S+coU6aMyqoNGcd5nTlzBu3bt8c333wDIG0OpQcPHqB8eU5OTJQTOIZKB6lCzv8RJaLc9b///Q9JSUkICgrCyZMn8eLFCxw8eBDNmjVDkSJF8NNPPynlP3PmDObPn48HDx5g2bJl2L59O4YPHy7u9/HxwdGjRxEREYGoqKgcK2epUqUQFhaGLVu24PHjx1i6dCl27dqVpXMMGDAA9+7dw/jx4/HgwQNs27YNa9euBQAxaCxVqhSCg4Nx9uxZ3L17FwMGDMDr169z7H0QFXQMqHRg5KvzEJEapUqVwqVLl1C8eHF07doVJUqUQP/+/dGoUSOEhoaqzLk0evRoXLp0CX5+fvjxxx/xyy+/ICgoSNy/cOFCBAcHw8vLC35+fjlWznbt2mHkyJEYMmQIqlatirNnz2LKlClZOoevry927NiBnTt3onLlylixYoXY4qV40nny5MmoVq0agoKC0LBhQ3h4eChNUkpE+jH6tfzyWmxsLBwcHFBlURWYWaf1iJqbmuNs37MGLlnBI5PJsH//frRq1YqzNxtIYmIinj59imLFiiE5ORn29vaf5RIbPj4+GDFihNIcTMZKLpcjNjY207r46aefsHLlSrx48SIPS1ew6FoXlHMUf5N8fX3VruXn6urKtfyMGcdQEZGxW758OWrWrAkXFxecOXMGCxYswJAhQwxdLKICgwGVDgSwEY+IjNvDhw/x448/4v379/D29sbo0aMxceJEQxeLqMBgQKUD9ooSfd5yYqoGQ1u0aFGWZlcnopzFTl8dsMuPiIiItGFARURERKQnBlREREREemJARURERKQnBlQamEhMsKb9GjhbOWN3t92GLg4REREZMT7lp4FckKOSeyUc7nHY0EUhIiIiI8cWKiIiyjVr166Fo6OjoYthFJ49ewaJRIJr164ZuigqJBIJdu/enavXOHHiBCQSCaKjo3P1OobCgEqDGoVrGLoIRKSHt2/fYuDAgfD29oaFhQU8PDwQFBSEM2fOGLpoejlx4gScnJzyzZfSl19+iQcPHhi6GLkuLwKS/K527doIDw+Hg4ODoYuSK9jlp4G11NrQRSAiPXTu3BnJyclYt24dihcvjtevX+Po0aOIjIzM9jkFQUBqairMzJT/dCYnJ8Pc3FzfIn+WrKysYGVlpXE/713BYW5uDg8PD0MXI9ewhUqDNiXbGLoIRJRN0dHROHXqFObNm4dGjRqhWLFi8Pf3x8SJE9GuXTsA6rtfoqOjIZFIcOLECQCfuigOHDiA6tWrw8LCAqdPn0bDhg0xZMgQjBgxAq6urggKCgIAhISEwN/fHxYWFvD09MSECROQkpIinv/Dhw/o3r07bGxs4OnpiUWLFqFhw4ZKizL/9ddfqFGjBuzs7ODh4YGvv/4ab968EcvcpEkTAICLiwskEgl69+4NIG2h3jlz5sDX1xdWVlaoUqUKduzYofU+JSUlYcyYMShSpAhsbGwQEBAgvnfgU3fdoUOHUK5cOdja2qJFixYIDw8HABw+fBiWlpYqrWXDhw9H48aNlc6hMH36dFStWhW///670gK3YWFhaN++PWxtbWFvb4+uXbvi9evXKsf99ddf8PHxgYODA7p164YPHz6IeRo2bIihQ4dixIgRcHJygru7O1avXo2PHz+iT58+sLOzQ8mSJXHgwAGl8t66dQstW7aEra0t3N3d0aNHD7x7907pvMOGDcO4cePg7OwMDw8PTJ8+XdxfvHhxAEDHjh0hkUjg4+Oj9b7fu3cPtWvXhqWlJSpWrIiQkBBxX2pqKvr27SvWY5kyZbBkyRKl40+cOAF/f3/Y2NjA0dERderUwfPnz8X9e/bsQbVq1WBpaYnixYtjxowZSp/Dhw8fon79+rC0tET58uURHBystbyA/p9dRbnTd/ll9vnKbxhQaeBo5WjoIhAZJUEQkCBLMMiPrstA2drawtbWFrt370ZSUpLe73nChAmYO3cu7t69i8qVKwMA1q1bB3Nzc5w5cwYrV67Ey5cv0apVK9SsWRPXr1/HihUr8Mcff+DHH38UzzNq1CicOXMGe/fuRXBwME6dOoUrV64oXUsmk2HWrFm4fv06du/ejWfPnolBk5eXF7Zv3w4AuHv3LsLDw8Uv2zlz5mD9+vVYuXIlbt++jZEjR+Kbb75R+rLOaMiQIQgNDcWWLVtw48YNdOnSBS1atMDDhw/FPPHx8fj555/x119/4eTJkwgLC8OYMWMAAE2aNIGjoyP+/vtvMX9qaiq2bt2K7t27a7zuo0eP8Pfff2Pnzp24du0a5HI52rdvj/fv3yMkJATBwcF48uQJvvzyS6XjHj9+jN27d2Pfvn3Yt28fQkJCMHfuXKU869atg6urKy5cuIChQ4di4MCB6NKlC2rXro0rV66gefPm6NGjB+Lj4wGkBdGNGzeGn58fLl26hIMHD+L169fo2rWrynltbGxw/vx5zJ8/HzNnzhQDkfPnzwMA1qxZg/DwcFy8eFHjeweAsWPHYvTo0bh69SoCAwPRtm1bseVULpejaNGi2L59O+7cuYOpU6di0qRJ2LZtGwAgJSUFHTp0QIMGDXDjxg2Ehoaif//+kEgkAIBTp06hZ8+eGD58OO7cuYPffvsNa9euxU8//SSev1OnTjA3N8f58+excuVKjB8/Xmt5Af0/u5po+3zlOwIpiYmJEQAIj148MnRRCrzk5GRh9+7dQnJysqGLUmAlJCQId+7cET5+/ChERUUJqampQnxyvFD9t+oG+YlPjte57Dt27BCcnJwES0tLoXbt2sLEiROF69evi/ufPn0qABCuXr0qpkVFRQkAhOPHjwuCIAjHjx8XAAi7d+9WOneDBg0EPz8/pbRJkyYJZcqUEeRyuZi2bNkywdbWVkhNTRViY2MFqVQqbN++XdwfHR0tWFtbC8OHD9f4Pi5evCgAED58+CAIgiAcPXpUACBERkaKeRITEwVra2vh7NmzSsf27dtX+Oqrr9Se9/nz54Kpqanw8uVLpfQmTZoIEydOFARBENasWZP29/DRp7+Hy5YtE9zd3cXt4cOHC40bNxa3Dx06JFhYWAhRUVHiORwcHMT906ZNE6RSqfDmzRsx7fDhw4KpqakQFhYmpt2+fVsAIFy4cEE8ztraWoiNjRXzjB07VggICBC3GzRoINStW1fcTklJEWxsbIQePXqIaeHh4QIAITQ0VBAEQZg1a5bQvHlzpXvw4sULAYBw//59tecVBEGoWbOmMG7cOPH3AoCwa9cuQRvFZ27u3LlimkwmE4oWLSrMmzdP43GDBw8WOnfuLAiCIERGRgoAhBMnTqjN26RJE2H27NlKaX/99Zfg6ekpCEJa/ZiZmSnV+4EDB7SWP6c+u4rfp/Sfjcw+Xxkp/iYlJCSo7Hv37p0AQIiJidF4fG5iC5UGbKEiyt86d+6MV69eYe/evWjRogVOnDiBatWqYe3atVk+V40aqg+pVK9eXWn77t27CAwMFFsKAKBOnTqIi4vDf//9hydPnkAmk8Hf31/c7+DggDJlyiid5/Lly2jbti28vb1hZ2eHBg0aAEjrEtPk0aNHiI+PR7NmzcTWOVtbW6xfvx6PHz9We8zNmzeRmpqK0qVLKx0TEhKidIy1tTVKlCghbnt6eip143Tv3h0nTpzAq1evAAAbN25E69attT7ZV6xYMbi5uSndOy8vL3h5eYlp5cuXh6OjI+7evSum+fj4wM7OTmNZAIgtiABgamoKFxcXVKpUSUxzd3cHAPG469ev4/jx40r3oGzZsgCgdB/Sn1fTtXUVGBgovjYzM0ONGjWU3ueyZctQvXp1uLm5wdbWFqtWrRLr39nZGb1790ZQUBDatm2LJUuWKHWRXb9+HTNnzlR6P/369UN4eDji4+PFe124cGG15VEnNz+7mX2+8hMOSieiLLE0s8SpPqcMdu0s5be0RLNmzdCsWTNMmTIF3333HaZNm4bevXvDxCTt/0khXTeiTCZTex4bGxud0vT18eNHBAUFISgoCBs3boSbmxvCwsIQFBSE5ORkjcfFxcUBAP79918UKVJEaZ+FhYXGY0xNTXH58mWYmpoq7bO1tRVfS6VSpX0SiUTpntWsWRMlSpTAli1bMHDgQOzatSvToDW7905dWeRyeaZ50qcpAl7FcXFxcWjbti3mzZuncj1PT88sXTsnbNmyBWPGjMHChQsRGBgIOzs7LFiwQOxWBNK6FocNG4aDBw9i69atmDx5MoKDg1GrVi3ExcVhxowZ6NSpk8q5FePVckN2P7uZfb7yEwZURJQlEokEVlLNT20Zs/Lly4uPtitaSMLDw+Hn5wcAes0PVK5cOfz9998QBEH80j5z5gzs7OxQtGhRODk5QSqV4uLFi/D29gYAxMTE4MGDB6hfvz6AtMHKkZGRmDt3rthac+nSJaXrKJ6IS01NVXpfFhYWCAsLE1sFMuPn54fU1FS8efMG9erVy/b7BtJaqTZu3IiiRYvCxMQErVu3ztLx5cqVw4sXL/DixQvxfd+5cwfR0dEoX768XmXLTLVq1fD333/Dx8dH5enNrJBKpUp1os25c+fEOk9JScHly5cxZMgQAGmfmdq1a2PQoEFifnWtjH5+fvDz88PEiRMRGBiITZs2oVatWqhWrRru37+PkiVLqr224l6Hh4eLAeO5c+e0lrd48eI58tn93LHLj4g+O5GRkWjcuDE2bNiAGzdu4OnTp9i+fTvmz5+P9u3bA0h7nL9WrVriYPOQkBBMnjw529ccNGgQXrx4gaFDh+LevXvYs2cPpk2bhlGjRsHExAR2dnbo1asXxo4di+PHj+P27dvo27cvTExMxADM29sb5ubm+PXXX/HkyRPs3bsXs2bNUrpOsWLFIJFIsG/fPrx9+xZxcXGws7PDmDFjMHLkSKxbtw6PHz/GlStX8Ouvv2LdunVqy1u6dGl0794dPXv2xM6dO/H06VNcuHABc+bMwb///pul9969e3dcuXIFP/30E7744guNrWKaNG3aFJUqVRLPc+HCBfTs2RMNGjRQ292akwYPHoz379/jq6++wsWLF/H48WMcOnQIffr00TlAAtK6I48ePYqIiAhERUVpzbts2TLs2rUL9+7dw+DBgxEVFYVvv/0WAFCqVClcunQJhw4dwoMHDzBlyhSlQe5Pnz7FxIkTERoaiufPn+Pw4cN4+PAhypUrBwCYOnUq1q9fjxkzZuD27du4e/cutmzZIn62mzZtitKlS6NXr164fv06Tp06hR9++EFreXPqs/u5Y0BFRJ8dW1tbBAQEYNGiRahfvz4qVqyIKVOmoF+/fvjf//4n5vvzzz+RkpKC6tWrY8SIEUpP5GVVkSJFsH//fly4cAFVqlTB999/j759+yoFab/88gsCAwPRpk0bNG3aFHXq1EG5cuXErhg3NzesXbsW27dvR/ny5TF37lz8/PPPKteZOHEiJk2aBHd3d7FlY9asWZgyZQrmzJmDcuXKoUWLFvj333/h6+urscxr1qxBz549MXr0aJQpUwYdOnRQaoXQVcmSJeHv748bN25ofbpPE4lEgj179sDJyQn169dH06ZNUbx4cWzdujXL58qqwoUL48yZM0hNTUXz5s1RqVIljBgxAo6OjmK3sC4WLlyI4OBgeHl5iS2emsydOxdz585FlSpVcPr0aezduxeurq4AgAEDBqBTp0748ssvERAQgMjISKXWKmtra9y7dw+dO3dG6dKl0b9/fwwePBgDBgwAAAQFBWHfvn04fPgwatasiVq1amHRokUoVqwYAMDExAS7du1CQkIC/P398d1334lPAGqTE5/dz51EyK+dlbkkNjYWDg4OePfuHVxcXAxdnAJNJpNh//79aNWqlUo/O+WNxMREPH36FMWKFUNycjLs7e2z9CVD2n38+BFFihTBwoUL0bdvX52OkcvliI2NZV0YgYJcF9n57OYExd+k9HOYKURGRsLV1RUxMTGwt7fPszIpcAwVEVEeuXr1Ku7duwd/f3/ExMRg5syZACB2QxIZK352M8eAiogoD/3888+4f/8+zM3NUb16dZw6dUrs7iEyZvzsaseAiogoj/j5+eHy5cuGLgZRlvGzm7mC1elLRERElAsYUBERERHpiQEVEWWKDwMTkTEw5r9F+Sag+umnn1C7dm1YW1trXCMqLCwMrVu3hrW1NQoVKoSxY8ciJSUlbwtK9BlRTFcRHx9v4JIQEX36W2SMU+nkm0HpycnJ6NKlCwIDA/HHH3+o7E9NTUXr1q3h4eGBs2fPIjw8HD179oRUKsXs2bMNUGKi/M/U1BSOjo54+/Yt7OzsIJVKVdZ9o7wll8uRnJyMxMTEAjf3kbFhXeQdQRAQHx+PN2/ewNHR0Sj/DuWbgGrGjBkAoHHRzcOHD+POnTs4cuQI3N3dUbVqVcyaNQvjx4/H9OnTxfWviChrPDw8kJqaivDwcHz48EFcaoIMQxAEJCQkwMrKinVhYKyLvOfo6AgPDw9DF0OtfBNQZSY0NBSVKlWCu7u7mBYUFISBAwfi9u3bGpcCSEpKQlJSkrgdGxsLIG2Wbk0rz1PeUNx/1oPhOTs748qVK6hXr55eC8iS/lJSUnD27FnUrl2bdWFgrIu8I5FIYGZmBlNTU41DeQz9XfHZfAIiIiKUgikA4nZERITG4+bMmSO2fqV3/PhxWFtb52whKVuCg4MNXQT6fydPnjR0Eej/sS6MB+vCOBh6rKdBA6oJEyZg3rx5WvPcvXsXZcuWzbUyTJw4EaNGjRK3Y2Nj4eXlhUaNGnEtPwOTyWQIDg5Gs2bNjHIAYkHCujAerAvjwbowLpGRkQa9vkEDqtGjR6N3795a8xQvXlync3l4eODChQtKaa9fvxb3aWJhYQELCwuVdKlUyl8QI8G6MB6sC+PBujAerAvjYOg6MGhA5ebmBjc3txw5V2BgIH766Se8efMGhQoVApDWVWRvb4/y5cvnyDWIiIiI1Mk3Y6jCwsLw/v17hIWFITU1FdeuXQMAlCxZEra2tmjevDnKly+PHj16YP78+YiIiMDkyZMxePBgtS1QmigmDfvw4YPBo92CTiaTIT4+HrGxsawLA2NdGA/WhfFgXRiXDx8+ADDg5J9CPtGrVy8BgMrP8ePHxTzPnj0TWrZsKVhZWQmurq7C6NGjBZlMlqXrPH78WO11+MMf/vCHP/zhj/H/PH78OIcjEN1IBMGI53E3gOjoaDg5OSEsLAwODg6GLk6BpnhA4MWLF7C3tzd0cQo01oXxYF0YD9aFcYmJiYG3tzeioqI0rqiSm/JNl19eUcx26+DgwF8QI2Fvb8+6MBKsC+PBujAerAvjYqhZ6zlXPhEREZGeGFARERER6YkBVQYWFhaYNm1alp4MpNzBujAerAvjwbowHqwL42Lo+uCgdCIiIiI9sYWKiIiISE8MqIiIiIj0xICKiIiISE8MqIiIiIj0xIAqnWXLlsHHxweWlpYICAjAhQsXDF2kfG3OnDmoWbMm7OzsUKhQIXTo0AH3799XypOYmIjBgwfDxcUFtra26Ny5M16/fq2UJywsDK1bt4a1tTUKFSqEsWPHIiUlRSnPiRMnUK1aNVhYWKBkyZJYu3Ztbr+9fG3u3LmQSCQYMWKEmMa6yFsvX77EN998AxcXF1hZWaFSpUq4dOmSuF8QBEydOhWenp6wsrJC06ZN8fDhQ6VzvH//Ht27d4e9vT0cHR3Rt29fxMXFKeW5ceMG6tWrB0tLS3h5eWH+/Pl58v7yi9TUVEyZMgW+vr6wsrJCiRIlMGvWLKX14FgXuePkyZNo27YtChcuDIlEgt27dyvtz8v7vn37dpQtWxaWlpaoVKkS9u/fn/U3ZJAFb4zQli1bBHNzc+HPP/8Ubt++LfTr109wdHQUXr9+beii5VtBQUHCmjVrhFu3bgnXrl0TWrVqJXh7ewtxcXFinu+//17w8vISjh49Kly6dEmoVauWULt2bXF/SkqKULFiRaFp06bC1atXhf379wuurq7CxIkTxTxPnjwRrK2thVGjRgl37twRfv31V8HU1FQ4ePBgnr7f/OLChQuCj4+PULlyZWH48OFiOusi77x//14oVqyY0Lt3b+H8+fPCkydPhEOHDgmPHj0S88ydO1dwcHAQdu/eLVy/fl1o166d4OvrKyQkJIh5WrRoIVSpUkU4d+6ccOrUKaFkyZLCV199Je6PiYkR3N3dhe7duwu3bt0SNm/eLFhZWQm//fZbnr5fY/bTTz8JLi4uwr59+4SnT58K27dvF2xtbYUlS5aIeVgXuWP//v3CDz/8IOzcuVMAIOzatUtpf17d9zNnzgimpqbC/PnzhTt37giTJ08WpFKpcPPmzSy9HwZU/8/f318YPHiwuJ2amioULlxYmDNnjgFL9Xl58+aNAEAICQkRBEEQoqOjBalUKmzfvl3Mc/fuXQGAEBoaKghC2i+ciYmJEBERIeZZsWKFYG9vLyQlJQmCIAjjxo0TKlSooHStL7/8UggKCsrtt5TvfPjwQShVqpQQHBwsNGjQQAyoWBd5a/z48ULdunU17pfL5YKHh4ewYMECMS06OlqwsLAQNm/eLAiCINy5c0cAIFy8eFHMc+DAAUEikQgvX74UBEEQli9fLjg5OYn1o7h2mTJlcvot5VutW7cWvv32W6W0Tp06Cd27dxcEgXWRVzIGVHl537t27Sq0bt1aqTwBAQHCgAEDsvQe2OUHIDk5GZcvX0bTpk3FNBMTEzRt2hShoaEGLNnnJSYmBgDg7OwMALh8+TJkMpnSfS9btiy8vb3F+x4aGopKlSrB3d1dzBMUFITY2Fjcvn1bzJP+HIo8rDtVgwcPRuvWrVXuF+sib+3duxc1atRAly5dUKhQIfj5+WH16tXi/qdPnyIiIkLpXjo4OCAgIECpPhwdHVGjRg0xT9OmTWFiYoLz58+LeerXrw9zc3MxT1BQEO7fv4+oqKjcfpv5Qu3atXH06FE8ePAAAHD9+nWcPn0aLVu2BMC6MJS8vO859XeLARWAd+/eITU1VemLAgDc3d0RERFhoFJ9XuRyOUaMGIE6deqgYsWKAICIiAiYm5urrAqe/r5HRESorRfFPm15YmNjkZCQkBtvJ1/asmULrly5gjlz5qjsY13krSdPnmDFihUoVaoUDh06hIEDB2LYsGFYt24dgE/3U9vfpIiICBQqVEhpv5mZGZydnbNUZwXdhAkT0K1bN5QtWxZSqRR+fn4YMWIEunfvDoB1YSh5ed815clqvZhlKTdRNg0ePBi3bt3C6dOnDV2UAunFixcYPnw4goODYWlpaejiFHhyuRw1atTA7NmzAQB+fn64desWVq5ciV69ehm4dAXLtm3bsHHjRmzatAkVKlTAtWvXMGLECBQuXJh1QVnCFioArq6uMDU1VXmi6fXr1/Dw8DBQqT4fQ4YMwb59+3D8+HEULVpUTPfw8EBycjKio6OV8qe/7x4eHmrrRbFPWx57e3tYWVnl9NvJly5fvow3b96gWrVqMDMzg5mZGUJCQrB06VKYmZnB3d2ddZGHPD09Ub58eaW0cuXKISwsDMCn+6ntb5KHhwfevHmjtD8lJQXv37/PUp0VdGPHjhVbqSpVqoQePXpg5MiRYksu68Iw8vK+a8qT1XphQAXA3Nwc1atXx9GjR8U0uVyOo0ePIjAw0IAly98EQcCQIUOwa9cuHDt2DL6+vkr7q1evDqlUqnTf79+/j7CwMPG+BwYG4ubNm0q/NMHBwbC3txe/kAIDA5XOocjDuvukSZMmuHnzJq5duyb+1KhRA927dxdfsy7yTp06dVSmEHnw4AGKFSsGAPD19YWHh4fSvYyNjcX58+eV6iM6OhqXL18W8xw7dgxyuRwBAQFinpMnT0Imk4l5goODUaZMGTg5OeXa+8tP4uPjYWKi/FVoamoKuVwOgHVhKHl533Ps71aWhrB/xrZs2SJYWFgIa9euFe7cuSP0799fcHR0VHqiibJm4MCBgoODg3DixAkhPDxc/ImPjxfzfP/994K3t7dw7Ngx4dKlS0JgYKAQGBgo7lc8qt+8eXPh2rVrwsGDBwU3Nze1j+qPHTtWuHv3rrBs2TI+qq+D9E/5CQLrIi9duHBBMDMzE3766Sfh4cOHwsaNGwVra2thw4YNYp65c+cKjo6Owp49e4QbN24I7du3V/vIuJ+fn3D+/Hnh9OnTQqlSpZQeGY+Ojhbc3d2FHj16CLdu3RK2bNkiWFtbF+hH9TPq1auXUKRIEXHahJ07dwqurq7CuHHjxDysi9zx4cMH4erVq8LVq1cFAMIvv/wiXL16VXj+/LkgCHl338+cOSOYmZkJP//8s3D37l1h2rRpnDZBX7/++qvg7e0tmJubC/7+/sK5c+cMXaR8DYDanzVr1oh5EhIShEGDBglOTk6CtbW10LFjRyE8PFzpPM+ePRNatmwpWFlZCa6ursLo0aMFmUymlOf48eNC1apVBXNzc6F48eJK1yD1MgZUrIu89c8//wgVK1YULCwshLJlywqrVq1S2i+Xy4UpU6YI7u7ugoWFhdCkSRPh/v37SnkiIyOFr776SrC1tRXs7e2FPn36CB8+fFDKc/36daFu3bqChYWFUKRIEWHu3Lm5/t7yk9jYWGH48OGCt7e3YGlpKRQvXlz44YcflB6zZ13kjuPHj6v9jujVq5cgCHl737dt2yaULl1aMDc3FypUqCD8+++/WX4/EkFINx0sERER/V979x9T0//HAfx5bvSTSFpqce/8yFrlljAkJeaykbbQrFHYYkpFGmaU29CllMiMP1x/GM2INpPNNFR+rx+MYnf5nd/MbjPcen//sM/5uN2onOj7/fZ8/HXf7/c57/fr7GztuXPet0vUZdxDRURERKQQAxURERGRQgxURERERAoxUBEREREpxEBFREREpBADFREREZFCDFRERERECjFQEdEf9ejRI0iShJqamp4uRVZfX4+JEyfC0dERQUFB7R4TERGBtLS0v1pXZ0iShNOnT/d0GUTUBgMV0f+5hIQESJKEnJwcq/7Tp09DkqQeqqpnZWZmwsXFBQ0NDTa/4fWPU6dOITs7W25rNBoUFBT8pQqBrKysdsNeU1MTZs+e/dfqIKLOYaAi6gUcHR1hMBjw4cOHni6l23z9+vW3zzWZTJgyZQrUajXc3d3bPWbQoEHo37//b6/xM0rqBoAhQ4bAwcGhm6ohou7CQEXUC8yYMQNDhgzBjh07fnpMe09ECgoKoNFo5HZCQgKio6Oxfft2eHp6YuDAgdDr9bBYLMjIyMCgQYPg4+ODw4cP28xfX1+PyZMnw9HREQEBAbh06ZLV+N27dzF79mz069cPnp6eWLx4Md6+fSuPR0REIDk5GWlpaRg8eDB0Ol2719Ha2gq9Xg8fHx84ODggKCgIZWVl8rgkSbh9+zb0ej0kSUJWVla78/z4yi8iIgKPHz/GmjVrIEmS1ZO9iooKhIWFwcnJCUOHDkVKSgqam5vlcY1Gg+zsbCxZsgSurq5ITEwEAKxfvx6+vr5wdnbG8OHDsXnzZnz79g0AYDQasXXrVtTW1srrGY1Guf4fX/nduXMHkZGRcHJygru7OxITE2E2m23uWW5uLry8vODu7o6kpCR5LSLqHgxURL2AnZ0dtm/fjr179+LZs2eK5rp48SJevHiBy5cvY/fu3cjMzMScOXPg5uaG69evY+XKlVixYoXNOhkZGUhPT0d1dTUmTZqEuXPn4t27dwCAjx8/IjIyEsHBwbh16xbKysrw6tUrLFy40GqOI0eOwN7eHpWVlThw4EC79e3Zswd5eXnIzc1FXV0ddDodoqKi8PDhQwDfX5n5+/sjPT0dTU1NWLduXYfXfOrUKfj4+ECv16OpqQlNTU0Avj/pmjVrFmJiYlBXV4fi4mJUVFQgOTnZ6vzc3FxotVpUV1dj8+bNAID+/fvDaDTi3r172LNnDw4dOoT8/HwAQGxsLNLT0+Hv7y+vFxsba1NXc3MzdDod3NzccPPmTZw4cQIXLlywWb+8vBwmkwnl5eU4cuQIjEajHNCIqJt0+eeUieh/Snx8vJg3b54QQoiJEyeKZcuWCSGEKCkpET/+CcjMzBRardbq3Pz8fKFWq63mUqvVoqWlRe4bPXq0CAsLk9sWi0W4uLiIY8eOCSGEaGxsFACsfuH927dvwsfHRxgMBiGEENnZ2WLmzJlWaz99+lQAkH9dPjw8XAQHB3d4vd7e3mLbtm1WfePHjxerVq2S21qtVmRmZv5ynvDwcJGamiq31Wq1yM/Ptzpm+fLlIjEx0arvypUrQqVSic+fP8vnRUdHd1j3rl27REhIiNxu734IIQQAUVJSIoQQ4uDBg8LNzU2YzWZ5/OzZs0KlUomXL18KIf69ZxaLRT5mwYIFIjY2tsOaiKjz+vRsnCOiv8lgMCAyMrJTT2V+xt/fHyrVvw+3PT09ERAQILft7Ozg7u6O169fW503adIk+XOfPn0wbtw43L9/HwBQW1uL8vJy9OvXz2Y9k8kEX19fAEBISMgva/v06RNevHiB0NBQq/7Q0FDU1tZ28go7r7a2FnV1dTh69KjcJ4RAa2srGhsb4efnBwAYN26czbnFxcUoLCyEyWSC2WyGxWKBq6trl9a/f/8+tFotXFxc5L7Q0FC0traioaEBnp6eAL7fMzs7O/kYLy8v3Llzp0trEdGvMVAR9SJTp06FTqfDxo0bkZCQYDWmUqkghLDqa2+fTd++fa3akiS129fa2trpusxmM+bOnQuDwWAz5uXlJX/+MTj8NzCbzVixYgVSUlJsxoYNGyZ/blv31atXERcXh61bt0Kn02HAgAE4fvw48vLy/kidSu8PEXWMgYqol8nJyUFQUBBGjx5t1e/h4YGXL19CCCFvuu7O/x117do1TJ06FQBgsVhw+/Ztea/P2LFjcfLkSWg0GvTp8/t/llxdXeHt7Y3KykqEh4fL/ZWVlZgwYYKi+u3t7dHS0mLVN3bsWNy7dw8jR47s0lxVVVVQq9XYtGmT3Pf48eMO12vLz88PRqMRzc3NcmirrKyESqWyub9E9GdxUzpRLxMYGIi4uDgUFhZa9UdERODNmzfYuXMnTCYTioqKcO7cuW5bt6ioCCUlJaivr0dSUhI+fPiAZcuWAQCSkpLw/v17LFq0CDdv3oTJZML58+exdOnSDkNFWxkZGTAYDCguLkZDQwM2bNiAmpoapKamKqpfo9Hg8uXLeP78ufztw/Xr16OqqgrJycmoqanBw4cPcebMGZtN4W2NGjUKT548wfHjx2EymVBYWIiSkhKb9RobG1FTU4O3b9/iy5cvNvPExcXB0dER8fHxuHv3LsrLy7F69WosXrxYft1HRH8HAxVRL6TX621e+fj5+WH//v0oKiqCVqvFjRs3FO21aisnJwc5OTnQarWoqKhAaWkpBg8eDADyU6WWlhbMnDkTgYGBSEtLw8CBA632a3VGSkoK1q5di/T0dAQGBqKsrAylpaUYNWqUovr1ej0ePXqEESNGwMPDAwAwZswYXLp0CQ8ePEBYWBiCg4OxZcsWeHt7/3KuqKgorFmzBsnJyQgKCkJVVZX87b9/xMTEYNasWZg2bRo8PDxw7Ngxm3mcnZ1x/vx5vH//HuPHj8f8+fMxffp07Nu3T9G1ElHXSaLtpgkiIiIi6hI+oSIiIiJSiIGKiIiISCEGKiIiIiKFGKiIiIiIFGKgIiIiIlKIgYqIiIhIIQYqIiIiIoUYqIiIiIgUYqAiIiIiUoiBioiIiEghBioiIiIihRioiIiIiBT6D9ORl6cUrAweAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_curve('0.5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "01DcJ4gRMT7L",
        "outputId": "5e861815-750b-47e3-a395-02bbe7a892df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17, 10000)\n",
            "MSE:  11.586407844161418\n",
            "MAE:  3.4834197490892933\n",
            "Max value DT: 19.836105346679688\n",
            "Max value actual: 25.33646285301225\n",
            "MSE: 10.629204284277494\n",
            "MAE: 2.4751118361581708\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_curve('1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lGGKfYYNMYyH",
        "outputId": "3dfd02a9-24c6-4e86-ded5-54f7865ce9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17, 10000)\n",
            "MSE:  12.034956757841801\n",
            "MAE:  3.5258714221562477\n",
            "Max value DT: 21.19365692138672\n",
            "Max value actual: 12.995337795554889\n",
            "MSE: 9.283886680593907\n",
            "MAE: 2.2181636927817556\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_curve('5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cOXO_gkHNACj",
        "outputId": "25faed85-058f-4619-f1e2-0e88f19a980f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17, 10000)\n",
            "MSE:  12.075405179657563\n",
            "MAE:  3.493287668744076\n",
            "Max value DT: 19.467254638671875\n",
            "Max value actual: 19.410967251046657\n",
            "MSE: 9.150556548203266\n",
            "MAE: 2.176775794744876\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_curve('10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ZAN9d__uczOQ",
        "outputId": "4dee99d5-57b2-4463-e45d-1aedf15658f5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_curve_active()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F24Spk_Xreg"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}